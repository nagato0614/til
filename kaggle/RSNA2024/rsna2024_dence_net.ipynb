{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:14:45.274064Z",
     "start_time": "2024-06-26T12:14:42.823117Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "from keras import Model, Input\n",
    "from keras.src import backend\n",
    "from keras.src import layers\n",
    "from keras.src.applications import imagenet_utils\n",
    "from keras.src.layers import Dense\n",
    "from keras.src.layers import GlobalAveragePooling2D\n",
    "from keras.src.models import Functional\n",
    "from keras.src.ops import operation_utils\n",
    "from keras.src.utils import file_utils\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from keras.callbacks import Callback\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:14:45.307357Z",
     "start_time": "2024-06-26T12:14:45.275123Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = './rsna-2024-lumbar-spine-degenerative-classification'\n",
    "train_png_file_path = f'{base_path}/train_dataset_png'\n",
    "\n",
    "label_coordinates_df = pd.read_csv(f'{base_path}/train_label_coordinates.csv')\n",
    "train_series = pd.read_csv(f'{base_path}/train_series_descriptions.csv')\n",
    "df_train = pd.read_csv(f'{base_path}/train.csv')\n",
    "df_sub = pd.read_csv(f'{base_path}/sample_submission.csv')\n",
    "test_series = pd.read_csv(f'{base_path}/test_series_descriptions.csv')\n",
    "\n",
    "train_dicom = f'{base_path}/train_images'\n",
    "test_dicom = f'{base_path}/test_images'\n",
    "\n",
    "model_path = f'model.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:14:45.523320Z",
     "start_time": "2024-06-26T12:14:45.308245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study_id : 2492114990, series_id : ['Axial T2' 'Sagittal T2/STIR']\n",
      "study_id : 2780132468, series_id : ['Sagittal T2/STIR' 'Axial T2']\n",
      "study_id : 3008676218, series_id : ['Sagittal T1' 'Axial T2']\n"
     ]
    }
   ],
   "source": [
    "# 一つのstudy_idにseries_id が3つ未満のstudy_idを探す\n",
    "study_id_list = train_series['study_id'].unique()\n",
    "for study_id in study_id_list:\n",
    "    series_id_list = train_series[train_series['study_id'] == study_id]['series_description'].unique()\n",
    "    \n",
    "    if len(series_id_list) < 3:\n",
    "        print(f'study_id : {study_id}, series_id : {series_id_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:14:30.767539Z",
     "start_time": "2024-06-26T12:14:30.764003Z"
    }
   },
   "outputs": [],
   "source": [
    "# パラメータ\n",
    "STUDY_NUM = 1975\n",
    "\n",
    "# 出力のラベル\n",
    "NORMAL_MILD = 0\n",
    "MODERATE = 1\n",
    "SEVERE = 2\n",
    "\n",
    "# 重症度一覧\n",
    "SEVERITY_LIST = ['Normal/Mild', 'Moderate', 'Severe']\n",
    "\n",
    "# dicomデータの種類\n",
    "SERIES_DESCRIPTION_LIST = train_series['series_description'].unique().tolist()\n",
    "# 学習用パラメータ\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "TRAIN_RATIO = 0.8\n",
    "VALID_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "# 入出力の形状\n",
    "INPUT_WIDTH = 256\n",
    "INPUT_HEIGHT = 256\n",
    "INPUT_CHANNEL_DICT = {\n",
    "    'Sagittal T1': 17,\n",
    "    'Sagittal T2/STIR': 17,\n",
    "    'Axial T2': 29\n",
    "}\n",
    "IN_CHANS = sum(INPUT_CHANNEL_DICT.values())\n",
    "IMG_SIZE = [INPUT_WIDTH, INPUT_HEIGHT]\n",
    "\n",
    "CONDITIONS = [\n",
    "    'spinal_canal_stenosis',\n",
    "    'left_neural_foraminal_narrowing',\n",
    "    'right_neural_foraminal_narrowing',\n",
    "    'left_subarticular_stenosis',\n",
    "    'right_subarticular_stenosis'\n",
    "]\n",
    "\n",
    "LEVELS = [\n",
    "    'l1_l2',\n",
    "    'l2_l3',\n",
    "    'l3_l4',\n",
    "    'l4_l5',\n",
    "    'l5_s1',\n",
    "]\n",
    "\n",
    "N_LABELS = 25\n",
    "N_CLASSES = 3 * N_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:14:32.226652Z",
     "start_time": "2024-06-26T12:14:32.211217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape : (1975, 26)\n",
      "['Normal/Mild' 'Moderate' 'Severe' nan]\n",
      "Normal/Mild : 1399\n",
      "Moderate : 396\n",
      "Severe : 173\n",
      "Normal/Mild ratio : 0.7083544303797469\n",
      "Moderate ratio : 0.20050632911392405\n",
      "Severe ratio : 0.08759493670886076\n"
     ]
    }
   ],
   "source": [
    "_target_name = \"right_subarticular_stenosis_l5_s1\"\n",
    "\n",
    "print(f'train shape : {df_train.shape}')\n",
    "print(df_train[_target_name].unique())\n",
    "\n",
    "# それぞれの重症度の数がいくらあるか計算\n",
    "print(f'Normal/Mild : {df_train[df_train[_target_name] == str(\"Normal/Mild\")].shape[0]}')\n",
    "print(f'Moderate : {df_train[df_train[_target_name] == str(\"Moderate\")].shape[0]}')\n",
    "print(f'Severe : {df_train[df_train[_target_name] == str(\"Severe\")].shape[0]}')\n",
    "\n",
    "print(\n",
    "    f'Normal/Mild ratio : {df_train[df_train[_target_name] == str(\"Normal/Mild\")].shape[0] / df_train.shape[0]}')\n",
    "print(\n",
    "    f'Moderate ratio : {df_train[df_train[_target_name] == str(\"Moderate\")].shape[0] / df_train.shape[0]}')\n",
    "print(\n",
    "    f'Severe ratio : {df_train[df_train[_target_name] == str(\"Severe\")].shape[0] / df_train.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:14:32.386242Z",
     "start_time": "2024-06-26T12:14:32.376001Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "\n",
    "def get_train_label(study_id):\n",
    "    \"\"\"\n",
    "    study_id に対応するラベルを取得 (testはデータがないのでtrainのみ)\n",
    "    \"\"\"\n",
    "\n",
    "    row = df_train[df_train['study_id'] == study_id].to_numpy()[0][1:].tolist()\n",
    "\n",
    "    row_category = np.zeros((N_LABELS, 3))\n",
    "    for i in range(N_LABELS):\n",
    "        if row[i] == 'Normal/Mild':\n",
    "            row_category[i][0] = 1\n",
    "        elif row[i] == 'Moderate':\n",
    "            row_category[i][1] = 1\n",
    "        elif row[i] == 'Severe':\n",
    "            row_category[i][2] = 1\n",
    "    # 1次元配列に変換\n",
    "    row_category = row_category.flatten()\n",
    "    return row_category\n",
    "\n",
    "\n",
    "def get_file_list(study_id, series_desc, file_kind='train'):\n",
    "    \"\"\"\n",
    "    study_id に対応するファイルリストを取得\n",
    "    \"\"\"\n",
    "    dicom_file_list = []\n",
    "    df = None\n",
    "    if file_kind == 'train':\n",
    "        df = train_series\n",
    "    elif file_kind == 'test':\n",
    "        df = test_series\n",
    "\n",
    "    # study_id に対応する series_desc のリストを取得\n",
    "    series_ids = df[df['study_id'] == study_id]\n",
    "    series_id = series_ids[series_ids['series_description'] == series_desc]['series_id'].values\n",
    "    if len(series_id) == 0:\n",
    "        return dicom_file_list\n",
    "    \n",
    "    series_id = series_id[0]\n",
    "    \n",
    "    target_dicom_dir_path = ''\n",
    "    if file_kind == 'train':\n",
    "        target_dicom_dir_path = f'{train_dicom}/{study_id}/{series_id}/'\n",
    "    elif file_kind == 'test':\n",
    "        target_dicom_dir_path = f'{test_dicom}/{study_id}/{series_id}/'\n",
    "    dicom_file_list = glob(target_dicom_dir_path + '/*.dcm')\n",
    "    # ファイル名が数字になっているのでソート\n",
    "    dicom_file_list = sorted(dicom_file_list, key=natural_keys)\n",
    "\n",
    "    # series_desc にt対応するファイルリストを取得\n",
    "    return dicom_file_list\n",
    "\n",
    "\n",
    "def load_dicom_img(dicom_filepath):\n",
    "    \"\"\"\n",
    "    dicomファイルを読み込む\n",
    "    \"\"\"\n",
    "    dicom_data = dicom.read_file(dicom_filepath).pixel_array\n",
    "\n",
    "    # 0-1に正規化\n",
    "    dicom_data = dicom_data / np.max(dicom_data)\n",
    "\n",
    "    # 0~255に変換\n",
    "    dicom_data = dicom_data * 255\n",
    "\n",
    "    # 512x512にリサイズ\n",
    "    dicom_data = cv2.resize(dicom_data, (INPUT_WIDTH, INPUT_HEIGHT))\n",
    "\n",
    "    # uint8 に変換\n",
    "    dicom_data = dicom_data.astype(np.uint8)\n",
    "\n",
    "    return dicom_data\n",
    "\n",
    "\n",
    "def get_dicom_input_data(study_id, file_kind='train'):\n",
    "    \"\"\"\n",
    "    study_id に対応するdicomデータを取得\n",
    "    \"\"\"\n",
    "\n",
    "    input_dicom = np.zeros((INPUT_WIDTH, INPUT_HEIGHT, IN_CHANS))\n",
    "    # Sagittal T1\n",
    "    sagittal_t1_dicom_list = get_file_list(study_id, 'Sagittal T1', file_kind)\n",
    "    if len(sagittal_t1_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Sagittal T1']):\n",
    "            if i < len(sagittal_t1_dicom_list):\n",
    "                dicom_data = load_dicom_img(sagittal_t1_dicom_list[i])\n",
    "                input_dicom[:, :, i] = dicom_data\n",
    "\n",
    "    # Sagittal T2/STIR\n",
    "    sagittal_t2_dicom_list = get_file_list(study_id, 'Sagittal T2/STIR', file_kind)\n",
    "    if len(sagittal_t2_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Sagittal T2/STIR']):\n",
    "            if i < len(sagittal_t2_dicom_list):\n",
    "                dicom_data = load_dicom_img(sagittal_t2_dicom_list[i])\n",
    "                input_dicom[:, :, i + INPUT_CHANNEL_DICT['Sagittal T1']] = dicom_data\n",
    "\n",
    "    # Axial T2\n",
    "    axial_t2_dicom_list = get_file_list(study_id, 'Axial T2', file_kind)\n",
    "    if len(axial_t2_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Axial T2']):\n",
    "            if i < len(axial_t2_dicom_list):\n",
    "                dicom_data = load_dicom_img(axial_t2_dicom_list[i])\n",
    "                input_dicom[:, :, i + INPUT_CHANNEL_DICT['Sagittal T1'] + INPUT_CHANNEL_DICT[\n",
    "                    'Sagittal T2/STIR']] = dicom_data\n",
    "\n",
    "    # uint8 に変換\n",
    "    input_dicom = input_dicom.astype(np.int8)\n",
    "    return input_dicom\n",
    "\n",
    "\n",
    "def get_dicom_and_label(study_id, file_kind):\n",
    "    x_data = get_dicom_input_data(study_id, file_kind)\n",
    "    y_data = get_train_label(study_id)\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "def generator(batch_size, study_ids, file_kind='train', is_augmentation=False, is_shuffle=False):\n",
    "    \"\"\"\n",
    "    データセットの読み込み関数\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        if is_shuffle:\n",
    "            np.random.shuffle(study_ids)\n",
    "        \n",
    "        for start in range(0, len(study_ids), batch_size):\n",
    "            gc.collect()\n",
    "            \n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(study_ids))\n",
    "            ids_train_batch = study_ids[start:end]\n",
    "\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                results = list(\n",
    "                    executor.map(lambda sid: get_dicom_and_label(sid, file_kind), ids_train_batch))\n",
    "\n",
    "            for x_data, y_data in results:\n",
    "                if is_augmentation:\n",
    "                    # 画像データにノイズを加える\n",
    "                    x_data = x_data + np.random.normal(0, 0.1, x_data.shape)\n",
    "                \n",
    "                \n",
    "                x_batch.append(x_data)\n",
    "                y_batch.append(y_data)\n",
    "            \n",
    "            yield np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "# 学習用データの study_id を取得\n",
    "def get_train_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 0.8 で分割\n",
    "    train_study_ids = _study_ids[:int(len(_study_ids) * 0.8)]\n",
    "    return train_study_ids\n",
    "\n",
    "\n",
    "# 検証用データの study_id を取得\n",
    "def get_valid_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 0.8 で分割\n",
    "    valid_study_ids = _study_ids[int(len(_study_ids) * 0.8):int(len(_study_ids) * 0.9)]\n",
    "    return valid_study_ids\n",
    "\n",
    "\n",
    "# テスト用データの study_id を取得 (実際のテストデータではなく, 学習データの一部をテストデータとして使用)\n",
    "def get_test_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "    # 0.8 で分割\n",
    "    test_study_ids = _study_ids[int(len(_study_ids) * 0.9):]\n",
    "    return test_study_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:14:32.791291Z",
     "start_time": "2024-06-26T12:14:32.786869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sagittal T2/STIR : 18\n",
      "Sagittal T1 : 0\n",
      "Axial T2 : 53\n"
     ]
    }
   ],
   "source": [
    "# 2492114990\n",
    "for desc in SERIES_DESCRIPTION_LIST:\n",
    "    dicom_image_list = get_file_list(2492114990, desc)\n",
    "    print(f'{desc} : {len(dicom_image_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:14:33.328481Z",
     "start_time": "2024-06-26T12:14:33.325652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1580\n",
      "197\n",
      "198\n"
     ]
    }
   ],
   "source": [
    "# study_id のリストを作成\n",
    "train_study_ids = get_train_study_id_array()\n",
    "print(len(train_study_ids))\n",
    "valid_study_ids = get_valid_study_id_array()\n",
    "print(len(valid_study_ids))\n",
    "test_study_ids = get_test_study_id_array()\n",
    "print(len(test_study_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:49:09.722916Z",
     "start_time": "2024-06-25T14:49:09.720241Z"
    }
   },
   "outputs": [],
   "source": [
    "get_train_label(4003253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:49:10.087577Z",
     "start_time": "2024-06-25T14:49:09.723742Z"
    }
   },
   "outputs": [],
   "source": [
    "dicom_image_list = get_file_list(4003253, 'Sagittal T1')\n",
    "\n",
    "input_image = get_dicom_input_data(4003253)\n",
    "plt.imshow(input_image[:, :, 0], cmap='gray')\n",
    "input_data_gb = input_image.nbytes / 1024 / 1024 / 1024\n",
    "print(f\"画像サイズ[GB] : {input_data_gb}\")\n",
    "print(f'データセット全体のデータ数は {input_data_gb * STUDY_NUM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T14:51:06.021632Z",
     "start_time": "2024-06-25T14:51:04.494897Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-26 21:36:26.435247: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2024-06-26 21:36:26.435270: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2024-06-26 21:36:26.435277: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2024-06-26 21:36:26.435343: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-26 21:36:26.435542: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "def transition_block(x, reduction, name):\n",
    "    \"\"\"A transition block.\n",
    "\n",
    "    Args:\n",
    "        x: input tensor.\n",
    "        reduction: float, compression rate at transition layers.\n",
    "        name: string, block label.\n",
    "\n",
    "    Returns:\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_bn\"\n",
    "    )(x)\n",
    "    x = layers.Activation(\"relu\", name=name + \"_relu\")(x)\n",
    "    x = layers.Conv2D(\n",
    "        int(x.shape[bn_axis] * reduction),\n",
    "        1,\n",
    "        use_bias=False,\n",
    "        name=name + \"_conv\",\n",
    "    )(x)\n",
    "    x = layers.AveragePooling2D(2, strides=2, name=name + \"_pool\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(x, growth_rate, name):\n",
    "    \"\"\"A building block for a dense block.\n",
    "\n",
    "    Args:\n",
    "        x: input tensor.\n",
    "        growth_rate: float, growth rate at dense layers.\n",
    "        name: string, block label.\n",
    "\n",
    "    Returns:\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n",
    "    x1 = layers.BatchNormalization(\n",
    "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_0_bn\"\n",
    "    )(x)\n",
    "    x1 = layers.Activation(\"relu\", name=name + \"_0_relu\")(x1)\n",
    "    x1 = layers.Conv2D(\n",
    "        4 * growth_rate, 1, use_bias=False, name=name + \"_1_conv\"\n",
    "    )(x1)\n",
    "    x1 = layers.BatchNormalization(\n",
    "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_1_bn\"\n",
    "    )(x1)\n",
    "    x1 = layers.Activation(\"relu\", name=name + \"_1_relu\")(x1)\n",
    "    x1 = layers.Conv2D(\n",
    "        growth_rate, 3, padding=\"same\", use_bias=False, name=name + \"_2_conv\"\n",
    "    )(x1)\n",
    "    x = layers.Concatenate(axis=bn_axis, name=name + \"_concat\")([x, x1])\n",
    "    return x\n",
    "\n",
    "\n",
    "def dense_block(x, blocks, name):\n",
    "    \"\"\"A dense block.\n",
    "\n",
    "    Args:\n",
    "        x: input tensor.\n",
    "        blocks: integer, the number of building blocks.\n",
    "        name: string, block label.\n",
    "\n",
    "    Returns:\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    for i in range(blocks):\n",
    "        x = conv_block(x, 32, name=name + \"_block\" + str(i + 1))\n",
    "    return x\n",
    "\n",
    "\n",
    "def DenseNet(\n",
    "        blocks,\n",
    "        include_top=True,\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=None,\n",
    "        input_shape=None,\n",
    "        pooling=None,\n",
    "        classes=1000,\n",
    "        classifier_activation=\"softmax\",\n",
    "):\n",
    "    if backend.image_data_format() == \"channels_first\":\n",
    "        raise ValueError(\n",
    "            \"DenseNet does not support the `channels_first` image data \"\n",
    "            \"format. Switch to `channels_last` by editing your local \"\n",
    "            \"config file at ~/.keras/keras.json\"\n",
    "        )\n",
    "    if not (weights in {\"imagenet\", None} or file_utils.exists(weights)):\n",
    "        raise ValueError(\n",
    "            \"The `weights` argument should be either \"\n",
    "            \"`None` (random initialization), `imagenet` \"\n",
    "            \"(pre-training on ImageNet), \"\n",
    "            \"or the path to the weights file to be loaded.\"\n",
    "        )\n",
    "\n",
    "    if weights == \"imagenet\" and include_top and classes != 1000:\n",
    "        raise ValueError(\n",
    "            'If using `weights` as `\"imagenet\"` with `include_top`'\n",
    "            \" as true, `classes` should be 1000\"\n",
    "        )\n",
    "\n",
    "    bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n",
    "\n",
    "    x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(input_tensor)\n",
    "    x = layers.Conv2D(64, 7, strides=2, use_bias=False, name=\"conv1_conv\")(x)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis, epsilon=1.001e-5, name=\"conv1_bn\"\n",
    "    )(x)\n",
    "    x = layers.Activation(\"relu\", name=\"conv1_relu\")(x)\n",
    "    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, name=\"pool1\")(x)\n",
    "\n",
    "    x = dense_block(x, blocks[0], name=\"conv2\")\n",
    "    x = transition_block(x, 0.5, name=\"pool2\")\n",
    "    x = dense_block(x, blocks[1], name=\"conv3\")\n",
    "    x = transition_block(x, 0.5, name=\"pool3\")\n",
    "    x = dense_block(x, blocks[2], name=\"conv4\")\n",
    "    x = transition_block(x, 0.5, name=\"pool4\")\n",
    "    x = dense_block(x, blocks[3], name=\"conv5\")\n",
    "\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=\"bn\")(x)\n",
    "    x = layers.Activation(\"relu\", name=\"relu\")(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "\n",
    "        imagenet_utils.validate_activation(classifier_activation, weights)\n",
    "        x = layers.Dense(\n",
    "            classes, activation=classifier_activation, name=\"predictions\"\n",
    "        )(x)\n",
    "    else:\n",
    "        if pooling == \"avg\":\n",
    "            x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "        elif pooling == \"max\":\n",
    "            x = layers.GlobalMaxPooling2D(name=\"max_pool\")(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = operation_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = input_tensor\n",
    "\n",
    "    # Create model.\n",
    "    if blocks == [6, 12, 24, 16]:\n",
    "        model = Functional(inputs, x, name=\"densenet121\")\n",
    "    elif blocks == [6, 12, 32, 32]:\n",
    "        model = Functional(inputs, x, name=\"densenet169\")\n",
    "    elif blocks == [6, 12, 48, 32]:\n",
    "        model = Functional(inputs, x, name=\"densenet201\")\n",
    "    else:\n",
    "        model = Functional(inputs, x, name=\"densenet\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # カスタム入力層\n",
    "    input_shape = (INPUT_WIDTH, INPUT_HEIGHT, IN_CHANS)\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # DenseNet201ベースモデル（トップ層なし）\n",
    "    base_model = DenseNet(\n",
    "        [6, 12, 48, 32],\n",
    "        include_top=False,\n",
    "        input_tensor=inputs,\n",
    "        input_shape=input_shape,\n",
    "        pooling='avg'\n",
    "    )\n",
    "\n",
    "    # カスタム出力層（75クラスのマルチラベル分類）\n",
    "    outputs = Dense(75, activation='sigmoid')(base_model.output)  # マルチラベル分類のためにシグモイド関数を使用\n",
    "\n",
    "    # モデルの作成\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # モデルのコンパイル\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T12:13:13.409280Z",
     "start_time": "2024-06-26T12:13:13.405593Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomCheckpoint(Callback):\n",
    "    def __init__(self, filepath):\n",
    "        super(CustomCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.best_val_accuracy = -1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_accuracy = logs.get('val_accuracy')\n",
    "        \n",
    "        if val_accuracy is not None and val_accuracy > self.best_val_accuracy:\n",
    "            self.best_val_accuracy = val_accuracy\n",
    "            filepath = f'{self.filepath}_epoch{epoch + 1}_val_acc{val_accuracy:.4f}.keras'\n",
    "            self.model.save(filepath, overwrite=True)\n",
    "            print(f'Saved model to {filepath} with validation accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# カスタムチェックポイントのコールバックを作成\n",
    "custom_checkpoint = CustomCheckpoint(filepath='model_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-25T14:51:06.540050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m1:14\u001b[0m 11s/step - accuracy: 0.3267 - loss: 0.3353"
     ]
    }
   ],
   "source": [
    "model.load_weights('model.keras')\n",
    "# モデルの学習\n",
    "history = model.fit(\n",
    "    generator(BATCH_SIZE, train_study_ids, is_augmentation=True, is_shuffle=True),\n",
    "    steps_per_epoch=len(train_study_ids) // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=generator(BATCH_SIZE, valid_study_ids),\n",
    "    validation_steps=len(valid_study_ids) // BATCH_SIZE,\n",
    "    callbacks=[custom_checkpoint]\n",
    ")\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの評価\n",
    "score = model.evaluate(generator(BATCH_SIZE, test_study_ids), steps=len(test_study_ids) // BATCH_SIZE)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_path)\n",
    "\n",
    "# test_study_ids を元に予測\n",
    "for x, y in generator(1, test_study_ids):\n",
    "    y_pred = model.predict(x)\n",
    "    \n",
    "    diff = np.abs(y_pred - y)\n",
    "    \n",
    "    for i in range(25):\n",
    "        print(f'Pred : {y_pred[0][i]:.3f}, True : {y[0][i]:.3f}, Diff : {diff[0][i]:.3f}')\n",
    "    print(diff.shape)\n",
    "    # 差をグラフで表示\n",
    "    plt.plot(diff[0], color='red', label='diff')\n",
    "    \n",
    "    # 線は点線\n",
    "    plt.plot(y[0], color='blue', label='true', linestyle='dashed')\n",
    "    plt.plot(y_pred[0], color='green', label='pred', linestyle='dashed')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
