{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:22:29.179260Z",
     "start_time": "2024-06-29T10:22:26.177196Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "from keras import Model, Input\n",
    "from keras.src import backend\n",
    "from keras.src import layers\n",
    "from keras.src.applications import imagenet_utils\n",
    "from keras.src.layers import Dense\n",
    "from keras.src.models import Functional\n",
    "from keras.src.ops import operation_utils\n",
    "from keras.src.utils import file_utils\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.src.applications.densenet import DenseNet201\n",
    "from keras.src.callbacks import Callback\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:22:29.186338Z",
     "start_time": "2024-06-29T10:22:29.183106Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用可能なGPUデバイスのリストを取得\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if len(physical_devices) > 0:\n",
    "    for device in physical_devices:\n",
    "        # メモリの動的割り当てを有効にする\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "        # 初期化時に割り当てるメモリ量を制限するための設定\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            device,\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]  # 1024MB = 1GB\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            '{} memory growth: {}'.format(device, tf.config.experimental.get_memory_growth(device)))\n",
    "        print('{} memory limit: {} MB'.format(device, 1024))\n",
    "else:\n",
    "    print(\"Not enough GPU hardware devices available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:22:29.221553Z",
     "start_time": "2024-06-29T10:22:29.187309Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = './rsna-2024-lumbar-spine-degenerative-classification'\n",
    "train_png_file_path = f'{base_path}/train_dataset_png'\n",
    "\n",
    "label_coordinates_df = pd.read_csv(f'{base_path}/train_label_coordinates.csv')\n",
    "train_series = pd.read_csv(f'{base_path}/train_series_descriptions.csv')\n",
    "df_train = pd.read_csv(f'{base_path}/train.csv')\n",
    "df_sub = pd.read_csv(f'{base_path}/sample_submission.csv')\n",
    "test_series = pd.read_csv(f'{base_path}/test_series_descriptions.csv')\n",
    "\n",
    "train_dicom = f'{base_path}/train_images'\n",
    "test_dicom = f'{base_path}/test_images'\n",
    "\n",
    "model_path = f'model.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:22:29.431974Z",
     "start_time": "2024-06-29T10:22:29.222376Z"
    }
   },
   "outputs": [],
   "source": [
    "# 一つのstudy_idにseries_id が3つ未満のstudy_idを探す\n",
    "study_id_list = train_series['study_id'].unique()\n",
    "for study_id in study_id_list:\n",
    "    series_id_list = train_series[train_series['study_id'] == study_id][\n",
    "        'series_description'].unique()\n",
    "\n",
    "    if len(series_id_list) < 3:\n",
    "        print(f'study_id : {study_id}, series_id : {series_id_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:22:29.437183Z",
     "start_time": "2024-06-29T10:22:29.433809Z"
    }
   },
   "outputs": [],
   "source": [
    "# パラメータ\n",
    "STUDY_NUM = 1975\n",
    "\n",
    "# 出力のラベル\n",
    "NORMAL_MILD = 0\n",
    "MODERATE = 1\n",
    "SEVERE = 2\n",
    "\n",
    "# 重症度一覧\n",
    "SEVERITY_LIST = ['Normal/Mild', 'Moderate', 'Severe']\n",
    "\n",
    "# dicomデータの種類\n",
    "SERIES_DESCRIPTION_LIST = train_series['series_description'].unique().tolist()\n",
    "# 学習用パラメータ\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "TRAIN_RATIO = 0.8\n",
    "VALID_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "# 入出力の形状 (DenseNet201の入力サイズ)\n",
    "INPUT_WIDTH = 224\n",
    "INPUT_HEIGHT = 224\n",
    "INPUT_CHANNEL_DICT = {\n",
    "    'Sagittal T1': 17,\n",
    "    'Sagittal T2/STIR': 17,\n",
    "    'Axial T2': 29\n",
    "}\n",
    "IN_CHANS = sum(INPUT_CHANNEL_DICT.values())\n",
    "IMG_SIZE = [INPUT_WIDTH, INPUT_HEIGHT]\n",
    "\n",
    "CONDITIONS = [\n",
    "    'spinal_canal_stenosis',\n",
    "    'left_neural_foraminal_narrowing',\n",
    "    'right_neural_foraminal_narrowing',\n",
    "    'left_subarticular_stenosis',\n",
    "    'right_subarticular_stenosis'\n",
    "]\n",
    "\n",
    "LEVELS = [\n",
    "    'l1_l2',\n",
    "    'l2_l3',\n",
    "    'l3_l4',\n",
    "    'l4_l5',\n",
    "    'l5_s1',\n",
    "]\n",
    "\n",
    "N_LABELS = 25\n",
    "N_CLASSES = 3 * N_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:22:29.443473Z",
     "start_time": "2024-06-29T10:22:29.437907Z"
    }
   },
   "outputs": [],
   "source": [
    "_target_name = \"right_subarticular_stenosis_l5_s1\"\n",
    "\n",
    "print(f'train shape : {df_train.shape}')\n",
    "print(df_train[_target_name].unique())\n",
    "\n",
    "# それぞれの重症度の数がいくらあるか計算\n",
    "print(f'Normal/Mild : {df_train[df_train[_target_name] == str(\"Normal/Mild\")].shape[0]}')\n",
    "print(f'Moderate : {df_train[df_train[_target_name] == str(\"Moderate\")].shape[0]}')\n",
    "print(f'Severe : {df_train[df_train[_target_name] == str(\"Severe\")].shape[0]}')\n",
    "\n",
    "print(\n",
    "    f'Normal/Mild ratio : {df_train[df_train[_target_name] == str(\"Normal/Mild\")].shape[0] / df_train.shape[0]}')\n",
    "print(\n",
    "    f'Moderate ratio : {df_train[df_train[_target_name] == str(\"Moderate\")].shape[0] / df_train.shape[0]}')\n",
    "print(\n",
    "    f'Severe ratio : {df_train[df_train[_target_name] == str(\"Severe\")].shape[0] / df_train.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:22:29.454747Z",
     "start_time": "2024-06-29T10:22:29.444252Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "\n",
    "def get_train_label(study_id):\n",
    "    \"\"\"\n",
    "    study_id に対応するラベルを取得 (testはデータがないのでtrainのみ)\n",
    "    \"\"\"\n",
    "\n",
    "    row = df_train[df_train['study_id'] == study_id].to_numpy()[0][1:].tolist()\n",
    "\n",
    "    row_category = np.zeros((N_LABELS, 3))\n",
    "    for i in range(N_LABELS):\n",
    "        if row[i] == 'Normal/Mild':\n",
    "            row_category[i][0] = 1\n",
    "        elif row[i] == 'Moderate':\n",
    "            row_category[i][1] = 1\n",
    "        elif row[i] == 'Severe':\n",
    "            row_category[i][2] = 1\n",
    "    # 1次元配列に変換\n",
    "    row_category = row_category.flatten()\n",
    "    return row_category\n",
    "\n",
    "\n",
    "def get_file_list(study_id, series_desc, file_kind='train'):\n",
    "    \"\"\"\n",
    "    study_id に対応するファイルリストを取得\n",
    "    \"\"\"\n",
    "    dicom_file_list = []\n",
    "    df = None\n",
    "    if file_kind == 'train':\n",
    "        df = train_series\n",
    "    elif file_kind == 'test':\n",
    "        df = test_series\n",
    "\n",
    "    # study_id に対応する series_desc のリストを取得\n",
    "    series_ids = df[df['study_id'] == study_id]\n",
    "    series_id = series_ids[series_ids['series_description'] == series_desc]['series_id'].values\n",
    "    if len(series_id) == 0:\n",
    "        return dicom_file_list\n",
    "\n",
    "    series_id = series_id[0]\n",
    "\n",
    "    target_dicom_dir_path = ''\n",
    "    if file_kind == 'train':\n",
    "        target_dicom_dir_path = f'{train_dicom}/{study_id}/{series_id}/'\n",
    "    elif file_kind == 'test':\n",
    "        target_dicom_dir_path = f'{test_dicom}/{study_id}/{series_id}/'\n",
    "    dicom_file_list = glob(target_dicom_dir_path + '/*.dcm')\n",
    "    # ファイル名が数字になっているのでソート\n",
    "    dicom_file_list = sorted(dicom_file_list, key=natural_keys)\n",
    "\n",
    "    # series_desc にt対応するファイルリストを取得\n",
    "    return dicom_file_list\n",
    "\n",
    "\n",
    "def load_dicom_img(dicom_filepath):\n",
    "    \"\"\"\n",
    "    dicomファイルを読み込む\n",
    "    \"\"\"\n",
    "    dicom_data = dicom.read_file(dicom_filepath).pixel_array\n",
    "\n",
    "    # 0-1に正規化\n",
    "    dicom_data = dicom_data / np.max(dicom_data)\n",
    "\n",
    "    # 0~255に変換\n",
    "    dicom_data = dicom_data * 255\n",
    "\n",
    "    # 512x512にリサイズ\n",
    "    dicom_data = cv2.resize(dicom_data, (INPUT_WIDTH, INPUT_HEIGHT))\n",
    "\n",
    "    # uint8 に変換\n",
    "    dicom_data = dicom_data.astype(np.uint8)\n",
    "\n",
    "    return dicom_data\n",
    "\n",
    "\n",
    "def get_dicom_input_data(study_id, file_kind='train'):\n",
    "    \"\"\"\n",
    "    study_id に対応するdicomデータを取得\n",
    "    \"\"\"\n",
    "\n",
    "    input_dicom = np.zeros((INPUT_WIDTH, INPUT_HEIGHT, IN_CHANS))\n",
    "    # Sagittal T1\n",
    "    sagittal_t1_dicom_list = get_file_list(study_id, 'Sagittal T1', file_kind)\n",
    "    if len(sagittal_t1_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Sagittal T1']):\n",
    "            if i < len(sagittal_t1_dicom_list):\n",
    "                dicom_data = load_dicom_img(sagittal_t1_dicom_list[i])\n",
    "                input_dicom[:, :, i] = dicom_data\n",
    "\n",
    "    # Sagittal T2/STIR\n",
    "    sagittal_t2_dicom_list = get_file_list(study_id, 'Sagittal T2/STIR', file_kind)\n",
    "    if len(sagittal_t2_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Sagittal T2/STIR']):\n",
    "            if i < len(sagittal_t2_dicom_list):\n",
    "                dicom_data = load_dicom_img(sagittal_t2_dicom_list[i])\n",
    "                input_dicom[:, :, i + INPUT_CHANNEL_DICT['Sagittal T1']] = dicom_data\n",
    "\n",
    "    # Axial T2\n",
    "    axial_t2_dicom_list = get_file_list(study_id, 'Axial T2', file_kind)\n",
    "    if len(axial_t2_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Axial T2']):\n",
    "            if i < len(axial_t2_dicom_list):\n",
    "                dicom_data = load_dicom_img(axial_t2_dicom_list[i])\n",
    "                input_dicom[:, :, i + INPUT_CHANNEL_DICT['Sagittal T1'] + INPUT_CHANNEL_DICT[\n",
    "                    'Sagittal T2/STIR']] = dicom_data\n",
    "\n",
    "    # uint8 に変換\n",
    "    input_dicom = input_dicom.astype(np.int8)\n",
    "    return input_dicom\n",
    "\n",
    "\n",
    "def get_dicom_and_label(study_id, file_kind):\n",
    "    x_data = get_dicom_input_data(study_id, file_kind)\n",
    "    y_data = get_train_label(study_id)\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "def generator(batch_size, study_ids, file_kind='train', is_augmentation=False, is_shuffle=False):\n",
    "    \"\"\"\n",
    "    データセットの読み込み関数\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        if is_shuffle:\n",
    "            np.random.shuffle(study_ids)\n",
    "\n",
    "        for start in range(0, len(study_ids), batch_size):\n",
    "            gc.collect()\n",
    "\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(study_ids))\n",
    "            ids_train_batch = study_ids[start:end]\n",
    "\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                results = list(\n",
    "                    executor.map(lambda sid: get_dicom_and_label(sid, file_kind), ids_train_batch))\n",
    "\n",
    "            for x_data, y_data in results:\n",
    "                if is_augmentation:\n",
    "                    # 画像データにノイズを加える\n",
    "                    x_data = x_data + np.random.normal(0, 0.1, x_data.shape)\n",
    "\n",
    "                x_batch.append(x_data)\n",
    "                y_batch.append(y_data)\n",
    "\n",
    "            yield np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "# 学習用データの study_id を取得\n",
    "def get_train_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 0.8 で分割\n",
    "    train_study_ids = _study_ids[:int(len(_study_ids) * 0.8)]\n",
    "    return train_study_ids\n",
    "\n",
    "\n",
    "# 検証用データの study_id を取得\n",
    "def get_valid_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 0.8 で分割\n",
    "    valid_study_ids = _study_ids[int(len(_study_ids) * 0.8):int(len(_study_ids) * 0.9)]\n",
    "    return valid_study_ids\n",
    "\n",
    "\n",
    "# テスト用データの study_id を取得 (実際のテストデータではなく, 学習データの一部をテストデータとして使用)\n",
    "def get_test_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "    # 0.8 で分割\n",
    "    test_study_ids = _study_ids[int(len(_study_ids) * 0.9):]\n",
    "    return test_study_ids\n",
    "\n",
    "\n",
    "def generator_for_test(batch_size, study_ids):\n",
    "    \"\"\"\n",
    "    test_seriesからテスト用のデータセット作成\n",
    "    kaggle で提出時に呼び出される\n",
    "    :param batch_size: \n",
    "    :param study_ids: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    for start in range(0, len(study_ids), batch_size):\n",
    "        x_batch = []\n",
    "        end = min(start + batch_size, len(study_ids))\n",
    "        ids_train_batch = study_ids[start:end]\n",
    "\n",
    "        results = []\n",
    "        for sid in ids_train_batch:\n",
    "            results.append(get_dicom_input_data(sid, 'train'))\n",
    "        for x_data in results:\n",
    "            x_batch.append(x_data)\n",
    "\n",
    "        yield np.array(x_batch), ids_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:22:29.459064Z",
     "start_time": "2024-06-29T10:22:29.455457Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2492114990\n",
    "for desc in SERIES_DESCRIPTION_LIST:\n",
    "    dicom_image_list = get_file_list(2492114990, desc)\n",
    "    print(f'{desc} : {len(dicom_image_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:22:29.462379Z",
     "start_time": "2024-06-29T10:22:29.459686Z"
    }
   },
   "outputs": [],
   "source": [
    "# study_id のリストを作成\n",
    "train_study_ids = get_train_study_id_array()\n",
    "print(len(train_study_ids))\n",
    "valid_study_ids = get_valid_study_id_array()\n",
    "print(len(valid_study_ids))\n",
    "test_study_ids = get_test_study_id_array()\n",
    "print(len(test_study_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:22:29.466873Z",
     "start_time": "2024-06-29T10:22:29.463067Z"
    }
   },
   "outputs": [],
   "source": [
    "get_train_label(4003253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:22:29.813074Z",
     "start_time": "2024-06-29T10:22:29.467477Z"
    }
   },
   "outputs": [],
   "source": [
    "dicom_image_list = get_file_list(4003253, 'Sagittal T1')\n",
    "\n",
    "input_image = get_dicom_input_data(4003253)\n",
    "plt.imshow(input_image[:, :, 0], cmap='gray')\n",
    "input_data_gb = input_image.nbytes / 1024 / 1024 / 1024\n",
    "print(f\"画像サイズ[GB] : {input_data_gb}\")\n",
    "print(f'データセット全体のデータ数は {input_data_gb * STUDY_NUM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:22:32.518243Z",
     "start_time": "2024-06-29T10:22:29.814911Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def total_acc(y_true, y_pred):\n",
    "    pred = tf.dtypes.cast(tf.math.greater(y_pred, 0.5), tf.float64)\n",
    "    flag = tf.dtypes.cast(tf.math.equal(y_true, pred), tf.float64)\n",
    "    return tf.reduce_prod(flag, axis=-1)\n",
    "\n",
    "\n",
    "def binary_acc(y_true, y_pred):\n",
    "    pred = tf.dtypes.cast(tf.math.greater(y_pred, 0.5), tf.float64)\n",
    "    flag = tf.dtypes.cast(tf.math.equal(y_true, pred), tf.float64)\n",
    "    return tf.reduce_mean(flag, axis=-1)\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # カスタム入力層\n",
    "    input_shape = (INPUT_WIDTH, INPUT_HEIGHT, IN_CHANS)\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # channel を 3 に変換\n",
    "    x = layers.Conv2D(3, (1, 1), padding='same')(inputs)\n",
    "\n",
    "    # DenseNet201ベースモデル（トップ層なし）\n",
    "    base_model = DenseNet201(\n",
    "        include_top=True,\n",
    "        weights=None,\n",
    "        input_tensor=x,\n",
    "        input_shape=(INPUT_WIDTH, INPUT_HEIGHT, 3),\n",
    "        pooling=None,\n",
    "        classes=75,\n",
    "        classifier_activation='sigmoid'\n",
    "    )\n",
    "\n",
    "    # モデルの作成\n",
    "    model = Model(inputs=inputs, outputs=base_model.output)\n",
    "\n",
    "    # モデルのコンパイル\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[binary_acc, total_acc])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:22:32.523725Z",
     "start_time": "2024-06-29T10:22:32.520457Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomCheckpoint(Callback):\n",
    "    def __init__(self, filepath):\n",
    "        super(CustomCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.best_val_accuracy = -1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_binary_acc = logs.get('val_binary_acc')\n",
    "\n",
    "        if val_binary_acc is not None and val_binary_acc > self.best_val_accuracy and val_binary_acc > 0.4:\n",
    "            self.best_val_accuracy = val_binary_acc\n",
    "            filepath = f'{self.filepath}_epoch{epoch + 1}_val_acc{val_binary_acc:.4f}.keras'\n",
    "            self.model.save(filepath, overwrite=True)\n",
    "            print(f'Saved model to {filepath} with validation accuracy: {val_binary_acc:.4f}')\n",
    "        else:\n",
    "            # 10 epoch ごとにモデルを保存\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                filepath = f'{self.filepath}_epoch{epoch + 1}_val_acc{val_binary_acc:.4f}.keras'\n",
    "                self.model.save(filepath, overwrite=True)\n",
    "                print(f'Saved model to {filepath} with validation accuracy: {val_binary_acc:.4f}')\n",
    "\n",
    "\n",
    "# カスタムチェックポイントのコールバックを作成\n",
    "custom_checkpoint = CustomCheckpoint(filepath='model_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:20:35.789209Z",
     "start_time": "2024-06-29T10:20:35.789114Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルの学習\n",
    "history = model.fit(\n",
    "    generator(BATCH_SIZE, train_study_ids, is_augmentation=True, is_shuffle=True),\n",
    "    steps_per_epoch=len(train_study_ids) // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=generator(BATCH_SIZE, valid_study_ids),\n",
    "    validation_steps=len(valid_study_ids) // BATCH_SIZE,\n",
    "    callbacks=[custom_checkpoint]\n",
    ")\n",
    "\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:22:36.343161Z",
     "start_time": "2024-06-29T10:22:32.524572Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "loaded_model = tf.keras.models.load_model(model_path, custom_objects={\n",
    "    'binary_acc': binary_acc,\n",
    "    'total_acc': total_acc\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの評価\n",
    "score = loaded_model.evaluate(generator(1, test_study_ids), steps=len(test_study_ids) // BATCH_SIZE)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:25:45.381193Z",
     "start_time": "2024-06-29T10:25:44.740766Z"
    }
   },
   "outputs": [],
   "source": [
    "# テストデータの確認\n",
    "\n",
    "y_pred_list = []\n",
    "for x, sids in generator_for_test(BATCH_SIZE, test_study_ids[:BATCH_SIZE]):\n",
    "    y = loaded_model.predict(x)\n",
    "    for i in range(min(BATCH_SIZE, y.shape[0])):\n",
    "        y_pred_list.append((y[i], sids[i]))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T10:50:38.949215Z",
     "start_time": "2024-06-29T10:50:38.942223Z"
    }
   },
   "outputs": [],
   "source": [
    "diff_list = []\n",
    "for y_pred, sid in y_pred_list:\n",
    "    y_true = get_train_label(sid)\n",
    "    \n",
    "    # (75,) -> (25, 3)\n",
    "    y_pred = y_pred.reshape((25, 3))\n",
    "    y_true = y_true.reshape((25, 3))\n",
    "\n",
    "    # 予測結果をsoftmax関数で確率に変換\n",
    "    y_pred = np.exp(y_pred) / np.sum(np.exp(y_pred), axis=1).reshape(-1, 1)\n",
    "    \n",
    "    # 小数点第3位まで表示\n",
    "    y_pred = np.round(y_pred, 3)\n",
    "    \n",
    "    for i in range(25):\n",
    "        print(f'{sid}_{CONDITIONS[i // 5]}_{LEVELS[i % 5]}')\n",
    "        print(f'  y_true   : {y_true[i]}')\n",
    "        print(f'  y_pred   : {y_pred[i]}')\n",
    "        print(f'  diff     : {np.abs(y_true[i] - y_pred[i])}')\n",
    "        print(f'  sum-diff : {np.sum(np.abs(y_true[i] - y_pred[i]))}')\n",
    "        \n",
    "        diff_list.append(np.sum(np.abs(y_true[i] - y_pred[i])))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T16:30:29.823895Z",
     "start_time": "2024-06-28T16:28:17.095108Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_dataset_ids = test_series['study_id'].unique()\n",
    "test_dataset_ids = train_series['study_id'].unique()\n",
    "\n",
    "row_names = []\n",
    "y_preds = []\n",
    "submission_labels = ['normal_mild', 'moderate', 'severe']\n",
    "# test_study_ids を元に予測\n",
    "count = 0\n",
    "for x, sids in generator_for_test(BATCH_SIZE, test_dataset_ids):\n",
    "    count += BATCH_SIZE\n",
    "    # 残りのデータ数\n",
    "    print(f'remaining data : {len(test_dataset_ids) - count}')\n",
    "\n",
    "    y = loaded_model.predict(x)\n",
    "    for i in range(min(BATCH_SIZE, y.shape[0])):\n",
    "        y_pred = y[i]\n",
    "\n",
    "        # 予測結果を (25, 3) に変換\n",
    "        y_pred = y_pred.reshape((25, 3))\n",
    "\n",
    "        # 予測結果をsoftmax関数で確率に変換\n",
    "        y_pred = np.exp(y_pred) / np.sum(np.exp(y_pred), axis=1).reshape(-1, 1)\n",
    "\n",
    "        # 小数点第3位まで表示\n",
    "        y_pred = np.round(y_pred, 3)\n",
    "\n",
    "        y_preds.append(y_pred)\n",
    "\n",
    "        # row の名前を作成\n",
    "        for cond in CONDITIONS:\n",
    "            for level in LEVELS:\n",
    "                row_names.append(f'{sids[i]}_{cond}_{level}')\n",
    "y_preds = np.concatenate(y_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T16:30:29.825252Z",
     "start_time": "2024-06-28T16:30:29.825165Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(submission_labels))\n",
    "print(y_preds.shape)\n",
    "print(len(row_names))\n",
    "submission_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['row_id'] = row_names\n",
    "sub[submission_labels] = y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
