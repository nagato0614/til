{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:06.577658Z",
     "start_time": "2024-06-29T13:35:04.465667Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "from keras import Model, Input\n",
    "from keras.src import backend\n",
    "from keras.src import layers\n",
    "from keras.src.applications import imagenet_utils\n",
    "from keras.src.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.src.models import Functional\n",
    "from keras.src.ops import operation_utils\n",
    "from keras.src.utils import file_utils\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.src.applications.densenet import DenseNet201\n",
    "from keras.src.callbacks import Callback\n",
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.ops import clip_ops, math_ops\n",
    "from scipy.ndimage import gaussian_filter, rotate, zoom\n",
    "import albumentations as A\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:06.622692Z",
     "start_time": "2024-06-29T13:35:06.578802Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = '/home/toru/PycharmProjects/rsna-2024-lumbar-spine-degenerative-classification'\n",
    "train_png_file_path = f'{base_path}/train_dataset_png'\n",
    "\n",
    "label_coordinates_df = pd.read_csv(f'{base_path}/train_label_coordinates.csv')\n",
    "train_series = pd.read_csv(f'{base_path}/train_series_descriptions.csv')\n",
    "df_train = pd.read_csv(f'{base_path}/train.csv')\n",
    "df_sub = pd.read_csv(f'{base_path}/sample_submission.csv')\n",
    "test_series = pd.read_csv(f'{base_path}/test_series_descriptions.csv')\n",
    "\n",
    "train_dicom = f'{base_path}/train_images'\n",
    "test_dicom = f'{base_path}/test_images'\n",
    "\n",
    "model_path = f'model.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.021986Z",
     "start_time": "2024-06-29T13:35:06.623607Z"
    }
   },
   "outputs": [],
   "source": [
    "# 一つのstudy_idにseries_id が3つ未満のstudy_idを探す\n",
    "study_id_list = train_series['study_id'].unique()\n",
    "for study_id in study_id_list:\n",
    "    series_id_list = train_series[train_series['study_id'] == study_id][\n",
    "        'series_description'].unique()\n",
    "\n",
    "    if len(series_id_list) < 3:\n",
    "        print(f'study_id : {study_id}, series_id : {series_id_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.027414Z",
     "start_time": "2024-06-29T13:35:07.023661Z"
    }
   },
   "outputs": [],
   "source": [
    "# パラメータ\n",
    "STUDY_NUM = 1975\n",
    "\n",
    "# 出力のラベル\n",
    "NORMAL_MILD = 0\n",
    "MODERATE = 1\n",
    "SEVERE = 2\n",
    "\n",
    "# 重症度一覧\n",
    "SEVERITY_LIST = ['Normal/Mild', 'Moderate', 'Severe']\n",
    "\n",
    "# dicomデータの種類\n",
    "SERIES_DESCRIPTION_LIST = train_series['series_description'].unique().tolist()\n",
    "# 学習用パラメータ\n",
    "BATCH_SIZE = 4\n",
    "VAL_BATCH_SIZE = BATCH_SIZE * 2\n",
    "EPOCHS = 1000\n",
    "PATIENCE = 1 # 早期終了のパラメータ\n",
    "TRAIN_RATIO = 0.95\n",
    "VALID_RATIO = 0.025\n",
    "TEST_RATIO = 0.025\n",
    "Lr = 1e-5 * (1.0 / BATCH_SIZE)\n",
    "N_FOLDS = 5 # 交差検証の分割数\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "# 入出力の形状 (DenseNet201の入力サイズ)\n",
    "INPUT_WIDTH = 512\n",
    "INPUT_HEIGHT = 512\n",
    "INPUT_CHANNEL_DICT = {\n",
    "    'Sagittal T1': 10,\n",
    "    'Sagittal T2/STIR': 10,\n",
    "    'Axial T2': 10,\n",
    "}\n",
    "IN_CHANS = sum(INPUT_CHANNEL_DICT.values())\n",
    "IMG_SIZE = [INPUT_WIDTH, INPUT_HEIGHT]\n",
    "\n",
    "CONDITIONS = [\n",
    "    'spinal_canal_stenosis',\n",
    "    'left_neural_foraminal_narrowing',\n",
    "    'right_neural_foraminal_narrowing',\n",
    "    'left_subarticular_stenosis',\n",
    "    'right_subarticular_stenosis'\n",
    "]\n",
    "\n",
    "LEVELS = [\n",
    "    'l1_l2',\n",
    "    'l2_l3',\n",
    "    'l3_l4',\n",
    "    'l4_l5',\n",
    "    'l5_s1',\n",
    "]\n",
    "\n",
    "N_LABELS = 25\n",
    "N_CLASSES = 3 * N_LABELS\n",
    "\n",
    "data_filenames = ['train_data', 'valid_data', 'test_data']  # パラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.036975Z",
     "start_time": "2024-06-29T13:35:07.028109Z"
    }
   },
   "outputs": [],
   "source": [
    "_target_name = \"right_subarticular_stenosis_l5_s1\"\n",
    "\n",
    "print(f'train shape : {df_train.shape}')\n",
    "print(df_train[_target_name].unique())\n",
    "\n",
    "# それぞれの重症度の数がいくらあるか計算\n",
    "print(f'Normal/Mild : {df_train[df_train[_target_name] == str(\"Normal/Mild\")].shape[0]}')\n",
    "print(f'Moderate : {df_train[df_train[_target_name] == str(\"Moderate\")].shape[0]}')\n",
    "print(f'Severe : {df_train[df_train[_target_name] == str(\"Severe\")].shape[0]}')\n",
    "\n",
    "print(\n",
    "    f'Normal/Mild ratio : {df_train[df_train[_target_name] == str(\"Normal/Mild\")].shape[0] / df_train.shape[0]}')\n",
    "print(\n",
    "    f'Moderate ratio : {df_train[df_train[_target_name] == str(\"Moderate\")].shape[0] / df_train.shape[0]}')\n",
    "print(\n",
    "    f'Severe ratio : {df_train[df_train[_target_name] == str(\"Severe\")].shape[0] / df_train.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.058616Z",
     "start_time": "2024-06-29T13:35:07.037734Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "\n",
    "def get_train_label(study_id):\n",
    "    \"\"\"\n",
    "    study_id に対応するラベルを取得 (testはデータがないのでtrainのみ)\n",
    "    \"\"\"\n",
    "\n",
    "    row = df_train[df_train['study_id'] == study_id].to_numpy()[0][1:].tolist()\n",
    "\n",
    "    row_category = np.zeros((N_LABELS, 3))\n",
    "    for i in range(N_LABELS):\n",
    "        if row[i] == 'Normal/Mild':\n",
    "            row_category[i][0] = 1\n",
    "        elif row[i] == 'Moderate':\n",
    "            row_category[i][1] = 1\n",
    "        elif row[i] == 'Severe':\n",
    "            row_category[i][2] = 1\n",
    "    # 1次元配列に変換\n",
    "    row_category = row_category.flatten()\n",
    "    return row_category\n",
    "\n",
    "\n",
    "def get_file_list(study_id, series_desc, file_kind='train'):\n",
    "    \"\"\"\n",
    "    study_id に対応するファイルリストを取得\n",
    "    \"\"\"\n",
    "    dicom_file_list = []\n",
    "    df = None\n",
    "    if file_kind == 'train':\n",
    "        df = train_series\n",
    "    elif file_kind == 'test':\n",
    "        df = test_series\n",
    "\n",
    "    # study_id に対応する series_desc のリストを取得\n",
    "    series_ids = df[df['study_id'] == study_id]\n",
    "    series_id = series_ids[series_ids['series_description'] == series_desc]['series_id'].values\n",
    "    if len(series_id) == 0:\n",
    "        return dicom_file_list\n",
    "\n",
    "    series_id = series_id[0]\n",
    "\n",
    "    target_dicom_dir_path = ''\n",
    "    if file_kind == 'train':\n",
    "        target_dicom_dir_path = f'{train_dicom}/{study_id}/{series_id}/'\n",
    "    elif file_kind == 'test':\n",
    "        target_dicom_dir_path = f'{test_dicom}/{study_id}/{series_id}/'\n",
    "    dicom_file_list = glob(target_dicom_dir_path + '/*.dcm')\n",
    "    # ファイル名が数字になっているのでソート\n",
    "    dicom_file_list = sorted(dicom_file_list, key=natural_keys)\n",
    "\n",
    "    # series_desc にt対応するファイルリストを取得\n",
    "    return dicom_file_list\n",
    "\n",
    "\n",
    "def load_dicom_img(dicom_filepath):\n",
    "    \"\"\"\n",
    "    dicomファイルを読み込む\n",
    "    \"\"\"\n",
    "    dicom_data = dicom.read_file(dicom_filepath).pixel_array\n",
    "\n",
    "    # マイナスの値があればオフセットを加える\n",
    "    if np.min(dicom_data) < 0:\n",
    "        dicom_data = dicom_data + np.abs(np.min(dicom_data))\n",
    "    # 0-1に正規化\n",
    "    dicom_data = (dicom_data - np.min(dicom_data)) / (np.max(dicom_data) - np.min(dicom_data) + 1e-6)\n",
    "\n",
    "    # 0-255に変換\n",
    "    dicom_data = dicom_data * 255\n",
    "\n",
    "    # 512x512にリサイズ\n",
    "    dicom_data = cv2.resize(dicom_data, (INPUT_WIDTH, INPUT_HEIGHT), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # uint8 に変換\n",
    "    dicom_data = dicom_data.astype(np.uint8)\n",
    "\n",
    "\n",
    "    return dicom_data\n",
    "\n",
    "\n",
    "def select_image(image_num, selected_count):\n",
    "    if selected_count <= 0:\n",
    "        raise ValueError(\"selected_count must be greater than 0.\")\n",
    "\n",
    "    if selected_count > image_num:\n",
    "        raise ValueError(\"selected_count cannot be greater than image_num.\")\n",
    "\n",
    "    interval = (image_num - 1) // (selected_count - 1)\n",
    "    selected_numbers = list(range(1, image_num + 1, interval))\n",
    "\n",
    "    if len(selected_numbers) > selected_count:\n",
    "        selected_numbers = selected_numbers[:selected_count]\n",
    "\n",
    "    return selected_numbers\n",
    "    \n",
    "\n",
    "\n",
    "def get_dicom_input_data(study_id, file_kind='train'):\n",
    "    \"\"\"\n",
    "    study_id に対応するdicomデータを取得\n",
    "    \"\"\"\n",
    "\n",
    "    input_dicom = np.zeros((INPUT_WIDTH, INPUT_HEIGHT, IN_CHANS))\n",
    "    # Sagittal T1\n",
    "    sagittal_t1_dicom_list = get_file_list(study_id, 'Sagittal T1', file_kind)\n",
    "    if len(sagittal_t1_dicom_list) > 0 and len(sagittal_t1_dicom_list) <= 10:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Sagittal T1']):\n",
    "            if i < len(sagittal_t1_dicom_list):\n",
    "                dicom_data = load_dicom_img(sagittal_t1_dicom_list[i])\n",
    "                input_dicom[:, :, i] = dicom_data\n",
    "    elif len(sagittal_t1_dicom_list) > 10:\n",
    "        selected_numbers = select_image(len(sagittal_t1_dicom_list), INPUT_CHANNEL_DICT['Sagittal T1'])\n",
    "        for i, selected_number in enumerate(selected_numbers):\n",
    "            dicom_data = load_dicom_img(sagittal_t1_dicom_list[selected_number - 1])\n",
    "            input_dicom[:, :, i] = dicom_data\n",
    "\n",
    "    # Sagittal T2/STIR\n",
    "    sagittal_t2_dicom_list = get_file_list(study_id, 'Sagittal T2/STIR', file_kind)\n",
    "    if len(sagittal_t2_dicom_list) > 0 and len(sagittal_t2_dicom_list) <= 10:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Sagittal T2/STIR']):\n",
    "            if i < len(sagittal_t2_dicom_list):\n",
    "                dicom_data = load_dicom_img(sagittal_t2_dicom_list[i])\n",
    "                input_dicom[:, :, i + INPUT_CHANNEL_DICT['Sagittal T1']] = dicom_data \n",
    "    elif len(sagittal_t2_dicom_list) > 10:\n",
    "        selected_numbers = select_image(len(sagittal_t2_dicom_list), INPUT_CHANNEL_DICT['Sagittal T2/STIR'])\n",
    "        for i, selected_number in enumerate(selected_numbers):\n",
    "            dicom_data = load_dicom_img(sagittal_t2_dicom_list[selected_number - 1])\n",
    "            input_dicom[:, :, i + INPUT_CHANNEL_DICT['Sagittal T1']] = dicom_data\n",
    "\n",
    "    # Axial T2\n",
    "    axial_t2_dicom_list = get_file_list(study_id, 'Axial T2', file_kind)\n",
    "    if len(axial_t2_dicom_list) > 0 and len(axial_t2_dicom_list) <= 10:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Axial T2']):\n",
    "            if i < len(axial_t2_dicom_list):\n",
    "                dicom_data = load_dicom_img(axial_t2_dicom_list[i])\n",
    "                input_dicom[:, :, i + INPUT_CHANNEL_DICT['Sagittal T1'] + INPUT_CHANNEL_DICT[\n",
    "                    'Sagittal T2/STIR']] = dicom_data\n",
    "    elif len(axial_t2_dicom_list) > 10:\n",
    "        selected_numbers = select_image(len(axial_t2_dicom_list), INPUT_CHANNEL_DICT['Axial T2'])\n",
    "        for i, selected_number in enumerate(selected_numbers):\n",
    "            dicom_data = load_dicom_img(axial_t2_dicom_list[selected_number - 1])\n",
    "            input_dicom[:, :, i + INPUT_CHANNEL_DICT['Sagittal T1'] + INPUT_CHANNEL_DICT[\n",
    "                'Sagittal T2/STIR']] = dicom_data\n",
    "                \n",
    "    # マイナスの値があればオフセットを加える\n",
    "    if np.min(input_dicom) < 0:\n",
    "        input_dicom = input_dicom + np.abs(np.min(input_dicom))\n",
    "\n",
    "    # 0-1に正規化\n",
    "    input_dicom = input_dicom / (np.max(input_dicom) + 1e-6)\n",
    "\n",
    "    # 0-255に変換\n",
    "    input_dicom = input_dicom * 255\n",
    "\n",
    "    # uint8 に変換\n",
    "    input_dicom = input_dicom.astype(np.uint8)\n",
    "\n",
    "    return input_dicom\n",
    "\n",
    "def get_dicom_and_label(study_id, file_kind):\n",
    "    x_data = get_dicom_input_data(study_id, file_kind)\n",
    "    y_data = get_train_label(study_id)\n",
    "    return x_data, y_data\n",
    "\n",
    "def process_one_batch(x_batch, \n",
    "                      y_batch,\n",
    "                      is_augmentation=False\n",
    "                      ):\n",
    "    \"\"\"\n",
    "    一つのバッチを処理する\n",
    "    \"\"\"\n",
    "\n",
    "    if is_augmentation:\n",
    "\n",
    "        # 画像を正規化. (-があるので, offstetを加える)\n",
    "        x_batch = x_batch.astype(np.float32)\n",
    "        if np.min(x_batch) < 0:\n",
    "            x_batch = x_batch + np.abs(np.min(x_batch))\n",
    "        x_batch = x_batch / (np.max(x_batch) + 1e-6)\n",
    "\n",
    "        for i in range(len(x_batch)):\n",
    "\n",
    "            # 明るさを変える\n",
    "            x_batch[i] = tf.image.adjust_brightness(x_batch[i], np.random.uniform(-0.5, 0.5))\n",
    "            \n",
    "            # ノイズを加える\n",
    "            x_batch[i] = x_batch[i] + np.random.normal(0, 0.1, x_batch[i].shape)\n",
    "            \n",
    "            # 画像をぼかす\n",
    "            # ランダムなシグマ値を生成\n",
    "            sigma = np.random.uniform(-2.0, 2.0)\n",
    "\n",
    "            # シグマ値の絶対値を取ってガウシアンブラーを適用\n",
    "            x_batch[i] = gaussian_filter(x_batch[i], sigma=np.abs(sigma))\n",
    "\n",
    "            # 特定の範囲を切り抜く (10 ~ 50)\n",
    "            n_cut = np.random.randint(5, 10)\n",
    "            for _ in range(n_cut):\n",
    "                WINDOW_SIZE = np.random.randint(10, 100)\n",
    "                x_min = np.random.randint(0, x_batch[i].shape[0] - WINDOW_SIZE)\n",
    "                x_max = x_min + WINDOW_SIZE\n",
    "                y_min = np.random.randint(0, x_batch[i].shape[1] - WINDOW_SIZE)\n",
    "                y_max = y_min + WINDOW_SIZE\n",
    "                x_batch[i][x_min:x_max, y_min:y_max] = 0\n",
    "\n",
    "            # 画像を回転\n",
    "            x_batch[i] = rotate(x_batch[i], np.random.randint(-30, 30), reshape=False)\n",
    "    \n",
    "            # 画像を上下, 左右にずらす\n",
    "            x_batch[i] = np.roll(x_batch[i], np.random.randint(-10, 10), axis=0)\n",
    "            x_batch[i] = np.roll(x_batch[i], np.random.randint(-10, 10), axis=1)\n",
    "\n",
    "    # 画像を正規化. (-があるので, offstetを加える)\n",
    "    x_batch = x_batch.astype(np.float32)\n",
    "    if np.min(x_batch) < 0:\n",
    "        x_batch = x_batch + np.abs(np.min(x_batch))\n",
    "    x_batch = x_batch / (np.max(x_batch) + 1e-6)\n",
    "\n",
    "    # (0, 255) に変換\n",
    "    x_batch = x_batch * 255\n",
    "\n",
    "    # uint8 に変換\n",
    "    x_batch = x_batch.astype(np.uint8)\n",
    "\n",
    "    return x_batch, y_batch\n",
    "\n",
    "def shuffle_data_and_labels(x_batch, y_batch):\n",
    "    # データとラベルをペアにしたリストを作成\n",
    "    data_label_pairs = list(zip(x_batch, y_batch))\n",
    "    # ペアのリストをシャッフル\n",
    "    np.random.shuffle(data_label_pairs)\n",
    "    # シャッフル後のデータとラベルをそれぞれ別々の配列に戻す\n",
    "    shuffled_x_batch, shuffled_y_batch = zip(*data_label_pairs)\n",
    "    # NumPy 配列に変換して返す（オプション）\n",
    "    shuffled_x_batch = np.array(shuffled_x_batch)\n",
    "    shuffled_y_batch = np.array(shuffled_y_batch)\n",
    "    return shuffled_x_batch, shuffled_y_batch\n",
    "\n",
    "def generator(batch_size, study_ids, file_kind='train', is_augmentation=False, is_shuffle=False):\n",
    "    \"\"\"\n",
    "    データセットの読み込み関数\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        if is_shuffle:\n",
    "            np.random.shuffle(study_ids)\n",
    "\n",
    "        for start in range(0, len(study_ids), batch_size):\n",
    "            gc.collect()\n",
    "\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(study_ids))\n",
    "            ids_train_batch = study_ids[start:end]\n",
    "\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                results = list(\n",
    "                    executor.map(lambda sid: get_dicom_and_label(sid, file_kind), ids_train_batch))\n",
    "\n",
    "            for x_data, y_data in results:\n",
    "                x_batch.append(x_data)\n",
    "                y_batch.append(y_data)\n",
    "\n",
    "            x_batch = np.array(x_batch, dtype=np.float64)  # x_batch を NumPy 配列に変換\n",
    "            y_batch = np.array(y_batch, dtype=np.float64)  # y_batch を NumPy 配列に変換\n",
    "\n",
    "            x_data, y_data = process_one_batch(x_batch, y_batch, is_augmentation)\n",
    "\n",
    "            yield x_data, y_data\n",
    "\n",
    "\n",
    "def generator_load_all(batch_size, study_ids, file_kind='train', is_augmentation=False, is_shuffle=False):\n",
    "    \"\"\"\n",
    "    実行時予めすべてのデータを読み込んだ上でデータを返す\n",
    "    \"\"\"\n",
    "    \n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    print(f'事前にすべてのデータを読み込みます')\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(\n",
    "            executor.map(lambda sid: get_dicom_and_label(sid, file_kind), study_ids))\n",
    "\n",
    "    for x_data, y_data in results:\n",
    "        x_batch.append(x_data)\n",
    "        y_batch.append(y_data)\n",
    "\n",
    "    # numpy配列に変換\n",
    "    x_batch = np.array(x_batch)\n",
    "    y_batch = np.array(y_batch)\n",
    "\n",
    "    print(f'データの読み込みが完了しました')\n",
    "    # データをバッチサイズに分割して返す\n",
    "    while True:\n",
    "        if is_shuffle:\n",
    "            x_batch, y_batch = shuffle_data_and_labels(x_batch, y_batch)\n",
    "        \n",
    "        for start in range(0, len(x_batch), batch_size):\n",
    "            end = min(start + batch_size, len(study_ids))\n",
    "\n",
    "            x_batch_batch = x_batch[start:end]\n",
    "            y_batch_batch = y_batch[start:end]\n",
    "\n",
    "            x_batch_batch, y_batch_batch = process_one_batch(x_batch_batch, y_batch_batch, is_augmentation)\n",
    "\n",
    "            yield np.array(x_batch_batch), np.array(y_batch_batch)\n",
    "    \n",
    "\n",
    "def save_train_data():\n",
    "    \"\"\"\n",
    "    学習用データを保存\n",
    "    \"\"\"\n",
    "    train_study_ids = get_train_study_id_array()\n",
    "    generate_and_save_data(train_study_ids, file_name='train_data', file_kind='train')\n",
    "\n",
    "\n",
    "def save_valid_data():\n",
    "    \"\"\"\n",
    "    検証用データを保存\n",
    "    \"\"\"\n",
    "    valid_study_ids = get_valid_study_id_array()\n",
    "    generate_and_save_data(valid_study_ids, file_name='valid_data', file_kind='train')\n",
    "\n",
    "\n",
    "def save_test_data():\n",
    "    \"\"\"\n",
    "    テスト用データを保存\n",
    "    \"\"\"\n",
    "    test_study_ids = get_test_study_id_array()\n",
    "    generate_and_save_data(test_study_ids, file_name='test_data', file_kind='train')\n",
    "\n",
    "\n",
    "# 学習用データの study_id を取得\n",
    "def get_train_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 0.8 で分割\n",
    "    train_study_ids = _study_ids[:int(len(_study_ids) * 0.8)]\n",
    "    return train_study_ids\n",
    "\n",
    "\n",
    "# 検証用データの study_id を取得\n",
    "def get_valid_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 0.8 で分割\n",
    "    valid_study_ids = _study_ids[int(len(_study_ids) * 0.8):int(len(_study_ids) * 0.9)]\n",
    "    return valid_study_ids\n",
    "\n",
    "\n",
    "# テスト用データの study_id を取得 (実際のテストデータではなく, 学習データの一部をテストデータとして使用)\n",
    "def get_test_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "    # 0.8 で分割\n",
    "    test_study_ids = _study_ids[int(len(_study_ids) * 0.9):]\n",
    "    return test_study_ids\n",
    "\n",
    "\n",
    "def generator_for_test(batch_size, study_ids):\n",
    "    \"\"\"\n",
    "    test_seriesからテスト用のデータセット作成\n",
    "    kaggle で提出時に呼び出される\n",
    "    :param batch_size: \n",
    "    :param study_ids: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    for start in range(0, len(study_ids), batch_size):\n",
    "        x_batch = []\n",
    "        end = min(start + batch_size, len(study_ids))\n",
    "        ids_train_batch = study_ids[start:end]\n",
    "\n",
    "        for sid in ids_train_batch:\n",
    "            dicom_img = get_dicom_input_data(sid, 'test')\n",
    "\n",
    "            x_batch.append(dicom_img)\n",
    "\n",
    "\n",
    "        yield np.array(x_batch), ids_train_batch\n",
    "\n",
    "\n",
    "def generate_and_save_data(study_ids, file_name, file_kind='train'):\n",
    "    \"\"\"\n",
    "    学習用データを生成して保存\n",
    "    \"\"\"\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for sid in study_ids:\n",
    "        print(f'残りのデータ数 : {len(study_ids) - len(x_batch)}')\n",
    "        x_data, y_data = get_dicom_and_label(sid, file_kind)\n",
    "        x_batch.append(x_data)\n",
    "        y_batch.append(y_data)\n",
    "\n",
    "    x_batch = np.array(x_batch)\n",
    "    y_batch = np.array(y_batch)\n",
    "\n",
    "    print(f'x_batch shape : {x_batch.shape}')\n",
    "    print(f'y_batch shape : {y_batch.shape}')\n",
    "\n",
    "    np.save(f'{file_name}_x.npy', x_batch)\n",
    "    np.save(f'{file_name}_y.npy', y_batch)\n",
    "\n",
    "    # メモリ解放\n",
    "    del x_batch\n",
    "    del y_batch\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    データを読み込む\n",
    "    \"\"\"\n",
    "    x_data = np.load(f'{file_name}_x.npy')\n",
    "    y_data = np.load(f'{file_name}_y.npy')\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "\n",
    "def generator_with_load_data(batch_size,\n",
    "                             study_ids,\n",
    "                             file_name,\n",
    "                             file_kind='train',\n",
    "                             is_augmentation=False,\n",
    "                             is_shuffle=False,\n",
    "                             is_only_one_epoch=False):\n",
    "    \"\"\"\n",
    "    事前にデータを読み込んでデータを返す\n",
    "    \"\"\"\n",
    "    x_batch, y_batch = load_data(file_name)\n",
    "\n",
    "    def shuffle_data_and_labels(x_batch, y_batch):\n",
    "        # データとラベルをペアにしたリストを作成\n",
    "        data_label_pairs = list(zip(x_batch, y_batch))\n",
    "\n",
    "        # ペアのリストをシャッフル\n",
    "        np.random.shuffle(data_label_pairs)\n",
    "\n",
    "        # シャッフル後のデータとラベルをそれぞれ別々の配列に戻す\n",
    "        shuffled_x_batch, shuffled_y_batch = zip(*data_label_pairs)\n",
    "\n",
    "        # NumPy 配列に変換して返す（オプション）\n",
    "        shuffled_x_batch = np.array(shuffled_x_batch)\n",
    "        shuffled_y_batch = np.array(shuffled_y_batch)\n",
    "\n",
    "        return shuffled_x_batch, shuffled_y_batch\n",
    "\n",
    "    while True:\n",
    "        # epochごとにシャッフル\n",
    "        if is_shuffle:\n",
    "            x_batch, y_batch = shuffle_data_and_labels(x_batch, y_batch)\n",
    "\n",
    "        for start in range(0, len(x_batch), batch_size):\n",
    "            end = min(start + batch_size, len(study_ids))\n",
    "\n",
    "            x_batch_batch = x_batch[start:end]\n",
    "            y_batch_batch = y_batch[start:end]\n",
    "\n",
    "            x_batch_batch, y_batch_batch = process_one_batch(x_batch_batch, y_batch_batch, is_augmentation)\n",
    "            \n",
    "            yield np.array(x_batch_batch), np.array(y_batch_batch)\n",
    "\n",
    "        if is_only_one_epoch:\n",
    "            break\n",
    "\n",
    "    # メモリ解放\n",
    "    del x_batch\n",
    "    del y_batch\n",
    "    gc.collect()\n",
    "\n",
    "def get_severity_rate(study_id):\n",
    "    \"\"\"\n",
    "    study_id に対応する重症度の割合を取得\n",
    "    \"\"\"\n",
    "    row = df_train[df_train['study_id'] == study_id].to_numpy()[0][1:].tolist()\n",
    "\n",
    "    # それぞれの重症度の数がいくらあるか計算\n",
    "    normal_mild = row.count('Normal/Mild')\n",
    "    moderate = row.count('Moderate')\n",
    "    severe = row.count('Severe')\n",
    "\n",
    "    # 重症度の割合を計算\n",
    "    normal_mild_rate = normal_mild / len(row)\n",
    "    moderate_rate = moderate / len(row)\n",
    "    severe_rate = severe / len(row)\n",
    "\n",
    "    return normal_mild_rate, moderate_rate, severe_rate\n",
    "\n",
    "def get_separated_studyid_lists():\n",
    "    \"\"\"\n",
    "    学習用に分割されたデータセットを取得する.\n",
    "    戻り値は, train, valid, test の3つのリスト\n",
    "    データの中の割合が 8:1:1 になるように分割する\n",
    "    重症度が均等に分布するようにデータを分割する\n",
    "    データセットには Normal/Mild, Moderate, Severe の3つの重症度がある\n",
    "    数としては Normal/Mild > Moderate > Severe となる\n",
    "    \"\"\"\n",
    "    study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # それぞれの重症度の数がいくらあるか計算\n",
    "    severity_rates = defaultdict(list)\n",
    "    for study_id in study_ids:\n",
    "        normal_mild_rate, moderate_rate, severe_rate = get_severity_rate(study_id)\n",
    "        severity_rates['Normal/Mild'].append(normal_mild_rate)\n",
    "        severity_rates['Moderate'].append(moderate_rate)\n",
    "        severity_rates['Severe'].append(severe_rate)\n",
    "\n",
    "    # 各リストをnumpy配列に変換\n",
    "    for key in severity_rates:\n",
    "        severity_rates[key] = np.array(severity_rates[key])\n",
    "\n",
    "    # 重症度の割合が均等になるようにstudy_idを分割\n",
    "    sorted_study_ids = sorted(study_ids, key=lambda x: (\n",
    "        severity_rates['Severe'][list(study_ids).index(x)],\n",
    "        severity_rates['Moderate'][list(study_ids).index(x)],\n",
    "        severity_rates['Normal/Mild'][list(study_ids).index(x)],\n",
    "    ))\n",
    "\n",
    "    # study_idをtrain, valid, testに分割\n",
    "    total_samples = len(sorted_study_ids)\n",
    "    train_end = int(total_samples * TRAIN_RATIO)\n",
    "    valid_end = int(total_samples * (TRAIN_RATIO + VALID_RATIO))\n",
    "\n",
    "    train_study_ids = []\n",
    "    valid_study_ids = []\n",
    "    test_study_ids = []\n",
    "\n",
    "    # 割合に応じで順番にデータを分割\n",
    "    for i, study_id in enumerate(sorted_study_ids):\n",
    "        # 0 ~ 1.0 の乱数を生成\n",
    "        while True:\n",
    "            rand = np.random.rand()\n",
    "\n",
    "            if rand < TRAIN_RATIO:\n",
    "                if len(train_study_ids) < train_end:\n",
    "                    train_study_ids.append(study_id)\n",
    "                    break\n",
    "            elif rand < TRAIN_RATIO + VALID_RATIO:\n",
    "                if len(valid_study_ids) < valid_end - train_end:\n",
    "                    valid_study_ids.append(study_id)\n",
    "                    break\n",
    "            else:\n",
    "                if len(test_study_ids) < total_samples - valid_end:\n",
    "                    test_study_ids.append(study_id)\n",
    "                    break\n",
    "\n",
    "\n",
    "    return train_study_ids, valid_study_ids, test_study_ids\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用のデータセットを作成\n",
    "train_tuple_dict = {}\n",
    "study_ids = train_series['study_id'].unique()\n",
    "\n",
    "# 並列にデータを読み込む\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    def func(sid):\n",
    "        print(f'残りのデータ数 : {len(study_ids) - len(train_tuple_dict)}')\n",
    "        x, y = get_dicom_and_label(sid, 'train')\n",
    "        train_tuple_dict[sid] = (x, y)\n",
    "\n",
    "    executor.map(func, study_ids)\n",
    "\n",
    "print(f'len(train_tuple_dict) : {len(train_tuple_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folds を作成\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# 交差検証用のデータセットを作成し保存\n",
    "for i, (train_index, test_index) in enumerate(kf.split(study_ids)):\n",
    "    train_study_ids = study_ids[train_index]\n",
    "    valid_study_ids = study_ids[test_index]\n",
    "\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    valid_x = []\n",
    "    valid_y = []\n",
    "\n",
    "    for study_id in train_study_ids:\n",
    "        x, y = train_tuple_dict[study_id]\n",
    "        train_x.append(x)\n",
    "        train_y.append(y)\n",
    "\n",
    "\n",
    "    for study_id in valid_study_ids:\n",
    "        x, y = train_tuple_dict[study_id]\n",
    "        valid_x.append(x)\n",
    "        valid_y.append(y)\n",
    "\n",
    "    # numpy配列に変換\n",
    "    train_x = np.array(train_x)\n",
    "    train_y = np.array(train_y)\n",
    "    valid_x = np.array(valid_x)\n",
    "    valid_y = np.array(valid_y)\n",
    "    \n",
    "    # データを保存\n",
    "    np.save(f'train_data_{i}_x.npy', train_x)\n",
    "    np.save(f'train_data_{i}_y.npy', train_y)\n",
    "    np.save(f'valid_data_{i}_x.npy', valid_x)\n",
    "    np.save(f'valid_data_{i}_y.npy', valid_y)\n",
    "\n",
    "    # メモリ解放\n",
    "    del train_x\n",
    "    del train_y\n",
    "    del valid_x\n",
    "    del valid_y\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを読み込んでサイズを確認\n",
    "train_x = np.load(f'train_data_0_x.npy')\n",
    "train_y = np.load(f'train_data_0_y.npy')\n",
    "\n",
    "valid_x = np.load(f'valid_data_0_x.npy')\n",
    "valid_y = np.load(f'valid_data_0_y.npy')\n",
    "\n",
    "print(f'train_x shape : {train_x.shape}')\n",
    "print(f'train_y shape : {train_y.shape}')\n",
    "print(f'valid_x shape : {valid_x.shape}')\n",
    "print(f'valid_y shape : {valid_y.shape}')\n",
    "\n",
    "# メモリ解放\n",
    "del train_x\n",
    "del train_y\n",
    "del valid_x\n",
    "del valid_y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.067694Z",
     "start_time": "2024-06-29T13:35:07.064655Z"
    }
   },
   "outputs": [],
   "source": [
    "# study_id のリストを作成\n",
    "train_study_ids, valid_study_ids, test_study_ids = get_separated_studyid_lists()\n",
    "\n",
    "print(f'train_study_ids : {len(train_study_ids)}')\n",
    "print(f'valid_study_ids : {len(valid_study_ids)}')\n",
    "print(f'test_study_ids : {len(test_study_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.675594Z",
     "start_time": "2024-06-29T13:35:07.673824Z"
    }
   },
   "outputs": [],
   "source": [
    "# データセットの保存\n",
    "generate_and_save_data(train_study_ids, file_name='train_data', file_kind='train')\n",
    "generate_and_save_data(valid_study_ids, file_name='valid_data', file_kind='train')\n",
    "generate_and_save_data(test_study_ids, file_name='test_data', file_kind='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:08.615697Z",
     "start_time": "2024-06-29T13:35:07.676286Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_imgs, train_data_labels = load_data('train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:08.619694Z",
     "start_time": "2024-06-29T13:35:08.616644Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
