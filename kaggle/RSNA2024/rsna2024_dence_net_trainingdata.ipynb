{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:06.577658Z",
     "start_time": "2024-06-29T13:35:04.465667Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "from keras import Model, Input\n",
    "from keras.src import backend\n",
    "from keras.src import layers\n",
    "from keras.src.applications import imagenet_utils\n",
    "from keras.src.layers import Dense\n",
    "from keras.src.models import Functional\n",
    "from keras.src.ops import operation_utils\n",
    "from keras.src.utils import file_utils\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.src.applications.densenet import DenseNet201\n",
    "from keras.src.callbacks import Callback\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:06.622692Z",
     "start_time": "2024-06-29T13:35:06.578802Z"
    }
   },
   "outputs": [],
   "source": [
    "base_path = '/home/toru/PycharmProjects/rsna-2024-lumbar-spine-degenerative-classification'\n",
    "train_png_file_path = f'{base_path}/train_dataset_png'\n",
    "\n",
    "label_coordinates_df = pd.read_csv(f'{base_path}/train_label_coordinates.csv')\n",
    "train_series = pd.read_csv(f'{base_path}/train_series_descriptions.csv')\n",
    "df_train = pd.read_csv(f'{base_path}/train.csv')\n",
    "df_sub = pd.read_csv(f'{base_path}/sample_submission.csv')\n",
    "test_series = pd.read_csv(f'{base_path}/test_series_descriptions.csv')\n",
    "\n",
    "train_dicom = f'{base_path}/train_images'\n",
    "test_dicom = f'{base_path}/test_images'\n",
    "\n",
    "model_path = f'model.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.021986Z",
     "start_time": "2024-06-29T13:35:06.623607Z"
    }
   },
   "outputs": [],
   "source": [
    "# 一つのstudy_idにseries_id が3つ未満のstudy_idを探す\n",
    "study_id_list = train_series['study_id'].unique()\n",
    "for study_id in study_id_list:\n",
    "    series_id_list = train_series[train_series['study_id'] == study_id][\n",
    "        'series_description'].unique()\n",
    "\n",
    "    if len(series_id_list) < 3:\n",
    "        print(f'study_id : {study_id}, series_id : {series_id_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.027414Z",
     "start_time": "2024-06-29T13:35:07.023661Z"
    }
   },
   "outputs": [],
   "source": [
    "# パラメータ\n",
    "STUDY_NUM = 1975\n",
    "\n",
    "# 出力のラベル\n",
    "NORMAL_MILD = 0\n",
    "MODERATE = 1\n",
    "SEVERE = 2\n",
    "\n",
    "# 重症度一覧\n",
    "SEVERITY_LIST = ['Normal/Mild', 'Moderate', 'Severe']\n",
    "\n",
    "# dicomデータの種類\n",
    "SERIES_DESCRIPTION_LIST = train_series['series_description'].unique().tolist()\n",
    "# 学習用パラメータ\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 200\n",
    "TRAIN_RATIO = 0.8\n",
    "VALID_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "# 入出力の形状 (DenseNet201の入力サイズ)\n",
    "INPUT_WIDTH = 512\n",
    "INPUT_HEIGHT = 512\n",
    "INPUT_CHANNEL_DICT = {\n",
    "    'Sagittal T1': 17,\n",
    "    'Sagittal T2/STIR': 17,\n",
    "    'Axial T2': 29\n",
    "}\n",
    "IN_CHANS = sum(INPUT_CHANNEL_DICT.values())\n",
    "IMG_SIZE = [INPUT_WIDTH, INPUT_HEIGHT]\n",
    "\n",
    "CONDITIONS = [\n",
    "    'spinal_canal_stenosis',\n",
    "    'left_neural_foraminal_narrowing',\n",
    "    'right_neural_foraminal_narrowing',\n",
    "    'left_subarticular_stenosis',\n",
    "    'right_subarticular_stenosis'\n",
    "]\n",
    "\n",
    "LEVELS = [\n",
    "    'l1_l2',\n",
    "    'l2_l3',\n",
    "    'l3_l4',\n",
    "    'l4_l5',\n",
    "    'l5_s1',\n",
    "]\n",
    "\n",
    "N_LABELS = 25\n",
    "N_CLASSES = 3 * N_LABELS\n",
    "\n",
    "data_filenames = ['train_data', 'valid_data', 'test_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.036975Z",
     "start_time": "2024-06-29T13:35:07.028109Z"
    }
   },
   "outputs": [],
   "source": [
    "_target_name = \"right_subarticular_stenosis_l5_s1\"\n",
    "\n",
    "print(f'train shape : {df_train.shape}')\n",
    "print(df_train[_target_name].unique())\n",
    "\n",
    "# それぞれの重症度の数がいくらあるか計算\n",
    "print(f'Normal/Mild : {df_train[df_train[_target_name] == str(\"Normal/Mild\")].shape[0]}')\n",
    "print(f'Moderate : {df_train[df_train[_target_name] == str(\"Moderate\")].shape[0]}')\n",
    "print(f'Severe : {df_train[df_train[_target_name] == str(\"Severe\")].shape[0]}')\n",
    "\n",
    "print(\n",
    "    f'Normal/Mild ratio : {df_train[df_train[_target_name] == str(\"Normal/Mild\")].shape[0] / df_train.shape[0]}')\n",
    "print(\n",
    "    f'Moderate ratio : {df_train[df_train[_target_name] == str(\"Moderate\")].shape[0] / df_train.shape[0]}')\n",
    "print(\n",
    "    f'Severe ratio : {df_train[df_train[_target_name] == str(\"Severe\")].shape[0] / df_train.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.058616Z",
     "start_time": "2024-06-29T13:35:07.037734Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "\n",
    "def get_train_label(study_id):\n",
    "    \"\"\"\n",
    "    study_id に対応するラベルを取得 (testはデータがないのでtrainのみ)\n",
    "    \"\"\"\n",
    "\n",
    "    row = df_train[df_train['study_id'] == study_id].to_numpy()[0][1:].tolist()\n",
    "\n",
    "    row_category = np.zeros((N_LABELS, 3))\n",
    "    for i in range(N_LABELS):\n",
    "        if row[i] == 'Normal/Mild':\n",
    "            row_category[i][0] = 1\n",
    "        elif row[i] == 'Moderate':\n",
    "            row_category[i][1] = 1\n",
    "        elif row[i] == 'Severe':\n",
    "            row_category[i][2] = 1\n",
    "    # 1次元配列に変換\n",
    "    row_category = row_category.flatten()\n",
    "    return row_category\n",
    "\n",
    "\n",
    "def get_file_list(study_id, series_desc, file_kind='train'):\n",
    "    \"\"\"\n",
    "    study_id に対応するファイルリストを取得\n",
    "    \"\"\"\n",
    "    dicom_file_list = []\n",
    "    df = None\n",
    "    if file_kind == 'train':\n",
    "        df = train_series\n",
    "    elif file_kind == 'test':\n",
    "        df = test_series\n",
    "\n",
    "    # study_id に対応する series_desc のリストを取得\n",
    "    series_ids = df[df['study_id'] == study_id]\n",
    "    series_id = series_ids[series_ids['series_description'] == series_desc]['series_id'].values\n",
    "    if len(series_id) == 0:\n",
    "        return dicom_file_list\n",
    "\n",
    "    series_id = series_id[0]\n",
    "\n",
    "    target_dicom_dir_path = ''\n",
    "    if file_kind == 'train':\n",
    "        target_dicom_dir_path = f'{train_dicom}/{study_id}/{series_id}/'\n",
    "    elif file_kind == 'test':\n",
    "        target_dicom_dir_path = f'{test_dicom}/{study_id}/{series_id}/'\n",
    "    dicom_file_list = glob(target_dicom_dir_path + '/*.dcm')\n",
    "    # ファイル名が数字になっているのでソート\n",
    "    dicom_file_list = sorted(dicom_file_list, key=natural_keys)\n",
    "\n",
    "    # series_desc にt対応するファイルリストを取得\n",
    "    return dicom_file_list\n",
    "\n",
    "\n",
    "def load_dicom_img(dicom_filepath):\n",
    "    \"\"\"\n",
    "    dicomファイルを読み込む\n",
    "    \"\"\"\n",
    "    dicom_data = dicom.read_file(dicom_filepath).pixel_array\n",
    "\n",
    "    # 正規化\n",
    "    dicom_data = (dicom_data - np.min(dicom_data)) / (np.max(dicom_data) - np.min(dicom_data) + 1e-8) * 255\n",
    "\n",
    "    # 512x512にリサイズ\n",
    "    dicom_data = cv2.resize(dicom_data, (INPUT_WIDTH, INPUT_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # uint8 に変換\n",
    "    dicom_data = dicom_data.astype(np.uint8)\n",
    "\n",
    "    return dicom_data\n",
    "\n",
    "\n",
    "def get_dicom_input_data(study_id, file_kind='train'):\n",
    "    \"\"\"\n",
    "    study_id に対応するdicomデータを取得\n",
    "    \"\"\"\n",
    "\n",
    "    input_dicom = np.zeros((INPUT_WIDTH, INPUT_HEIGHT, IN_CHANS))\n",
    "    # Sagittal T1\n",
    "    sagittal_t1_dicom_list = get_file_list(study_id, 'Sagittal T1', file_kind)\n",
    "    if len(sagittal_t1_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Sagittal T1']):\n",
    "            if i < len(sagittal_t1_dicom_list):\n",
    "                dicom_data = load_dicom_img(sagittal_t1_dicom_list[i])\n",
    "                input_dicom[:, :, i] = dicom_data\n",
    "\n",
    "    # Sagittal T2/STIR\n",
    "    sagittal_t2_dicom_list = get_file_list(study_id, 'Sagittal T2/STIR', file_kind)\n",
    "    if len(sagittal_t2_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Sagittal T2/STIR']):\n",
    "            if i < len(sagittal_t2_dicom_list):\n",
    "                dicom_data = load_dicom_img(sagittal_t2_dicom_list[i])\n",
    "                input_dicom[:, :, i + INPUT_CHANNEL_DICT['Sagittal T1']] = dicom_data\n",
    "\n",
    "    # Axial T2\n",
    "    axial_t2_dicom_list = get_file_list(study_id, 'Axial T2', file_kind)\n",
    "    if len(axial_t2_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Axial T2']):\n",
    "            if i < len(axial_t2_dicom_list):\n",
    "                dicom_data = load_dicom_img(axial_t2_dicom_list[i])\n",
    "                input_dicom[:, :, i + INPUT_CHANNEL_DICT['Sagittal T1'] + INPUT_CHANNEL_DICT[\n",
    "                    'Sagittal T2/STIR']] = dicom_data\n",
    "\n",
    "    # uint8 に変換\n",
    "    input_dicom = input_dicom.astype(np.int8)\n",
    "    return input_dicom\n",
    "\n",
    "\n",
    "def get_dicom_and_label(study_id, file_kind):\n",
    "    x_data = get_dicom_input_data(study_id, file_kind)\n",
    "    y_data = get_train_label(study_id)\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "def generator(batch_size, study_ids, file_kind='train', is_augmentation=False, is_shuffle=False):\n",
    "    \"\"\"\n",
    "    データセットの読み込み関数\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        if is_shuffle:\n",
    "            np.random.shuffle(study_ids)\n",
    "\n",
    "        for start in range(0, len(study_ids), batch_size):\n",
    "            gc.collect()\n",
    "\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(study_ids))\n",
    "            ids_train_batch = study_ids[start:end]\n",
    "\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                results = list(\n",
    "                    executor.map(lambda sid: get_dicom_and_label(sid, file_kind), ids_train_batch))\n",
    "\n",
    "            for x_data, y_data in results:\n",
    "                if is_augmentation:\n",
    "                    # 画像データにノイズを加える\n",
    "                    x_data = x_data + np.random.normal(0, 0.1, x_data.shape)\n",
    "\n",
    "                x_batch.append(x_data)\n",
    "                y_batch.append(y_data)\n",
    "\n",
    "            yield np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "def generator_load_all(batch_size, study_ids, file_kind='train', is_augmentation=False, is_shuffle=False):\n",
    "    \"\"\"\n",
    "    実行時予めすべてのデータを読み込んだ上でデータを返す\n",
    "    \"\"\"\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    print(f'事前にすべてのデータを読み込みます')\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(\n",
    "            executor.map(lambda sid: get_dicom_and_label(sid, file_kind), study_ids))\n",
    "    \n",
    "    for x_data, y_data in results:\n",
    "        x_batch.append(x_data)\n",
    "        y_batch.append(y_data)\n",
    "    \n",
    "    print(f'データの読み込みが完了しました')\n",
    "    # データをバッチサイズに分割して返す\n",
    "    while True:\n",
    "        for start in range(0, len(x_batch), batch_size):\n",
    "            end = min(start + batch_size, len(study_ids))\n",
    "            \n",
    "            x_batch_batch = x_batch[start:end]\n",
    "            y_batch_batch = y_batch[start:end]\n",
    "            \n",
    "            if is_augmentation:\n",
    "                for i in range(len(x_batch_batch)):\n",
    "                    # ノイズを加える\n",
    "                    x_batch_batch[i] = x_batch_batch[i] + np.random.normal(0, 0.1, x_batch_batch[i].shape)\n",
    "                    \n",
    "            yield np.array(x_batch_batch), np.array(y_batch_batch)\n",
    "            \n",
    "def generate_and_save_data(study_ids, file_name, file_kind='train'):\n",
    "    \"\"\"\n",
    "    学習用データを生成して保存\n",
    "    \"\"\"\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for sid in study_ids:\n",
    "        print(f'残りのデータ数 : {len(study_ids) - len(x_batch)}')\n",
    "        x_data, y_data = get_dicom_and_label(sid, file_kind)\n",
    "\n",
    "\n",
    "        x_batch.append(x_data)\n",
    "        y_batch.append(y_data)\n",
    "    \n",
    "    x_batch = np.array(x_batch)\n",
    "    y_batch = np.array(y_batch)\n",
    "    \n",
    "    print(f'x_batch shape : {x_batch.shape}')\n",
    "    print(f'y_batch shape : {y_batch.shape}')\n",
    "    \n",
    "    np.save(f'{file_name}_x.npy', x_batch)\n",
    "    np.save(f'{file_name}_y.npy', y_batch)\n",
    "    \n",
    "    # メモリ解放\n",
    "    del x_batch\n",
    "    del y_batch\n",
    "    gc.collect()\n",
    "    \n",
    "def save_train_data():\n",
    "    \"\"\"\n",
    "    学習用データを保存\n",
    "    \"\"\"\n",
    "    train_study_ids = get_train_study_id_array()\n",
    "    generate_and_save_data(train_study_ids, file_name='train_data', file_kind='train')\n",
    "\n",
    "def save_valid_data():\n",
    "    \"\"\"\n",
    "    検証用データを保存\n",
    "    \"\"\"\n",
    "    valid_study_ids = get_valid_study_id_array()\n",
    "    generate_and_save_data(valid_study_ids, file_name='valid_data', file_kind='train')\n",
    "\n",
    "def save_test_data():\n",
    "    \"\"\"\n",
    "    テスト用データを保存\n",
    "    \"\"\"\n",
    "    test_study_ids = get_test_study_id_array()\n",
    "    generate_and_save_data(test_study_ids, file_name='test_data', file_kind='train')\n",
    "        \n",
    "# 学習用データの study_id を取得\n",
    "def get_train_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 0.8 で分割\n",
    "    train_study_ids = _study_ids[:int(len(_study_ids) * 0.8)]\n",
    "    return train_study_ids\n",
    "\n",
    "\n",
    "# 検証用データの study_id を取得\n",
    "def get_valid_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 0.8 で分割\n",
    "    valid_study_ids = _study_ids[int(len(_study_ids) * 0.8):int(len(_study_ids) * 0.9)]\n",
    "    return valid_study_ids\n",
    "\n",
    "\n",
    "# テスト用データの study_id を取得 (実際のテストデータではなく, 学習データの一部をテストデータとして使用)\n",
    "def get_test_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "    # 0.8 で分割\n",
    "    test_study_ids = _study_ids[int(len(_study_ids) * 0.9):]\n",
    "    return test_study_ids\n",
    "\n",
    "\n",
    "def generator_for_test(batch_size, study_ids):\n",
    "    \"\"\"\n",
    "    test_seriesからテスト用のデータセット作成\n",
    "    kaggle で提出時に呼び出される\n",
    "    :param batch_size: \n",
    "    :param study_ids: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    for start in range(0, len(study_ids), batch_size):\n",
    "        x_batch = []\n",
    "        end = min(start + batch_size, len(study_ids))\n",
    "        ids_train_batch = study_ids[start:end]\n",
    "\n",
    "        results = []\n",
    "        for sid in ids_train_batch:\n",
    "            results.append(get_dicom_input_data(sid, 'train'))\n",
    "        for x_data in results:\n",
    "            x_batch.append(x_data)\n",
    "\n",
    "        yield np.array(x_batch), ids_train_batch\n",
    "        \n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    データを読み込む\n",
    "    \"\"\"\n",
    "    x_data = np.load(f'{file_name}_x.npy')\n",
    "    y_data = np.load(f'{file_name}_y.npy')\n",
    "    \n",
    "    return x_data, y_data\n",
    "\n",
    "def generator_with_load_data(batch_size, \n",
    "                             study_ids, \n",
    "                             file_name,\n",
    "                             file_kind='train', \n",
    "                             is_augmentation=False, \n",
    "                             is_shuffle=False):\n",
    "    \"\"\"\n",
    "    事前にデータを読み込んでデータを返す\n",
    "    \"\"\"\n",
    "    x_batch, y_batch = load_data(file_name)\n",
    "    \n",
    "    def shuffle_data_and_labels(x_batch, y_batch):\n",
    "        # データとラベルをペアにしたリストを作成\n",
    "        data_label_pairs = list(zip(x_batch, y_batch))\n",
    "    \n",
    "        # ペアのリストをシャッフル\n",
    "        np.random.shuffle(data_label_pairs)\n",
    "    \n",
    "        # シャッフル後のデータとラベルをそれぞれ別々の配列に戻す\n",
    "        shuffled_x_batch, shuffled_y_batch = zip(*data_label_pairs)\n",
    "    \n",
    "        # NumPy 配列に変換して返す（オプション）\n",
    "        shuffled_x_batch = np.array(shuffled_x_batch)\n",
    "        shuffled_y_batch = np.array(shuffled_y_batch)\n",
    "    \n",
    "        return shuffled_x_batch, shuffled_y_batch\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        # epochごとにシャッフル\n",
    "        if is_shuffle:\n",
    "            x_batch, y_batch = shuffle_data_and_labels(x_batch, y_batch)\n",
    "            \n",
    "        for start in range(0, len(x_batch), batch_size):\n",
    "            end = min(start + batch_size, len(study_ids))\n",
    "            \n",
    "            x_batch_batch = x_batch[start:end]\n",
    "            y_batch_batch = y_batch[start:end]\n",
    "            \n",
    "            if is_augmentation:\n",
    "                for i in range(len(x_batch_batch)):\n",
    "                    # ノイズを加える\n",
    "                    x_batch_batch[i] = x_batch_batch[i] + np.random.normal(0, 0.1, x_batch_batch[i].shape)\n",
    "                    \n",
    "                    # ブラーをかける\n",
    "                    x_batch_batch[i] = cv2.GaussianBlur(x_batch_batch[i], (5, 5), 0)\n",
    "                    \n",
    "            yield np.array(x_batch_batch), np.array(y_batch_batch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.063855Z",
     "start_time": "2024-06-29T13:35:07.059454Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2492114990\n",
    "for desc in SERIES_DESCRIPTION_LIST:\n",
    "    dicom_image_list = get_file_list(2492114990, desc)\n",
    "    print(f'{desc} : {len(dicom_image_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.067694Z",
     "start_time": "2024-06-29T13:35:07.064655Z"
    }
   },
   "outputs": [],
   "source": [
    "# study_id のリストを作成\n",
    "train_study_ids = get_train_study_id_array()\n",
    "print(len(train_study_ids))\n",
    "valid_study_ids = get_valid_study_id_array()\n",
    "print(len(valid_study_ids))\n",
    "test_study_ids = get_test_study_id_array()\n",
    "print(len(test_study_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.073400Z",
     "start_time": "2024-06-29T13:35:07.068659Z"
    }
   },
   "outputs": [],
   "source": [
    "get_train_label(4003253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.672915Z",
     "start_time": "2024-06-29T13:35:07.075385Z"
    }
   },
   "outputs": [],
   "source": [
    "dicom_image_list = get_file_list(4003253, 'Sagittal T1')\n",
    "\n",
    "input_image = get_dicom_input_data(4003253)\n",
    "plt.imshow(input_image[:, :, 0:3])\n",
    "input_data_gb = input_image.nbytes / 1024 / 1024 / 1024\n",
    "print(f\"画像サイズ[GB] : {input_data_gb}\")\n",
    "print(f'データセット全体のデータ数は {input_data_gb * STUDY_NUM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:07.675594Z",
     "start_time": "2024-06-29T13:35:07.673824Z"
    }
   },
   "outputs": [],
   "source": [
    "# データセットの保存\n",
    "save_train_data()\n",
    "save_valid_data()\n",
    "save_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:08.615697Z",
     "start_time": "2024-06-29T13:35:07.676286Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_imgs, train_data_labels = load_data('train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-29T13:35:08.619694Z",
     "start_time": "2024-06-29T13:35:08.616644Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
