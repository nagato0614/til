{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "from keras import Model, Input\n",
    "from keras.src import backend\n",
    "from keras.src import layers\n",
    "from keras.src.applications import imagenet_utils\n",
    "from keras.src.layers import Dense\n",
    "from keras.src.models import Functional\n",
    "from keras.src.ops import operation_utils\n",
    "from keras.src.utils import file_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc5eee8c5e19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/toru/PycharmProjects/rsna-2024-lumbar-spine-degenerative-classification'\n",
    "train_png_file_path = f'{base_path}/train_dataset_png'\n",
    "\n",
    "label_coordinates_df = pd.read_csv(f'{base_path}/train_label_coordinates.csv')\n",
    "train_series = pd.read_csv(f'{base_path}/train_series_descriptions.csv')\n",
    "df_train = pd.read_csv(f'{base_path}/train.csv')\n",
    "df_sub = pd.read_csv(f'{base_path}/sample_submission.csv')\n",
    "test_series = pd.read_csv(f'{base_path}/test_series_descriptions.csv')\n",
    "\n",
    "train_dicom = f'{base_path}/train_images'\n",
    "test_dicom = f'{base_path}/test_images'\n",
    "\n",
    "model_path = f'model.keras'\n",
    "model_base_path = f'.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d20176",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba73e05e10985c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ\n",
    "STUDY_NUM = 1975\n",
    "\n",
    "# 出力のラベル\n",
    "NORMAL_MILD = 0\n",
    "MODERATE = 1\n",
    "SEVERE = 2\n",
    "\n",
    "# 重症度一覧\n",
    "SEVERITY_LIST = ['Normal/Mild', 'Moderate', 'Severe']\n",
    "\n",
    "# dicomデータの種類\n",
    "SERIES_DESCRIPTION_LIST = train_series['series_description'].unique().tolist()\n",
    "# 学習用パラメータ\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "TRAIN_RATIO = 0.8\n",
    "VALID_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "# 入出力の形状\n",
    "INPUT_WIDTH = 256\n",
    "INPUT_HEIGHT = 256\n",
    "INPUT_CHANNEL_DICT = {\n",
    "    'Sagittal T1': 17,\n",
    "    'Sagittal T2/STIR': 17,\n",
    "    'Axial T2': 29\n",
    "}\n",
    "IN_CHANS = sum(INPUT_CHANNEL_DICT.values())\n",
    "IMG_SIZE = [INPUT_WIDTH, INPUT_HEIGHT]\n",
    "\n",
    "CONDITIONS = [\n",
    "    'spinal_canal_stenosis',\n",
    "    'left_neural_foraminal_narrowing',\n",
    "    'right_neural_foraminal_narrowing',\n",
    "    'left_subarticular_stenosis',\n",
    "    'right_subarticular_stenosis'\n",
    "]\n",
    "\n",
    "LEVELS = [\n",
    "    'l1_l2',\n",
    "    'l2_l3',\n",
    "    'l3_l4',\n",
    "    'l4_l5',\n",
    "    'l5_s1',\n",
    "]\n",
    "\n",
    "N_LABELS = 25\n",
    "N_CLASSES = 3 * N_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a888263a33a46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "\n",
    "def get_train_label(study_id):\n",
    "    \"\"\"\n",
    "    study_id に対応するラベルを取得 (testはデータがないのでtrainのみ)\n",
    "    \"\"\"\n",
    "\n",
    "    row = df_train[df_train['study_id'] == study_id].to_numpy()[0][1:].tolist()\n",
    "\n",
    "    row_category = np.zeros((N_LABELS, 3))\n",
    "    for i in range(N_LABELS):\n",
    "        if row[i] == 'Normal/Mild':\n",
    "            row_category[i][0] = 1\n",
    "        elif row[i] == 'Moderate':\n",
    "            row_category[i][1] = 1\n",
    "        elif row[i] == 'Severe':\n",
    "            row_category[i][2] = 1\n",
    "    # 1次元配列に変換\n",
    "    row_category = row_category.flatten()\n",
    "    return row_category\n",
    "\n",
    "\n",
    "def get_file_list(study_id, series_desc, file_kind='train'):\n",
    "    \"\"\"\n",
    "    study_id に対応するファイルリストを取得\n",
    "    \"\"\"\n",
    "    dicom_file_list = []\n",
    "    df = None\n",
    "    if file_kind == 'train':\n",
    "        df = train_series\n",
    "    elif file_kind == 'test':\n",
    "        df = test_series\n",
    "\n",
    "    # study_id に対応する series_desc のリストを取得\n",
    "    series_ids = df[df['study_id'] == study_id]\n",
    "    series_id = series_ids[series_ids['series_description'] == series_desc]['series_id'].values\n",
    "    if len(series_id) == 0:\n",
    "        return dicom_file_list\n",
    "\n",
    "    series_id = series_id[0]\n",
    "\n",
    "    target_dicom_dir_path = ''\n",
    "    if file_kind == 'train':\n",
    "        target_dicom_dir_path = f'{train_dicom}/{study_id}/{series_id}/'\n",
    "    elif file_kind == 'test':\n",
    "        target_dicom_dir_path = f'{test_dicom}/{study_id}/{series_id}/'\n",
    "    dicom_file_list = glob(target_dicom_dir_path + '/*.dcm')\n",
    "    # ファイル名が数字になっているのでソート\n",
    "    dicom_file_list = sorted(dicom_file_list, key=natural_keys)\n",
    "\n",
    "    # series_desc にt対応するファイルリストを取得\n",
    "    return dicom_file_list\n",
    "\n",
    "\n",
    "def load_dicom_img(dicom_filepath):\n",
    "    \"\"\"\n",
    "    dicomファイルを読み込む\n",
    "    \"\"\"\n",
    "    dicom_data = dicom.read_file(dicom_filepath).pixel_array\n",
    "\n",
    "    # 0-1に正規化\n",
    "    dicom_data = dicom_data / np.max(dicom_data)\n",
    "\n",
    "    # 0~255に変換\n",
    "    dicom_data = dicom_data * 255\n",
    "\n",
    "    # 512x512にリサイズ\n",
    "    dicom_data = cv2.resize(dicom_data, (INPUT_WIDTH, INPUT_HEIGHT))\n",
    "\n",
    "    # uint8 に変換\n",
    "    dicom_data = dicom_data.astype(np.uint8)\n",
    "\n",
    "    return dicom_data\n",
    "\n",
    "\n",
    "def get_dicom_input_data(study_id, file_kind='train'):\n",
    "    \"\"\"\n",
    "    study_id に対応するdicomデータを取得\n",
    "    \"\"\"\n",
    "\n",
    "    input_dicom = np.zeros((INPUT_WIDTH, INPUT_HEIGHT, IN_CHANS))\n",
    "    # Sagittal T1\n",
    "    sagittal_t1_dicom_list = get_file_list(study_id, 'Sagittal T1', file_kind)\n",
    "    if len(sagittal_t1_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Sagittal T1']):\n",
    "            if i < len(sagittal_t1_dicom_list):\n",
    "                dicom_data = load_dicom_img(sagittal_t1_dicom_list[i])\n",
    "                input_dicom[:, :, i] = dicom_data\n",
    "\n",
    "    # Sagittal T2/STIR\n",
    "    sagittal_t2_dicom_list = get_file_list(study_id, 'Sagittal T2/STIR', file_kind)\n",
    "    if len(sagittal_t2_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Sagittal T2/STIR']):\n",
    "            if i < len(sagittal_t2_dicom_list):\n",
    "                dicom_data = load_dicom_img(sagittal_t2_dicom_list[i])\n",
    "                input_dicom[:, :, i + INPUT_CHANNEL_DICT['Sagittal T1']] = dicom_data\n",
    "\n",
    "    # Axial T2\n",
    "    axial_t2_dicom_list = get_file_list(study_id, 'Axial T2', file_kind)\n",
    "    if len(axial_t2_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Axial T2']):\n",
    "            if i < len(axial_t2_dicom_list):\n",
    "                dicom_data = load_dicom_img(axial_t2_dicom_list[i])\n",
    "                input_dicom[:, :, i + INPUT_CHANNEL_DICT['Sagittal T1'] + INPUT_CHANNEL_DICT[\n",
    "                    'Sagittal T2/STIR']] = dicom_data\n",
    "\n",
    "    # uint8 に変換\n",
    "    input_dicom = input_dicom.astype(np.int8)\n",
    "    return input_dicom\n",
    "\n",
    "\n",
    "def get_dicom_and_label(study_id, file_kind):\n",
    "    x_data = get_dicom_input_data(study_id, file_kind)\n",
    "    y_data = get_train_label(study_id)\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "def generator(batch_size, study_ids):\n",
    "    \"\"\"\n",
    "    データセットの読み込み関数\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        for start in range(0, len(study_ids), batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(study_ids))\n",
    "            ids_train_batch = study_ids[start:end]\n",
    "\n",
    "            results = []\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                results = list(\n",
    "                    executor.map(lambda sid: get_dicom_and_label(sid),\n",
    "                                 ids_train_batch))\n",
    "            for x_data, y_data in results:\n",
    "                x_batch.append(x_data)\n",
    "                y_batch.append(y_data)\n",
    "\n",
    "            yield np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "def generator_for_test(batch_size, study_ids):\n",
    "    \"\"\"\n",
    "    test_seriesからテスト用のデータセット作成\n",
    "    kaggle で提出時に呼び出される\n",
    "    :param batch_size: \n",
    "    :param study_ids: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    for start in range(0, len(study_ids), batch_size):\n",
    "        x_batch = []\n",
    "        end = min(start + batch_size, len(study_ids))\n",
    "        ids_train_batch = study_ids[start:end]\n",
    "\n",
    "        results = []\n",
    "        for sid in ids_train_batch:\n",
    "            results.append(get_dicom_input_data(sid, 'test'))\n",
    "        for x_data in results:\n",
    "            x_batch.append(x_data)\n",
    "\n",
    "        yield np.array(x_batch), ids_train_batch\n",
    "\n",
    "\n",
    "# 学習用データの study_id を取得\n",
    "def get_train_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 0.8 で分割\n",
    "    train_study_ids = _study_ids[:int(len(_study_ids) * 0.8)]\n",
    "    return train_study_ids\n",
    "\n",
    "\n",
    "# 検証用データの study_id を取得\n",
    "def get_valid_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 0.8 で分割\n",
    "    valid_study_ids = _study_ids[int(len(_study_ids) * 0.8):int(len(_study_ids) * 0.9)]\n",
    "    return valid_study_ids\n",
    "\n",
    "\n",
    "# テスト用データの study_id を取得 (実際のテストデータではなく, 学習データの一部をテストデータとして使用)\n",
    "def get_test_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "    # 0.8 で分割\n",
    "    test_study_ids = _study_ids[int(len(_study_ids) * 0.9):]\n",
    "    return test_study_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1e62d5afe3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_block(x, reduction, name):\n",
    "    \"\"\"A transition block.\n",
    "\n",
    "    Args:\n",
    "        x: input tensor.\n",
    "        reduction: float, compression rate at transition layers.\n",
    "        name: string, block label.\n",
    "\n",
    "    Returns:\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_bn\"\n",
    "    )(x)\n",
    "    x = layers.Activation(\"relu\", name=name + \"_relu\")(x)\n",
    "    x = layers.Conv2D(\n",
    "        int(x.shape[bn_axis] * reduction),\n",
    "        1,\n",
    "        use_bias=False,\n",
    "        name=name + \"_conv\",\n",
    "    )(x)\n",
    "    x = layers.AveragePooling2D(2, strides=2, name=name + \"_pool\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(x, growth_rate, name):\n",
    "    \"\"\"A building block for a dense block.\n",
    "\n",
    "    Args:\n",
    "        x: input tensor.\n",
    "        growth_rate: float, growth rate at dense layers.\n",
    "        name: string, block label.\n",
    "\n",
    "    Returns:\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n",
    "    x1 = layers.BatchNormalization(\n",
    "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_0_bn\"\n",
    "    )(x)\n",
    "    x1 = layers.Activation(\"relu\", name=name + \"_0_relu\")(x1)\n",
    "    x1 = layers.Conv2D(\n",
    "        4 * growth_rate, 1, use_bias=False, name=name + \"_1_conv\"\n",
    "    )(x1)\n",
    "    x1 = layers.BatchNormalization(\n",
    "        axis=bn_axis, epsilon=1.001e-5, name=name + \"_1_bn\"\n",
    "    )(x1)\n",
    "    x1 = layers.Activation(\"relu\", name=name + \"_1_relu\")(x1)\n",
    "    x1 = layers.Conv2D(\n",
    "        growth_rate, 3, padding=\"same\", use_bias=False, name=name + \"_2_conv\"\n",
    "    )(x1)\n",
    "    x = layers.Concatenate(axis=bn_axis, name=name + \"_concat\")([x, x1])\n",
    "    return x\n",
    "\n",
    "\n",
    "def dense_block(x, blocks, name):\n",
    "    \"\"\"A dense block.\n",
    "\n",
    "    Args:\n",
    "        x: input tensor.\n",
    "        blocks: integer, the number of building blocks.\n",
    "        name: string, block label.\n",
    "\n",
    "    Returns:\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    for i in range(blocks):\n",
    "        x = conv_block(x, 32, name=name + \"_block\" + str(i + 1))\n",
    "    return x\n",
    "\n",
    "\n",
    "def DenseNet(\n",
    "        blocks,\n",
    "        include_top=True,\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=None,\n",
    "        input_shape=None,\n",
    "        pooling=None,\n",
    "        classes=1000,\n",
    "        classifier_activation=\"softmax\",\n",
    "):\n",
    "    if backend.image_data_format() == \"channels_first\":\n",
    "        raise ValueError(\n",
    "            \"DenseNet does not support the `channels_first` image data \"\n",
    "            \"format. Switch to `channels_last` by editing your local \"\n",
    "            \"config file at ~/.keras/keras.json\"\n",
    "        )\n",
    "    if not (weights in {\"imagenet\", None} or file_utils.exists(weights)):\n",
    "        raise ValueError(\n",
    "            \"The `weights` argument should be either \"\n",
    "            \"`None` (random initialization), `imagenet` \"\n",
    "            \"(pre-training on ImageNet), \"\n",
    "            \"or the path to the weights file to be loaded.\"\n",
    "        )\n",
    "\n",
    "    if weights == \"imagenet\" and include_top and classes != 1000:\n",
    "        raise ValueError(\n",
    "            'If using `weights` as `\"imagenet\"` with `include_top`'\n",
    "            \" as true, `classes` should be 1000\"\n",
    "        )\n",
    "\n",
    "    bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n",
    "\n",
    "    x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(input_tensor)\n",
    "    x = layers.Conv2D(64, 7, strides=2, use_bias=False, name=\"conv1_conv\")(x)\n",
    "    x = layers.BatchNormalization(\n",
    "        axis=bn_axis, epsilon=1.001e-5, name=\"conv1_bn\"\n",
    "    )(x)\n",
    "    x = layers.Activation(\"relu\", name=\"conv1_relu\")(x)\n",
    "    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, name=\"pool1\")(x)\n",
    "\n",
    "    x = dense_block(x, blocks[0], name=\"conv2\")\n",
    "    x = transition_block(x, 0.5, name=\"pool2\")\n",
    "    x = dense_block(x, blocks[1], name=\"conv3\")\n",
    "    x = transition_block(x, 0.5, name=\"pool3\")\n",
    "    x = dense_block(x, blocks[2], name=\"conv4\")\n",
    "    x = transition_block(x, 0.5, name=\"pool4\")\n",
    "    x = dense_block(x, blocks[3], name=\"conv5\")\n",
    "\n",
    "    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=\"bn\")(x)\n",
    "    x = layers.Activation(\"relu\", name=\"relu\")(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "\n",
    "        imagenet_utils.validate_activation(classifier_activation, weights)\n",
    "        x = layers.Dense(\n",
    "            classes, activation=classifier_activation, name=\"predictions\"\n",
    "        )(x)\n",
    "    else:\n",
    "        if pooling == \"avg\":\n",
    "            x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "        elif pooling == \"max\":\n",
    "            x = layers.GlobalMaxPooling2D(name=\"max_pool\")(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = operation_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = input_tensor\n",
    "\n",
    "    # Create model.\n",
    "    if blocks == [6, 12, 24, 16]:\n",
    "        model = Functional(inputs, x, name=\"densenet121\")\n",
    "    elif blocks == [6, 12, 32, 32]:\n",
    "        model = Functional(inputs, x, name=\"densenet169\")\n",
    "    elif blocks == [6, 12, 48, 32]:\n",
    "        model = Functional(inputs, x, name=\"densenet201\")\n",
    "    else:\n",
    "        model = Functional(inputs, x, name=\"densenet\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # カスタム入力層\n",
    "    input_shape = (INPUT_WIDTH, INPUT_HEIGHT, IN_CHANS)\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # DenseNet201ベースモデル（トップ層なし）\n",
    "    base_model = DenseNet(\n",
    "        [6, 12, 48, 32],\n",
    "        include_top=False,\n",
    "        input_tensor=inputs,\n",
    "        input_shape=input_shape,\n",
    "        pooling='avg'\n",
    "    )\n",
    "\n",
    "    # カスタム出力層（75クラスのマルチラベル分類）\n",
    "    outputs = Dense(75, activation='sigmoid')(base_model.output)  # マルチラベル分類のためにシグモイド関数を使用\n",
    "\n",
    "    # モデルの作成\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # モデルのコンパイル\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bced9166f97421",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_ids = test_series['study_id'].unique()\n",
    "for x, sid in generator_for_test(1, test_dataset_ids):\n",
    "    print(x.shape)\n",
    "    print(sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4d7e25f12c1f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "model.load_weights(model_path)\n",
    "test_dataset_ids = test_series['study_id'].unique()\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "\n",
    "submission_labels = ['normal_mild', 'moderate', 'severe']\n",
    "\n",
    "# test_study_ids を元に予測\n",
    "for x, sids in generator_for_test(1, test_dataset_ids):\n",
    "    y_pred = model.predict(x)[0]\n",
    "\n",
    "    # 予測結果を (25, 3) に変換\n",
    "    y_pred = y_pred.reshape((25, 3))\n",
    "\n",
    "    # 予測結果をsoftmax関数で確率に変換\n",
    "    y_pred = np.exp(y_pred) / np.sum(np.exp(y_pred), axis=1).reshape(-1, 1)\n",
    "    \n",
    "    # 小数点第3位まで表示\n",
    "    y_pred = np.round(y_pred, 3)\n",
    "    \n",
    "    # 予測結果を出力\n",
    "    print(y_pred.shape)\n",
    "    \n",
    "    # row の名前を作成\n",
    "    row_names = []\n",
    "    for cond in CONDITIONS:\n",
    "        for level in LEVELS:\n",
    "            row_names.append(f'{sids}_{cond}_{level}')\n",
    "            \n",
    "    print(len(row_names))\n",
    "    # pd に追加\n",
    "    sub['row_id'] = row_names\n",
    "    sub[submission_labels] = y_pred\n",
    "    \n",
    "    del x, y_pred\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff295ccb1d757a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418749bc6f4f889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
