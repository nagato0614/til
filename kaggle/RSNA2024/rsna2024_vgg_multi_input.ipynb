{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import pydicom as dicom\n",
    "import pydicom\n",
    "import json\n",
    "import glob\n",
    "import collections\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut\n",
    "import cv2\n",
    "from skimage import measure\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "from glob import glob\n",
    "import warnings\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import ssl\n",
    "from keras.src.layers import Flatten, Dense, Concatenate\n",
    "from keras import Model, Input\n",
    "from keras.src.utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "import gc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "# label_coordinates_df = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_label_coordinates.csv')\n",
    "# train_series = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train_series_descriptions.csv')\n",
    "# df_train = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/train.csv')\n",
    "# df_sub = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/sample_submission.csv')\n",
    "# test_series = pd.read_csv('/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_series_descriptions.csv')\n",
    "\n",
    "base_path = '/Volumes/SSD/rsna-2024-lumbar-spine-degenerative-classification'\n",
    "train_png_file_path = f'{base_path}/train_dataset_png'\n",
    "\n",
    "label_coordinates_df = pd.read_csv(f'{base_path}/train_label_coordinates.csv')\n",
    "train_series = pd.read_csv(f'{base_path}/train_series_descriptions.csv')\n",
    "df_train = pd.read_csv(f'{base_path}/train.csv')\n",
    "df_sub = pd.read_csv(f'{base_path}/sample_submission.csv')\n",
    "test_series = pd.read_csv(f'{base_path}/test_series_descriptions.csv')\n",
    "\n",
    "train_dicom = f'{base_path}/train_images'\n",
    "test_dicom = f'{base_path}/test_images'\n",
    "\n",
    "model_path = f'model.keras'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_series"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# パラメータ\n",
    "CROP_SIZE = 64\n",
    "STUDY_NUM = 1975\n",
    "\n",
    "# 出力のラベル\n",
    "NORMAL_MILD = 0\n",
    "MODERATE = 1\n",
    "SEVERE = 2\n",
    "\n",
    "# 重症度一覧\n",
    "SEVERITY_LIST = ['Normal/Mild', 'Moderate', 'Severe']\n",
    "\n",
    "# 学習用パラメータ\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 50\n",
    "TRAIN_RATIO = 0.8\n",
    "VALID_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "# 入出力の形状\n",
    "INPUT_WIDTH = 512\n",
    "INPUT_HEIGHT = 512\n",
    "INPUT_CHANNEL = 20"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "train_series[train_series['study_id'] == 2492114990]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_target_name = \"right_subarticular_stenosis_l5_s1\"\n",
    "\n",
    "print(f'train shape : {df_train.shape}')\n",
    "print(df_train[_target_name].unique())\n",
    "\n",
    "# それぞれの重症度の数がいくらあるか計算\n",
    "print(f'Normal/Mild : {df_train[df_train[_target_name] == str(\"Normal/Mild\")].shape[0]}')\n",
    "print(f'Moderate : {df_train[df_train[_target_name] == str(\"Moderate\")].shape[0]}')\n",
    "print(f'Severe : {df_train[df_train[_target_name] == str(\"Severe\")].shape[0]}')\n",
    "\n",
    "print(\n",
    "    f'Normal/Mild ratio : {df_train[df_train[_target_name] == str(\"Normal/Mild\")].shape[0] / df_train.shape[0]}')\n",
    "print(\n",
    "    f'Moderate ratio : {df_train[df_train[_target_name] == str(\"Moderate\")].shape[0] / df_train.shape[0]}')\n",
    "print(\n",
    "    f'Severe ratio : {df_train[df_train[_target_name] == str(\"Severe\")].shape[0] / df_train.shape[0]}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "trusted": true
   },
   "source": [
    "print(label_coordinates_df[label_coordinates_df['study_id'] == 4646740].shape)\n",
    "label_coordinates_df[label_coordinates_df['study_id'] == 4646740]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T10:39:24.441374Z",
     "iopub.status.busy": "2024-06-17T10:39:24.440991Z",
     "iopub.status.idle": "2024-06-17T10:39:24.448384Z",
     "shell.execute_reply": "2024-06-17T10:39:24.447081Z",
     "shell.execute_reply.started": "2024-06-17T10:39:24.441343Z"
    },
    "trusted": true
   },
   "source": [
    "label_coordinates_df.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def resize_volume_with_opencv(volume, new_shape):\n",
    "    resized_volume = np.zeros((new_shape[0], new_shape[1], new_shape[2]), dtype=volume.dtype)\n",
    "\n",
    "    for i in range(volume.shape[0]):\n",
    "        resized_volume[i] = cv2.resize(volume[i], (new_shape[2], new_shape[1]),\n",
    "                                       interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return resized_volume\n",
    "\n",
    "\n",
    "# serial 画像を一つの配列としてまとめる, (WIDTH, HEIGHT, CHANNEL)の形にする\n",
    "def merge_dicom_images(study_id, series_id, file_kind='train',\n",
    "                       image_size=(INPUT_WIDTH, INPUT_HEIGHT)):\n",
    "    # 画像ファイルのパスを取得\n",
    "    if file_kind == 'train':\n",
    "        _file_path = f'{train_dicom}/{study_id}/{series_id}/*.dcm'\n",
    "    else:\n",
    "        _file_path = f'{test_dicom}/{study_id}/{series_id}/*.dcm'\n",
    "    dicom_files = glob(_file_path)\n",
    "\n",
    "    if len(dicom_files) == 0:\n",
    "        raise ValueError('dicom file not found')\n",
    "\n",
    "    images = []\n",
    "    for file in dicom_files:\n",
    "        # 画像ファイルを読み込む\n",
    "        image = dicom.read_file(file).pixel_array\n",
    "\n",
    "        # 0~1 に正規化\n",
    "        image = image / np.max(image)\n",
    "\n",
    "        # 0~255 にスケーリングして, int8 に変換\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "\n",
    "        image = cv2.resize(image, (image_size[0], image_size[1])).astype(np.uint8)\n",
    "        images.append(image)\n",
    "\n",
    "    # 画像ファイルを読み込む\n",
    "    images = np.array(images)\n",
    "\n",
    "    # arrayの経常を変更\n",
    "    images = images.transpose(1, 2, 0)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "# dicom画像を指定した枚数に整形する\n",
    "def reshape_dicom_img(dicom_img_array: np.ndarray, target_shape: int):\n",
    "    channel = dicom_img_array.shape[2]\n",
    "    output_dicom_img = dicom_img_array\n",
    "    if channel < target_shape:\n",
    "        # 画像の枚数が足りない場合は、最後の画像をコピーして追加\n",
    "        padding_frame = np.zeros((dicom_img_array.shape[0], dicom_img_array.shape[1], 1)).astype(\n",
    "            np.uint8)\n",
    "        for i in range(target_shape - channel):\n",
    "            output_dicom_img = np.append(output_dicom_img, padding_frame, axis=2)\n",
    "    elif channel == target_shape:\n",
    "        # 画像の枚数がちょうどの場合はそのまま返す\n",
    "        pass\n",
    "    else:\n",
    "        # 画像の枚数が多い場合は、前からtarget_shape枚を取得\n",
    "        output_dicom_img = output_dicom_img[:, :, :target_shape]\n",
    "\n",
    "    return output_dicom_img\n",
    "\n",
    "\n",
    "# 一つのstudy_id に複数のseries_id が存在する場合を見つける\n",
    "def find_multi_series_study_id():\n",
    "    multi_series_study_id = []\n",
    "    for study_id in train_series['study_id'].unique():\n",
    "        series_ids = train_series[train_series['study_id'] == study_id]['series_id']\n",
    "        if len(series_ids) > 3:\n",
    "            multi_series_study_id.append(study_id)\n",
    "    return multi_series_study_id[0]\n",
    "\n",
    "\n",
    "# 指定した study_id から 'Sagittal T1' 'Axial T2' 'Sagittal T2/STIR' の画像をそれぞれ取得\n",
    "def get_dicom_img_set(study_id, file_kind='train'):\n",
    "    # 画像の種類\n",
    "    image_types = ['Sagittal T1', 'Axial T2', 'Sagittal T2/STIR']\n",
    "\n",
    "    # study_id に対応する series_id を取得\n",
    "    if file_kind == 'train':\n",
    "        series_ids = train_series[train_series['study_id'] == study_id]\n",
    "    else:\n",
    "        series_ids = test_series[test_series['study_id'] == study_id]\n",
    "\n",
    "    dicom_image_dict = {}\n",
    "    for img_type in image_types:\n",
    "        series_id = series_ids[series_ids['series_description'] == img_type]['series_id']\n",
    "\n",
    "        dicom_image = np.zeros((INPUT_WIDTH, INPUT_HEIGHT, INPUT_CHANNEL)).astype(np.uint8)\n",
    "        if len(series_id) != 0:\n",
    "            # 画像を取得して整形\n",
    "            dicom_image = merge_dicom_images(study_id, series_id.values[0], file_kind)\n",
    "            dicom_image = reshape_dicom_img(dicom_image, INPUT_CHANNEL)\n",
    "            print(f'{img_type} : {dicom_image.dtype}')\n",
    "        dicom_image_dict[img_type] = dicom_image\n",
    "\n",
    "    # numpy 配列に変換\n",
    "    dicom_image_dict = np.array([dicom_image_dict['Sagittal T1'],\n",
    "                                 dicom_image_dict['Axial T2'],\n",
    "                                 dicom_image_dict['Sagittal T2/STIR']])\n",
    "\n",
    "    return dicom_image_dict\n",
    "\n",
    "\n",
    "# study_id を配列として取得\n",
    "def get_study_id_array(file_kind='train') -> np.ndarray:\n",
    "    if file_kind == 'train':\n",
    "        _study_ids = train_series['study_id'].unique()\n",
    "    else:\n",
    "        _study_ids = test_series['study_id'].unique()\n",
    "    return _study_ids\n",
    "\n",
    "\n",
    "# 学習用データの study_id を取得\n",
    "def get_train_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 0.8 で分割\n",
    "    train_study_ids = _study_ids[:int(len(_study_ids) * 0.8)]\n",
    "    return train_study_ids\n",
    "\n",
    "\n",
    "# 検証用データの study_id を取得\n",
    "def get_valid_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 0.8 で分割\n",
    "    valid_study_ids = _study_ids[int(len(_study_ids) * 0.8):int(len(_study_ids) * 0.9)]\n",
    "    return valid_study_ids\n",
    "\n",
    "\n",
    "# テスト用データの study_id を取得 (実際のテストデータではなく, 学習データの一部をテストデータとして使用)\n",
    "def get_test_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "    print(f'start : {int(len(_study_ids) * 0.9)}')\n",
    "    # 0.8 で分割\n",
    "    test_study_ids = _study_ids[int(len(_study_ids) * 0.9):]\n",
    "    return test_study_ids\n",
    "\n",
    "\n",
    "# target 一覧を取得, targetに対して重症度が付与されている\n",
    "def get_target_list():\n",
    "    target_list = df_train.columns.to_list()[1:]\n",
    "    return target_list\n",
    "\n",
    "\n",
    "# study_id に対応する重症度一覧を取得する\n",
    "def get_target_severity(study_id):\n",
    "    target_severity = df_train[df_train['study_id'] == study_id].values[0][1:]\n",
    "    output_severity = []\n",
    "    # 文字列から数値に変換\n",
    "    for i in range(len(target_severity)):\n",
    "        if target_severity[i] == 'Normal/Mild':\n",
    "            output_severity.append(NORMAL_MILD)\n",
    "        elif target_severity[i] == 'Moderate':\n",
    "            output_severity.append(MODERATE)\n",
    "        elif target_severity[i] == 'Severe':\n",
    "            output_severity.append(SEVERE)\n",
    "        else:\n",
    "            output_severity.append(NORMAL_MILD)\n",
    "\n",
    "    return output_severity\n",
    "\n",
    "\n",
    "# すべてのstudy_id に対して重症度を取得する\n",
    "def get_all_target_severity(study_ids) -> np.ndarray:\n",
    "    _target_severity_dict = []\n",
    "    for study_id in study_ids.tolist():\n",
    "        target_severity = get_target_severity(study_id)\n",
    "        _target_severity_dict.append(target_severity)\n",
    "    _target_severity_dict = np.array(_target_severity_dict)\n",
    "    return _target_severity_dict\n",
    "\n",
    "\n",
    "def get_target_severity_from_array(start, end):\n",
    "    _target_severity_dict = []\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    if end >= STUDY_NUM:\n",
    "        end = STUDY_NUM - 1\n",
    "\n",
    "    print(f'start : {start}, end : {end}, STUDY_NUM : {STUDY_NUM}')\n",
    "    print(f'get_data_size : {end - start}')\n",
    "\n",
    "    for study_id in _study_ids[start:end]:\n",
    "        target_severity = get_target_severity(study_id)\n",
    "        _target_severity_dict.append(target_severity)\n",
    "    _target_severity_dict = np.array(_target_severity_dict)\n",
    "    return _target_severity_dict\n",
    "\n",
    "\n",
    "def get_target_severity_index(severity_index: int):\n",
    "    _index = severity_index * 100\n",
    "    _target_severity_dict = get_target_severity_from_array(_index, _index + 100)\n",
    "    return _target_severity_dict\n",
    "\n",
    "\n",
    "def get_target_severity_valid():\n",
    "    _valid_study_ids = get_valid_study_id_array()\n",
    "    _target_severity_dict = []\n",
    "    for study_id in _valid_study_ids:\n",
    "        target_severity = get_target_severity(study_id)\n",
    "        _target_severity_dict.append(target_severity)\n",
    "    _target_severity_dict = np.array(_target_severity_dict)\n",
    "    return _target_severity_dict\n",
    "\n",
    "\n",
    "def get_target_severity_test():\n",
    "    _test_study_ids = get_test_study_id_array()\n",
    "    _target_severity_dict = []\n",
    "    for study_id in _test_study_ids:\n",
    "        target_severity = get_target_severity(study_id)\n",
    "        _target_severity_dict.append(target_severity)\n",
    "    _target_severity_dict = np.array(_target_severity_dict)\n",
    "    return _target_severity_dict\n",
    "\n",
    "\n",
    "# すべてのstudy_id に対して 'Sagittal T1' 'Axial T2' 'Sagittal T2/STIR' の画像を取得する\n",
    "def get_all_dicom_img_set(study_ids, file_kind='train') -> np.ndarray:\n",
    "    dicom_img_sets = []\n",
    "    for study_id in study_ids.tolist():\n",
    "        print(f'[{study_id}] remaining : {len(study_ids) - study_ids.tolist().index(study_id)}')\n",
    "        _dicom_img_set = get_dicom_img_set(study_id, file_kind)\n",
    "        dicom_img_sets.append(_dicom_img_set)\n",
    "    dicom_img_sets = np.array(dicom_img_sets)\n",
    "    return dicom_img_sets\n",
    "\n",
    "\n",
    "def show_dicom_img_from_array(dicom_img_sets, index):\n",
    "    for i in range(3):\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        for j in range(15):\n",
    "            plt.subplot(3, 5, j + 1)\n",
    "            plt.imshow(dicom_img_sets[0, i, :, :, j], cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# dicom image を指定した範囲だけ取得する\n",
    "def get_dicom_img_from_array(start, end):\n",
    "    dicom_img_sets = []\n",
    "    if end >= STUDY_NUM:\n",
    "        end = STUDY_NUM - 1\n",
    "    print(f'start : {start}, end : {end}, STUDY_NUM : {STUDY_NUM}')\n",
    "    print(f'get_data_size : {end - start}')\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "    # 取得する範囲のstudy_id を取得\n",
    "    process_study_ids = _study_ids[start:end]\n",
    "    print(f'process_study_ids : {process_study_ids.shape}')\n",
    "\n",
    "    for study_id in process_study_ids:\n",
    "        print(f'[{study_id}] remaining : '\n",
    "              f'{len(process_study_ids) - process_study_ids.tolist().index(study_id)}')\n",
    "        _dicom_img_set = get_dicom_img_set(study_id)\n",
    "        print(f'_dicom_img_set : {_dicom_img_set.dtype}')\n",
    "        dicom_img_sets.append(_dicom_img_set)\n",
    "    dicom_img_sets = np.array(dicom_img_sets)\n",
    "    return dicom_img_sets"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# dicom画像を取得, 情報を表示\n",
    "dicom_img_set = get_dicom_img_set(3008676218)\n",
    "print(f'dicom_img_set : {dicom_img_set.shape}')\n",
    "print(f'dicom image type: {dicom_img_set.dtype}')\n",
    "\n",
    "# dicom_img_setのサイズを取得 (byte)\n",
    "print(f'dicom_img_set size : {dicom_img_set.nbytes / 1000000000}[GB]')\n",
    "\n",
    "# データセット全体のサイズを取得 (byte)\n",
    "print(f'dicom_img_set size : {dicom_img_set.nbytes * 1975 / (10 ** 9)}[GB]')\n",
    "\n",
    "# 3 つの画像を表示\n",
    "plt.subplots(1, 3, figsize=(20, 20))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(dicom_img_set[i, :, :, 9], cmap='gray')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# すべてのstudy_idを取得する\n",
    "study_ids = get_study_id_array()\n",
    "print(f'study_ids : {study_ids.shape}')\n",
    "\n",
    "# すべてのstudy_idに対して重症度を取得する. 今回は一つだけ分類するので、一つの重症度を取得する\n",
    "target_severity_dict = get_all_target_severity(study_ids)\n",
    "print(f'target_severity_dict : {target_severity_dict.shape}')\n",
    "\n",
    "target_list = get_target_list()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# すべてのstudy_idに対して 'Sagittal T1' 'Axial T2' 'Sagittal T2/STIR' の画像を取得する\n",
    "# dicom_img_sets = get_all_dicom_img_set(study_ids)\n",
    "\n",
    "# dicom画像を読み込む\n",
    "# dicom_img_sets = np.load('dicom_img_set.npy')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def convert_dicom_img_to_npy(index: int):\n",
    "    start = index * 100\n",
    "    end = index * 100 + 100\n",
    "    num_of_train_study = len(get_train_study_id_array())\n",
    "\n",
    "    # start が範囲外の場合は,処理を終了\n",
    "    if start >= num_of_train_study:\n",
    "        return\n",
    "\n",
    "    if end >= num_of_train_study:\n",
    "        end = num_of_train_study - 1\n",
    "    print(f'start : {start}, end : {end}, num_of_train_study : {num_of_train_study}')\n",
    "\n",
    "    dicom_img_sets = get_dicom_img_from_array(start, end)\n",
    "    print(f'dicom_img_sets : {dicom_img_sets.shape}')\n",
    "    print(f'dicom_img_sets : {dicom_img_sets.dtype}')\n",
    "    np.save(f'{base_path}/train_dicom_img_sets_{index}.npy', dicom_img_sets)\n",
    "    print(f'{base_path}/train_dicom_img_sets_{index}.npy saved')\n",
    "\n",
    "\n",
    "# dicom画像を少しずつ読み込んで.npyに変換していく (train画像)\n",
    "def convert_dicom_img_to_npy_all():\n",
    "    train_study_ids = get_train_study_id_array()\n",
    "    print(f'train_study_ids : {train_study_ids.shape}')\n",
    "    train_study_id_num = len(train_study_ids)\n",
    "    for i in range(11, train_study_id_num // 100 + 1):\n",
    "        convert_dicom_img_to_npy(i)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# train の対象になる範囲を表示\n",
    "train_study_ids = get_train_study_id_array()\n",
    "print(f'train_study_ids : {train_study_ids.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# dicom画像を少しずつ読み込んで.npyに変換していく (valid画像)\n",
    "def convert_dicom_img_to_npy_valid():\n",
    "    valid_study_ids = get_valid_study_id_array()\n",
    "    print(f'valid_study_ids : {valid_study_ids.shape}')\n",
    "    valid_dicom_img_sets = get_all_dicom_img_set(valid_study_ids)\n",
    "    np.save(f'{base_path}/valid_dicom_img_sets.npy', valid_dicom_img_sets)\n",
    "    print(f'{base_path}/valid_dicom_img_sets.npy saved')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# dicom画像を少しずつ読み込んで.npyに変換していく (test画像)\n",
    "def convert_dicom_img_to_npy_test():\n",
    "    test_study_ids = get_test_study_id_array()\n",
    "    print(f'test_study_ids : {test_study_ids.shape}')\n",
    "    test_dicom_img_sets = get_all_dicom_img_set(test_study_ids)\n",
    "    np.save(f'{base_path}/test_dicom_img_sets.npy', test_dicom_img_sets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# dicom画像を保存する\n",
    "# np.save('dicom_img_set.npy', dicom_img_sets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# train_index = 3\n",
    "\n",
    "# # データを分割する\n",
    "# train_data = np.load(f'{base_path}/train_dicom_img_sets_{train_index}.npy')\n",
    "# # valid_data = np.load(f'{base_path}/valid_dicom_img_sets.npy')\n",
    "\n",
    "# # test_data \n",
    "# print(f'train_data : {train_data.shape}')\n",
    "# # print(f'valid_data : {valid_data.shape}')\n",
    "# # print(f'test_data : {test_data.shape}')\n",
    "\n",
    "# # 学習用データを取得\n",
    "# train_sagittal_t1 = train_data[:, 0, :, :, :]\n",
    "# train_axial_t2 = train_data[:, 1, :, :, :]\n",
    "# train_sagittal_t2_stir = train_data[:, 2, :, :, :]\n",
    "\n",
    "# print(f'train_sagittal_t1 : {train_sagittal_t1.shape}')\n",
    "# print(f'train_axial_t2 : {train_axial_t2.shape}')\n",
    "# print(f'train_sagittal_t2_stir : {train_sagittal_t2_stir.shape}')\n",
    "\n",
    "# # 検証用データを取得\n",
    "# # valid_sagittal_t1 = valid_data[:, 0, :, :, :]\n",
    "# # valid_axial_t2 = valid_data[:, 1, :, :, :]\n",
    "# # valid_sagittal_t2_stir = valid_data[:, 2, :, :, :]\n",
    "\n",
    "# # テスト用データを取得\n",
    "# # test_sagittal_t1 = test_data[:, 0, :, :, :]\n",
    "# # test_axial_t2 = test_data[:, 1, :, :, :]\n",
    "# # test_sagittal_t2_stir = test_data[:, 2, :, :, :]\n",
    "\n",
    "\n",
    "# # 出力を分割\n",
    "# train_target_severity = get_target_severity_index(train_index)\n",
    "# # valid_target_severity = get_target_severity_valid()\n",
    "# test_target_severity = get_target_severity_test()\n",
    "\n",
    "# train_target_severity_list = [to_categorical(train_target_severity[:, i], num_classes=3) for i in\n",
    "#                               range(25)]\n",
    "# # valid_target_severity_list = [to_categorical(valid_target_severity[:, i], num_classes=3) for i in\n",
    "# #                               range(25)]\n",
    "# test_target_severity_list = [to_categorical(test_target_severity[:, i], num_classes=3) for i in\n",
    "#                              range(25)]\n",
    "\n",
    "# train_datasets = [train_sagittal_t1, train_axial_t2, train_sagittal_t2_stir]\n",
    "# # valid_datasets = [valid_sagittal_t1, valid_axial_t2, valid_sagittal_t2_stir]\n",
    "\n",
    "# del train_data\n",
    "# # del valid_data\n",
    "# gc.collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from keras.src import backend\n",
    "from keras.src import layers\n",
    "from keras.src.api_export import keras_export\n",
    "from keras.src.applications import imagenet_utils\n",
    "from keras.src.models import Functional\n",
    "from keras.src.ops import operation_utils\n",
    "from keras.src.utils import file_utils\n",
    "\n",
    "\n",
    "def VGG16(\n",
    "        include_top=True,\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=None,\n",
    "        input_shape=None,\n",
    "        pooling=None,\n",
    "        classes=1000,\n",
    "        classifier_activation=\"softmax\",\n",
    "):\n",
    "    if not (weights in {\"imagenet\", None} or file_utils.exists(weights)):\n",
    "        raise ValueError(\n",
    "            \"The `weights` argument should be either \"\n",
    "            \"`None` (random initialization), 'imagenet' \"\n",
    "            \"(pre-training on ImageNet), \"\n",
    "            \"or the path to the weights file to be loaded.  Received: \"\n",
    "            f\"weights={weights}\"\n",
    "        )\n",
    "\n",
    "    if weights == \"imagenet\" and include_top and classes != 1000:\n",
    "        raise ValueError(\n",
    "            \"If using `weights='imagenet'` with `include_top=True`, \"\n",
    "            \"`classes` should be 1000.  \"\n",
    "            f\"Received classes={classes}\"\n",
    "        )\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not backend.is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    # Get the number of input channels\n",
    "    input_channels = input_shape[-1]\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(\n",
    "        64, (3, 3), activation=\"relu\", padding=\"same\", name=\"block1_conv1\",\n",
    "        input_shape=(None, None, input_channels)\n",
    "    )(img_input)\n",
    "    x = layers.Conv2D(\n",
    "        64, (3, 3), activation=\"relu\", padding=\"same\", name=\"block1_conv2\"\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block1_pool\")(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(\n",
    "        128, (3, 3), activation=\"relu\", padding=\"same\", name=\"block2_conv1\"\n",
    "    )(x)\n",
    "    x = layers.Conv2D(\n",
    "        128, (3, 3), activation=\"relu\", padding=\"same\", name=\"block2_conv2\"\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block2_pool\")(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(\n",
    "        256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv1\"\n",
    "    )(x)\n",
    "    x = layers.Conv2D(\n",
    "        256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv2\"\n",
    "    )(x)\n",
    "    x = layers.Conv2D(\n",
    "        256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv3\"\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block3_pool\")(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = layers.Conv2D(\n",
    "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv1\"\n",
    "    )(x)\n",
    "    x = layers.Conv2D(\n",
    "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv2\"\n",
    "    )(x)\n",
    "    x = layers.Conv2D(\n",
    "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv3\"\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block4_pool\")(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = layers.Conv2D(\n",
    "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv1\"\n",
    "    )(x)\n",
    "    x = layers.Conv2D(\n",
    "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv2\"\n",
    "    )(x)\n",
    "    x = layers.Conv2D(\n",
    "        512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv3\"\n",
    "    )(x)\n",
    "    x = layers.MaxPooling2D((2, 2), strides=(2, 2), name=\"block5_pool\")(x)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = layers.Flatten(name=\"flatten\")(x)\n",
    "        x = layers.Dense(4096, activation=\"relu\", name=\"fc1\")(x)\n",
    "        x = layers.Dense(4096, activation=\"relu\", name=\"fc2\")(x)\n",
    "\n",
    "        imagenet_utils.validate_activation(classifier_activation, weights)\n",
    "        x = layers.Dense(\n",
    "            classes, activation=classifier_activation, name=\"predictions\"\n",
    "        )(x)\n",
    "    else:\n",
    "        if pooling == \"avg\":\n",
    "            x = layers.GlobalAveragePooling2D()(x)\n",
    "        elif pooling == \"max\":\n",
    "            x = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = operation_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    # Create model.\n",
    "    model = Functional(inputs, x, name=\"vgg16\")\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# VGG16エンコーダを準備\n",
    "def create_vgg_encoder(input_shape):\n",
    "    vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    for layer in vgg_base.layers:\n",
    "        layer.trainable = False\n",
    "    flatten = Flatten()(vgg_base.output)\n",
    "    model = Model(inputs=vgg_base.input, outputs=flatten)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model(input_shape):\n",
    "    # 2つのVGGエンコーダを作成\n",
    "    vgg_encoder1 = create_vgg_encoder(input_shape)\n",
    "    vgg_encoder2 = create_vgg_encoder(input_shape)\n",
    "    vgg_encoder3 = create_vgg_encoder(input_shape)\n",
    "\n",
    "    # 入力の定義\n",
    "    input1 = Input(shape=input_shape)\n",
    "    input2 = Input(shape=input_shape)\n",
    "    input3 = Input(shape=input_shape)\n",
    "\n",
    "    # VGGエンコーダーの出力を取得\n",
    "    features1 = vgg_encoder1(input1)\n",
    "    features2 = vgg_encoder2(input2)\n",
    "    features3 = vgg_encoder3(input3)\n",
    "\n",
    "    # 特徴ベクトルを結合\n",
    "    combined_features = Concatenate()([features1, features2, features3])\n",
    "\n",
    "    # 分類器を作成\n",
    "    x = Dense(256, activation='relu')(combined_features)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "\n",
    "    outputs = []\n",
    "    metrics = {}\n",
    "    target_list = get_target_list()\n",
    "    for target_name in target_list:\n",
    "        print(f'target_name : {target_name}')\n",
    "        outputs.append(Dense(3, activation='softmax', name=target_name)(x))\n",
    "        metrics[target_name] = 'accuracy'\n",
    "\n",
    "    # モデルを定義\n",
    "    model = Model(inputs=[input1, input2, input3], outputs=outputs)\n",
    "\n",
    "    # モデルのコンパイル\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "                  metrics=metrics)\n",
    "\n",
    "    # モデルのサマリーを表示\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "input_shape = (INPUT_WIDTH, INPUT_HEIGHT, INPUT_CHANNEL)\n",
    "print(f'input_shape : {input_shape}')\n",
    "model = create_model(input_shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class CustomCheckpoint(Callback):\n",
    "    def __init__(self, filepath, period=1):\n",
    "        super(CustomCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.period = period\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.period == 0:\n",
    "            self.model.save_weights(self.filepath, overwrite=True)\n",
    "            print(f'Saved model to {self.filepath}')\n",
    "\n",
    "\n",
    "# カスタムチェックポイントのコールバックを作成\n",
    "custom_checkpoint = CustomCheckpoint(filepath='model_checkpoint.keras', period=50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.load_weights(model_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 学習\n",
    "valid_data = np.load(f'{base_path}/valid_dicom_img_sets.npy')\n",
    "# 検証用データを取得\n",
    "valid_sagittal_t1 = valid_data[:, 0, :, :, :]\n",
    "valid_axial_t2 = valid_data[:, 1, :, :, :]\n",
    "valid_sagittal_t2_stir = valid_data[:, 2, :, :, :]\n",
    "\n",
    "valid_target_severity = get_target_severity_valid()\n",
    "valid_datasets = [valid_sagittal_t1, valid_axial_t2, valid_sagittal_t2_stir]\n",
    "valid_target_severity_list = [to_categorical(valid_target_severity[:, i], num_classes=3) for i in\n",
    "                              range(25)]\n",
    "\n",
    "del valid_data\n",
    "gc.collect()\n",
    "\n",
    "for i in range(15):\n",
    "  train_index = i\n",
    "\n",
    "  # データを分割する\n",
    "  train_data = np.load(f'{base_path}/train_dicom_img_sets_{train_index}.npy')\n",
    "\n",
    "  # test_data \n",
    "  print(f'train_data : {train_data.shape}')\n",
    "  # print(f'valid_data : {valid_data.shape}')\n",
    "  # print(f'test_data : {test_data.shape}')\n",
    "\n",
    "  # 学習用データを取得\n",
    "  train_sagittal_t1 = train_data[:, 0, :, :, :]\n",
    "  train_axial_t2 = train_data[:, 1, :, :, :]\n",
    "  train_sagittal_t2_stir = train_data[:, 2, :, :, :]\n",
    "\n",
    "  print(f'train_sagittal_t1 : {train_sagittal_t1.shape}')\n",
    "  print(f'train_axial_t2 : {train_axial_t2.shape}')\n",
    "  print(f'train_sagittal_t2_stir : {train_sagittal_t2_stir.shape}')\n",
    "\n",
    "\n",
    "  # テスト用データを取得\n",
    "  # test_sagittal_t1 = test_data[:, 0, :, :, :]\n",
    "  # test_axial_t2 = test_data[:, 1, :, :, :]\n",
    "  # test_sagittal_t2_stir = test_data[:, 2, :, :, :]\n",
    "\n",
    "\n",
    "  # 出力を分割\n",
    "  train_target_severity = get_target_severity_index(train_index)\n",
    "  # test_target_severity = get_target_severity_test()\n",
    "\n",
    "  train_target_severity_list = [to_categorical(train_target_severity[:, i], num_classes=3) for i in\n",
    "                                range(25)]\n",
    "\n",
    "  # test_target_severity_list = [to_categorical(test_target_severity[:, i], num_classes=3) for i in\n",
    "                              #  range(25)]\n",
    "\n",
    "  train_datasets = [train_sagittal_t1, train_axial_t2, train_sagittal_t2_stir]\n",
    "\n",
    "  del train_data\n",
    "  gc.collect()\n",
    "  history = model.fit(train_datasets, train_target_severity_list, batch_size=BATCH_SIZE,\n",
    "                      epochs=20,\n",
    "                      callbacks=[custom_checkpoint],\n",
    "                      validation_data=(valid_datasets, valid_target_severity_list)\n",
    "                      )\n",
    "  \n",
    "  # このループで取得した train データを削除\n",
    "  del train_sagittal_t1\n",
    "  del train_axial_t2\n",
    "  del train_sagittal_t2_stir\n",
    "  del train_target_severity\n",
    "  del train_target_severity_list\n",
    "  del train_datasets\n",
    "  gc.collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.save('model.keras')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 学習結果をプロット\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.load_weights(model_path)\n",
    "test_dataset = np.load(f'{base_path}/test_dicom_img_sets.npy')\n",
    "\n",
    "# テスト用データを取得\n",
    "test_sagittal_t1 = test_dataset[:, 0, :, :, :]\n",
    "test_axial_t2 = test_dataset[:, 1, :, :, :]\n",
    "test_sagittal_t2_stir = test_dataset[:, 2, :, :, :]\n",
    "test_datasets = [test_sagittal_t1, test_axial_t2, test_sagittal_t2_stir]\n",
    "# testデータで予測\n",
    "model.load_weights('model.keras')\n",
    "test_predict = model.predict(test_datasets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(np.array(test_predict).shape)\n",
    "print(test_dataset.shape)\n",
    "test_target_severity_list = get_target_severity_test()\n",
    "test_predict_array = np.array(test_predict)\n",
    "print(f'test_target_severity_list : {test_target_severity_list.shape}')\n",
    "print(f'test_predict_array : {test_predict_array.shape}')\n",
    "\n",
    "# arrayを並び替え\n",
    "test_predict_array = test_predict_array.transpose(1, 0, 2)\n",
    "print(f'test_predict_array : {test_predict_array.shape}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# 正答率を計算\n",
    "correct_count = 0\n",
    "\n",
    "# 重症度が高いものの正答率を計算\n",
    "high_severity_correct_count = 0\n",
    "high_severity_count = 0\n",
    "\n",
    "# ターゲットの名前一覧\n",
    "target_name_list = df_train.columns.to_list()[1:]\n",
    "print(f'target_name_list : {target_name_list}')\n",
    "\n",
    "# ターゲットごとの正答率を求めるため正答数をカウント\n",
    "target_correct_count = {}\n",
    "for name in target_name_list:\n",
    "    target_correct_count[name] = 0\n",
    "\n",
    "# サイズを表示\n",
    "print(f'test_target_severity_list : {test_target_severity_list.shape}')\n",
    "print(f'test_predict_array : {test_predict_array.shape}')\n",
    "\n",
    "for i in range(test_target_severity_list.shape[0]):\n",
    "    for j in range(25):\n",
    "        target_name = target_name_list[j]\n",
    "        # print(f'i, j : {i}, {j}')\n",
    "        # print(f'test_target_severity_list : {test_target_severity_list[i][j]}')\n",
    "        # print(f'test_predict : {np.argmax(test_predict_array[i][j])}')\n",
    "        if test_target_severity_list[i][j] == np.argmax(test_predict_array[i][j]):\n",
    "            correct_count += 1\n",
    "            target_correct_count[target_name] += 1\n",
    "\n",
    "        # 重症度が高いものの正答率を計算\n",
    "        if (test_target_severity_list[i][j] == SEVERE or\n",
    "                test_target_severity_list[i][j] == MODERATE):\n",
    "            high_severity_count += 1\n",
    "            if test_target_severity_list[i][j] == np.argmax(test_predict_array[i][j]):\n",
    "                high_severity_correct_count += 1\n",
    "\n",
    "print(f'correct_count : {correct_count}')\n",
    "print(f'len(test_target_severity_list) : {(test_target_severity_list.shape[0] * 25)}')\n",
    "\n",
    "accuracy = correct_count / (test_target_severity_list.shape[0] * 25)\n",
    "print(f'accuracy : {accuracy}')\n",
    "\n",
    "# ターゲットごとの正答率を表示\n",
    "for name in target_name_list:\n",
    "    target_accuracy = target_correct_count[name] / test_target_severity_list.shape[0]\n",
    "    print(f'{name} : {target_accuracy}')\n",
    "\n",
    "if high_severity_count != 0:\n",
    "    print(f'high_severity_count : {high_severity_count}')\n",
    "    high_severity_accuracy = high_severity_correct_count / high_severity_count\n",
    "    print(f'high_severity_accuracy : {high_severity_accuracy}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# acc の変異を出力する\n",
    "for key, value in history.history.items():\n",
    "    print(key)\n",
    "    plt.plot(value, label=key)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# kaggle が提供しているテストデータからsubmissionを作成する\n",
    "def submission():\n",
    "    model.load_weights(model_path)\n",
    "    print(test_series)\n",
    "\n",
    "    # テストデータの study_id 一覧を取得\n",
    "    # test_study_ids = test_series['study_id'].unique()\n",
    "    test_study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 提出用のデータを作成 (row_id,normal_mild,moderate,severe)\n",
    "    submission_df = pd.DataFrame(columns=['row_id', 'normal_mild', 'moderate', 'severe'])\n",
    "\n",
    "    # testデータのdicom画像一覧を取得\n",
    "    for study_id in test_study_ids:\n",
    "        print(\n",
    "            f'[{study_id}] remaining : {len(test_study_ids) - test_study_ids.tolist().index(study_id)}')\n",
    "        test_dicom_image_list = []\n",
    "        # test_dicom_image_list.append(get_dicom_img_set(study_id, 'test'))\n",
    "        test_dicom_image_list.append(get_dicom_img_set(study_id, 'train'))\n",
    "        test_dicom_image_list = np.array(test_dicom_image_list)\n",
    "        print(f'test_dicom_image_list : {test_dicom_image_list.shape}')\n",
    "\n",
    "        lb_dataset = [\n",
    "            test_dicom_image_list[:, 0, :, :, :],\n",
    "            test_dicom_image_list[:, 1, :, :, :],\n",
    "            test_dicom_image_list[:, 2, :, :, :],\n",
    "        ]\n",
    "\n",
    "        # テストデータを予測\n",
    "        test_predict = model.predict(lb_dataset)\n",
    "        test_predict_array = np.array(test_predict)\n",
    "\n",
    "        # 並び替え\n",
    "        test_predict_array = test_predict_array.transpose(1, 0, 2)\n",
    "\n",
    "        print(f'test_predict_array : {test_predict_array.shape}')\n",
    "\n",
    "        # ターゲットの名前一覧\n",
    "        target_name_list = df_train.columns.to_list()[1:]\n",
    "\n",
    "        for i in range(test_predict_array.shape[0]):\n",
    "            for j in range(25):\n",
    "                # row_id の戦闘に study_id を追加\n",
    "                row_id = f'{test_study_ids[i]}_{target_name_list[j]}'\n",
    "                normal_mild = test_predict_array[i][j][0].round(5)\n",
    "                moderate = test_predict_array[i][j][1].round(5)\n",
    "                severe = test_predict_array[i][j][2].round(5)\n",
    "                submission_df.loc[len(submission_df)] = [row_id, normal_mild, moderate, severe]\n",
    "\n",
    "        # メモリ解放\n",
    "        test_dicom_image_list = None\n",
    "        del test_dicom_image_list\n",
    "        gc.collect()\n",
    "\n",
    "    # 提出用のデータを保存\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "submission()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# submission を表示\n",
    "submission_df = pd.read_csv('submission.csv')\n",
    "submission_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8561470,
     "sourceId": 71549,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
