{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "from keras import Model, Input\n",
    "from keras.src import backend\n",
    "from keras.src import layers\n",
    "from keras.src.applications import imagenet_utils\n",
    "from keras.src.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.src.models import Functional\n",
    "from keras.src.ops import operation_utils\n",
    "from keras.src.utils import file_utils\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.src.applications.densenet import DenseNet201\n",
    "from keras.src.callbacks import Callback\n",
    "import gc\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.ops import clip_ops, math_ops\n",
    "from scipy.ndimage import gaussian_filter, rotate, zoom\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/toru/PycharmProjects/rsna-2024-lumbar-spine-degenerative-classification'\n",
    "train_png_file_path = f'{base_path}/train_dataset_png'\n",
    "\n",
    "label_coordinates_df = pd.read_csv(f'{base_path}/train_label_coordinates.csv')\n",
    "train_series = pd.read_csv(f'{base_path}/train_series_descriptions.csv')\n",
    "df_train = pd.read_csv(f'{base_path}/train.csv')\n",
    "df_sub = pd.read_csv(f'{base_path}/sample_submission.csv')\n",
    "test_series = pd.read_csv(f'{base_path}/test_series_descriptions.csv')\n",
    "\n",
    "train_dicom = f'{base_path}/train_images'\n",
    "test_dicom = f'{base_path}/test_images'\n",
    "\n",
    "model_path = f'model.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一つのstudy_idにseries_id が3つ未満のstudy_idを探す\n",
    "study_id_list = train_series['study_id'].unique()\n",
    "for study_id in study_id_list:\n",
    "    series_id_list = train_series[train_series['study_id'] == study_id][\n",
    "        'series_description'].unique()\n",
    "\n",
    "    if len(series_id_list) < 3:\n",
    "        print(f'study_id : {study_id}, series_id : {series_id_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ\n",
    "STUDY_NUM = 1975\n",
    "\n",
    "# 出力のラベル\n",
    "NORMAL_MILD = 0\n",
    "MODERATE = 1\n",
    "SEVERE = 2\n",
    "\n",
    "# 重症度一覧\n",
    "SEVERITY_LIST = ['Normal/Mild', 'Moderate', 'Severe']\n",
    "\n",
    "# dicomデータの種類\n",
    "SERIES_DESCRIPTION_LIST = train_series['series_description'].unique().tolist()\n",
    "# 学習用パラメータ\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1000\n",
    "PATIENCE = 5 # 早期終了のパラメータ\n",
    "TRAIN_RATIO = 0.8\n",
    "VALID_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "Lr = 1e-5 * (1.0 / BATCH_SIZE)\n",
    "\n",
    "# 入出力の形状 (DenseNet201の入力サイズ)\n",
    "INPUT_WIDTH = 512\n",
    "INPUT_HEIGHT = 512\n",
    "INPUT_CHANNEL_DICT = {\n",
    "    'Sagittal T1': 17,\n",
    "    'Sagittal T2/STIR': 17,\n",
    "    'Axial T2': 29\n",
    "}\n",
    "IN_CHANS = sum(INPUT_CHANNEL_DICT.values())\n",
    "IMG_SIZE = [INPUT_WIDTH, INPUT_HEIGHT]\n",
    "\n",
    "CONDITIONS = [\n",
    "    'spinal_canal_stenosis',\n",
    "    'left_neural_foraminal_narrowing',\n",
    "    'right_neural_foraminal_narrowing',\n",
    "    'left_subarticular_stenosis',\n",
    "    'right_subarticular_stenosis'\n",
    "]\n",
    "\n",
    "LEVELS = [\n",
    "    'l1_l2',\n",
    "    'l2_l3',\n",
    "    'l3_l4',\n",
    "    'l4_l5',\n",
    "    'l5_s1',\n",
    "]\n",
    "\n",
    "N_LABELS = 25\n",
    "N_CLASSES = 3 * N_LABELS\n",
    "\n",
    "data_filenames = ['train_data', 'valid_data', 'test_data']  # パラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_target_name = \"right_subarticular_stenosis_l5_s1\"\n",
    "\n",
    "print(f'train shape : {df_train.shape}')\n",
    "print(df_train[_target_name].unique())\n",
    "\n",
    "# それぞれの重症度の数がいくらあるか計算\n",
    "print(f'Normal/Mild : {df_train[df_train[_target_name] == str(\"Normal/Mild\")].shape[0]}')\n",
    "print(f'Moderate : {df_train[df_train[_target_name] == str(\"Moderate\")].shape[0]}')\n",
    "print(f'Severe : {df_train[df_train[_target_name] == str(\"Severe\")].shape[0]}')\n",
    "\n",
    "print(\n",
    "    f'Normal/Mild ratio : {df_train[df_train[_target_name] == str(\"Normal/Mild\")].shape[0] / df_train.shape[0]}')\n",
    "print(\n",
    "    f'Moderate ratio : {df_train[df_train[_target_name] == str(\"Moderate\")].shape[0] / df_train.shape[0]}')\n",
    "print(\n",
    "    f'Severe ratio : {df_train[df_train[_target_name] == str(\"Severe\")].shape[0] / df_train.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "\n",
    "def get_train_label(study_id):\n",
    "    \"\"\"\n",
    "    study_id に対応するラベルを取得 (testはデータがないのでtrainのみ)\n",
    "    \"\"\"\n",
    "\n",
    "    row = df_train[df_train['study_id'] == study_id].to_numpy()[0][1:].tolist()\n",
    "\n",
    "    row_category = np.zeros((N_LABELS, 3))\n",
    "    for i in range(N_LABELS):\n",
    "        if row[i] == 'Normal/Mild':\n",
    "            row_category[i][0] = 1\n",
    "        elif row[i] == 'Moderate':\n",
    "            row_category[i][1] = 1\n",
    "        elif row[i] == 'Severe':\n",
    "            row_category[i][2] = 1\n",
    "    # 1次元配列に変換\n",
    "    row_category = row_category.flatten()\n",
    "    return row_category\n",
    "\n",
    "\n",
    "def get_file_list(study_id, series_desc, file_kind='train'):\n",
    "    \"\"\"\n",
    "    study_id に対応するファイルリストを取得\n",
    "    \"\"\"\n",
    "    dicom_file_list = []\n",
    "    df = None\n",
    "    if file_kind == 'train':\n",
    "        df = train_series\n",
    "    elif file_kind == 'test':\n",
    "        df = test_series\n",
    "\n",
    "    # study_id に対応する series_desc のリストを取得\n",
    "    series_ids = df[df['study_id'] == study_id]\n",
    "    series_id = series_ids[series_ids['series_description'] == series_desc]['series_id'].values\n",
    "    if len(series_id) == 0:\n",
    "        return dicom_file_list\n",
    "\n",
    "    series_id = series_id[0]\n",
    "\n",
    "    target_dicom_dir_path = ''\n",
    "    if file_kind == 'train':\n",
    "        target_dicom_dir_path = f'{train_dicom}/{study_id}/{series_id}/'\n",
    "    elif file_kind == 'test':\n",
    "        target_dicom_dir_path = f'{test_dicom}/{study_id}/{series_id}/'\n",
    "    dicom_file_list = glob(target_dicom_dir_path + '/*.dcm')\n",
    "    # ファイル名が数字になっているのでソート\n",
    "    dicom_file_list = sorted(dicom_file_list, key=natural_keys)\n",
    "\n",
    "    # series_desc にt対応するファイルリストを取得\n",
    "    return dicom_file_list\n",
    "\n",
    "\n",
    "def load_dicom_img(dicom_filepath):\n",
    "    \"\"\"\n",
    "    dicomファイルを読み込む\n",
    "    \"\"\"\n",
    "    dicom_data = dicom.read_file(dicom_filepath).pixel_array\n",
    "\n",
    "    # 0-1に正規化\n",
    "    dicom_data = dicom_data / np.max(dicom_data)\n",
    "\n",
    "    # 0~255に変換\n",
    "    dicom_data = dicom_data * 255\n",
    "\n",
    "    # 512x512にリサイズ\n",
    "    dicom_data = cv2.resize(dicom_data, (INPUT_WIDTH, INPUT_HEIGHT))\n",
    "\n",
    "    # uint8 に変換\n",
    "    dicom_data = dicom_data.astype(np.uint8)\n",
    "\n",
    "    return dicom_data\n",
    "\n",
    "\n",
    "def get_dicom_input_data(study_id, file_kind='train'):\n",
    "    \"\"\"\n",
    "    study_id に対応するdicomデータを取得\n",
    "    \"\"\"\n",
    "\n",
    "    input_dicom = np.zeros((INPUT_WIDTH, INPUT_HEIGHT, IN_CHANS))\n",
    "    # Sagittal T1\n",
    "    sagittal_t1_dicom_list = get_file_list(study_id, 'Sagittal T1', file_kind)\n",
    "    if len(sagittal_t1_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Sagittal T1']):\n",
    "            if i < len(sagittal_t1_dicom_list):\n",
    "                dicom_data = load_dicom_img(sagittal_t1_dicom_list[i])\n",
    "                input_dicom[:, :, i] = dicom_data\n",
    "\n",
    "    # Sagittal T2/STIR\n",
    "    sagittal_t2_dicom_list = get_file_list(study_id, 'Sagittal T2/STIR', file_kind)\n",
    "    if len(sagittal_t2_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Sagittal T2/STIR']):\n",
    "            if i < len(sagittal_t2_dicom_list):\n",
    "                dicom_data = load_dicom_img(sagittal_t2_dicom_list[i])\n",
    "                input_dicom[:, :, i + INPUT_CHANNEL_DICT['Sagittal T1']] = dicom_data\n",
    "\n",
    "    # Axial T2\n",
    "    axial_t2_dicom_list = get_file_list(study_id, 'Axial T2', file_kind)\n",
    "    if len(axial_t2_dicom_list) > 0:\n",
    "        for i in range(INPUT_CHANNEL_DICT['Axial T2']):\n",
    "            if i < len(axial_t2_dicom_list):\n",
    "                dicom_data = load_dicom_img(axial_t2_dicom_list[i])\n",
    "                input_dicom[:, :, i + INPUT_CHANNEL_DICT['Sagittal T1'] + INPUT_CHANNEL_DICT[\n",
    "                    'Sagittal T2/STIR']] = dicom_data\n",
    "\n",
    "    # uint8 に変換\n",
    "    input_dicom = input_dicom.astype(np.int8)\n",
    "    return input_dicom\n",
    "\n",
    "\n",
    "def get_dicom_and_label(study_id, file_kind):\n",
    "    x_data = get_dicom_input_data(study_id, file_kind)\n",
    "    y_data = get_train_label(study_id)\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "def generator(batch_size, study_ids, file_kind='train', is_augmentation=False, is_shuffle=False):\n",
    "    \"\"\"\n",
    "    データセットの読み込み関数\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        if is_shuffle:\n",
    "            np.random.shuffle(study_ids)\n",
    "\n",
    "        for start in range(0, len(study_ids), batch_size):\n",
    "            gc.collect()\n",
    "\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, len(study_ids))\n",
    "            ids_train_batch = study_ids[start:end]\n",
    "\n",
    "            with ThreadPoolExecutor() as executor:\n",
    "                results = list(\n",
    "                    executor.map(lambda sid: get_dicom_and_label(sid, file_kind), ids_train_batch))\n",
    "\n",
    "            for x_data, y_data in results:\n",
    "                if is_augmentation:\n",
    "                    # 画像データにノイズを加える\n",
    "                    x_data = x_data + np.random.normal(0, 0.1, x_data.shape)\n",
    "\n",
    "                x_batch.append(x_data)\n",
    "                y_batch.append(y_data)\n",
    "\n",
    "            yield np.array(x_batch), np.array(y_batch)\n",
    "\n",
    "\n",
    "def generator_load_all(batch_size, study_ids, file_kind='train', is_augmentation=False, is_shuffle=False):\n",
    "    \"\"\"\n",
    "    実行時予めすべてのデータを読み込んだ上でデータを返す\n",
    "    \"\"\"\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    print(f'事前にすべてのデータを読み込みます')\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(\n",
    "            executor.map(lambda sid: get_dicom_and_label(sid, file_kind), study_ids))\n",
    "\n",
    "    for x_data, y_data in results:\n",
    "        x_batch.append(x_data)\n",
    "        y_batch.append(y_data)\n",
    "\n",
    "    print(f'データの読み込みが完了しました')\n",
    "    # データをバッチサイズに分割して返す\n",
    "    while True:\n",
    "        for start in range(0, len(x_batch), batch_size):\n",
    "            end = min(start + batch_size, len(study_ids))\n",
    "\n",
    "            x_batch_batch = x_batch[start:end]\n",
    "            y_batch_batch = y_batch[start:end]\n",
    "\n",
    "            if is_augmentation:\n",
    "                for i in range(len(x_batch_batch)):\n",
    "                    # ノイズを加える\n",
    "                    x_batch_batch[i] = x_batch_batch[i] + np.random.normal(0, 0.1, x_batch_batch[i].shape)\n",
    "\n",
    "            yield np.array(x_batch_batch), np.array(y_batch_batch)\n",
    "\n",
    "\n",
    "def generate_and_save_data(study_ids, file_name, file_kind='train'):\n",
    "    \"\"\"\n",
    "    学習用データを生成して保存\n",
    "    \"\"\"\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for sid in study_ids:\n",
    "        print(f'残りのデータ数 : {len(study_ids) - len(x_batch)}')\n",
    "        x_data, y_data = get_dicom_and_label(sid, file_kind)\n",
    "        x_batch.append(x_data)\n",
    "        y_batch.append(y_data)\n",
    "\n",
    "    x_batch = np.array(x_batch)\n",
    "    y_batch = np.array(y_batch)\n",
    "\n",
    "    print(f'x_batch shape : {x_batch.shape}')\n",
    "    print(f'y_batch shape : {y_batch.shape}')\n",
    "\n",
    "    np.save(f'{file_name}_x.npy', x_batch)\n",
    "    np.save(f'{file_name}_y.npy', y_batch)\n",
    "\n",
    "    # メモリ解放\n",
    "    del x_batch\n",
    "    del y_batch\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def save_train_data():\n",
    "    \"\"\"\n",
    "    学習用データを保存\n",
    "    \"\"\"\n",
    "    train_study_ids = get_train_study_id_array()\n",
    "    generate_and_save_data(train_study_ids, file_name='train_data', file_kind='train')\n",
    "\n",
    "\n",
    "def save_valid_data():\n",
    "    \"\"\"\n",
    "    検証用データを保存\n",
    "    \"\"\"\n",
    "    valid_study_ids = get_valid_study_id_array()\n",
    "    generate_and_save_data(valid_study_ids, file_name='valid_data', file_kind='train')\n",
    "\n",
    "\n",
    "def save_test_data():\n",
    "    \"\"\"\n",
    "    テスト用データを保存\n",
    "    \"\"\"\n",
    "    test_study_ids = get_test_study_id_array()\n",
    "    generate_and_save_data(test_study_ids, file_name='test_data', file_kind='train')\n",
    "\n",
    "\n",
    "# 学習用データの study_id を取得\n",
    "def get_train_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 0.8 で分割\n",
    "    train_study_ids = _study_ids[:int(len(_study_ids) * 0.8)]\n",
    "    return train_study_ids\n",
    "\n",
    "\n",
    "# 検証用データの study_id を取得\n",
    "def get_valid_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "\n",
    "    # 0.8 で分割\n",
    "    valid_study_ids = _study_ids[int(len(_study_ids) * 0.8):int(len(_study_ids) * 0.9)]\n",
    "    return valid_study_ids\n",
    "\n",
    "\n",
    "# テスト用データの study_id を取得 (実際のテストデータではなく, 学習データの一部をテストデータとして使用)\n",
    "def get_test_study_id_array():\n",
    "    _study_ids = train_series['study_id'].unique()\n",
    "    # 0.8 で分割\n",
    "    test_study_ids = _study_ids[int(len(_study_ids) * 0.9):]\n",
    "    return test_study_ids\n",
    "\n",
    "\n",
    "def generator_for_test(batch_size, study_ids):\n",
    "    \"\"\"\n",
    "    test_seriesからテスト用のデータセット作成\n",
    "    kaggle で提出時に呼び出される\n",
    "    :param batch_size: \n",
    "    :param study_ids: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    for start in range(0, len(study_ids), batch_size):\n",
    "        x_batch = []\n",
    "        end = min(start + batch_size, len(study_ids))\n",
    "        ids_train_batch = study_ids[start:end]\n",
    "\n",
    "        for sid in ids_train_batch:\n",
    "            dicom_img = get_dicom_input_data(sid, 'test')\n",
    "\n",
    "            x_batch.append(dicom_img)\n",
    "\n",
    "\n",
    "        yield np.array(x_batch), ids_train_batch\n",
    "\n",
    "\n",
    "def generate_and_save_data(study_ids, file_name, file_kind='train'):\n",
    "    \"\"\"\n",
    "    学習用データを生成して保存\n",
    "    \"\"\"\n",
    "    x_batch = []\n",
    "    y_batch = []\n",
    "    for sid in study_ids:\n",
    "        print(f'残りのデータ数 : {len(study_ids) - len(x_batch)}')\n",
    "        x_data, y_data = get_dicom_and_label(sid, file_kind)\n",
    "        x_batch.append(x_data)\n",
    "        y_batch.append(y_data)\n",
    "\n",
    "    x_batch = np.array(x_batch)\n",
    "    y_batch = np.array(y_batch)\n",
    "\n",
    "    print(f'x_batch shape : {x_batch.shape}')\n",
    "    print(f'y_batch shape : {y_batch.shape}')\n",
    "\n",
    "    np.save(f'{file_name}_x.npy', x_batch)\n",
    "    np.save(f'{file_name}_y.npy', y_batch)\n",
    "\n",
    "    # メモリ解放\n",
    "    del x_batch\n",
    "    del y_batch\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    データを読み込む\n",
    "    \"\"\"\n",
    "    x_data = np.load(f'{file_name}_x.npy')\n",
    "    y_data = np.load(f'{file_name}_y.npy')\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "def generator_with_load_data(batch_size,\n",
    "                             study_ids,\n",
    "                             file_name,\n",
    "                             file_kind='train',\n",
    "                             is_augmentation=False,\n",
    "                             is_shuffle=False,\n",
    "                             is_only_one_epoch=False):\n",
    "    \"\"\"\n",
    "    事前にデータを読み込んでデータを返す\n",
    "    \"\"\"\n",
    "    x_batch, y_batch = load_data(file_name)\n",
    "\n",
    "    def shuffle_data_and_labels(x_batch, y_batch):\n",
    "        # データとラベルをペアにしたリストを作成\n",
    "        data_label_pairs = list(zip(x_batch, y_batch))\n",
    "\n",
    "        # ペアのリストをシャッフル\n",
    "        np.random.shuffle(data_label_pairs)\n",
    "\n",
    "        # シャッフル後のデータとラベルをそれぞれ別々の配列に戻す\n",
    "        shuffled_x_batch, shuffled_y_batch = zip(*data_label_pairs)\n",
    "\n",
    "        # NumPy 配列に変換して返す（オプション）\n",
    "        shuffled_x_batch = np.array(shuffled_x_batch)\n",
    "        shuffled_y_batch = np.array(shuffled_y_batch)\n",
    "\n",
    "        return shuffled_x_batch, shuffled_y_batch\n",
    "\n",
    "    while True:\n",
    "        # epochごとにシャッフル\n",
    "        if is_shuffle:\n",
    "            x_batch, y_batch = shuffle_data_and_labels(x_batch, y_batch)\n",
    "\n",
    "        for start in range(0, len(x_batch), batch_size):\n",
    "            end = min(start + batch_size, len(study_ids))\n",
    "\n",
    "            x_batch_batch = x_batch[start:end]\n",
    "            y_batch_batch = y_batch[start:end]\n",
    "\n",
    "            if is_augmentation:\n",
    "\n",
    "                # 画像を正規化. (-があるので, offstetを加える)\n",
    "                x_batch_batch = x_batch_batch.astype(np.float32)\n",
    "                if np.min(x_batch_batch) < 0:\n",
    "                    x_batch_batch = x_batch_batch + np.abs(np.min(x_batch_batch))\n",
    "                x_batch_batch = x_batch_batch / (np.max(x_batch_batch) + 1e-6)\n",
    "\n",
    "                for i in range(len(x_batch_batch)):\n",
    "                    # ノイズを加える\n",
    "                    x_batch_batch[i] = x_batch_batch[i] + np.random.normal(0, 0.1, x_batch_batch[i].shape)\n",
    "                    \n",
    "                    # 画像をぼかす\n",
    "                    # ランダムなシグマ値を生成（例：-2.0から2.0の範囲）\n",
    "                    sigma = np.random.uniform(-1.0, 1.0)\n",
    "                    # シグマ値の絶対値を取ってガウシアンブラーを適用\n",
    "                    x_batch_batch[i] = gaussian_filter(x_batch_batch[i], sigma=np.abs(sigma))\n",
    "\n",
    "                    # 特定の範囲を切り抜く (10 ~ 50)\n",
    "                    n_cut = np.random.randint(1, 5)\n",
    "                    for _ in range(n_cut):\n",
    "                        WINDOW_SIZE = np.random.randint(10, 100)\n",
    "                        x_min = np.random.randint(0, x_batch_batch[i].shape[0] - WINDOW_SIZE)\n",
    "                        x_max = x_min + WINDOW_SIZE\n",
    "                        y_min = np.random.randint(0, x_batch_batch[i].shape[1] - WINDOW_SIZE)\n",
    "                        y_max = y_min + WINDOW_SIZE\n",
    "                        x_batch_batch[i][x_min:x_max, y_min:y_max, :] = 0\n",
    "        \n",
    "                    # 画像を上下, 左右にずらす\n",
    "                    x_batch_batch[i] = np.roll(x_batch_batch[i], np.random.randint(-10, 10), axis=0)\n",
    "                    x_batch_batch[i] = np.roll(x_batch_batch[i], np.random.randint(-10, 10), axis=1)\n",
    "\n",
    "            # 画像を正規化. (-があるので, offstetを加える)\n",
    "            x_batch_batch = x_batch_batch.astype(np.float32)\n",
    "            if np.min(x_batch_batch) < 0:\n",
    "                x_batch_batch = x_batch_batch + np.abs(np.min(x_batch_batch))\n",
    "            x_batch_batch = x_batch_batch / (np.max(x_batch_batch) + 1e-6)\n",
    "\n",
    "            # (0, 255) に変換\n",
    "            x_batch_batch = x_batch_batch * 255\n",
    "\n",
    "            # uint8 に変換\n",
    "            x_batch_batch = x_batch_batch.astype(np.uint8)\n",
    "            \n",
    "            yield np.array(x_batch_batch), np.array(y_batch_batch)\n",
    "\n",
    "        if is_only_one_epoch:\n",
    "            break\n",
    "\n",
    "    # メモリ解放\n",
    "    del x_batch\n",
    "    del y_batch\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_for_test を使ってテストデータを読み込む\n",
    "test_dataset_ids = test_series['study_id'].unique()\n",
    "for x_batch, study_ids in generator_for_test(BATCH_SIZE, test_dataset_ids):\n",
    "    print(f'x_batch shape : {x_batch.shape}')\n",
    "    print(f'study_ids : {study_ids}')\n",
    "\n",
    "    # 画像を表示\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    for i in range(1):\n",
    "        plt.imshow(x_batch[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2492114990\n",
    "for desc in SERIES_DESCRIPTION_LIST:\n",
    "    dicom_image_list = get_file_list(2492114990, desc)\n",
    "    print(f'{desc} : {len(dicom_image_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_id のリストを作成\n",
    "train_study_ids = get_train_study_id_array()\n",
    "print(len(train_study_ids))\n",
    "valid_study_ids = get_valid_study_id_array()\n",
    "print(len(valid_study_ids))\n",
    "test_study_ids = get_test_study_id_array()\n",
    "print(len(test_study_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator_with_load_dataのテスト\n",
    "for i, (x_batch, y_batch) in enumerate(\n",
    "        generator_with_load_data(\n",
    "            BATCH_SIZE,\n",
    "            train_study_ids,\n",
    "            'train_data',\n",
    "            is_augmentation=True,\n",
    "            is_shuffle=False,\n",
    "            is_only_one_epoch=True\n",
    "        )):\n",
    "    print(f'x_batch shape : {x_batch.shape}')\n",
    "    print(f'y_batch shape : {y_batch.shape}')\n",
    "\n",
    "    # 画像の最大と最小値を表示\n",
    "    print(f'x_batch max : {np.max(x_batch)}')\n",
    "    print(f'x_batch min : {np.min(x_batch)}')\n",
    "\n",
    "    # 型を表示\n",
    "    print(f'x_batch dtype : {x_batch.dtype}')\n",
    "\n",
    "    start_index = 5\n",
    "    plt.imshow(x_batch[0, :, :, start_index:start_index+3])\n",
    "    plt.show()\n",
    "\n",
    "    if i==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_train_label(4003253)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_image_list = get_file_list(4003253, 'Sagittal T1')\n",
    "\n",
    "input_image = get_dicom_input_data(4003253)\n",
    "plt.imshow(input_image[:, :, 3], cmap='gray')\n",
    "input_data_gb = input_image.nbytes / 1024 / 1024 / 1024\n",
    "print(f\"画像サイズ[GB] : {input_data_gb}\")\n",
    "print(f'データセット全体のデータ数は {input_data_gb * STUDY_NUM}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_acc(y_true, y_pred):\n",
    "    pred = tf.dtypes.cast(tf.math.greater(y_pred, 0.5), tf.float64)\n",
    "    flag = tf.dtypes.cast(tf.math.equal(y_true, pred), tf.float64)\n",
    "    return tf.reduce_prod(flag, axis=-1)\n",
    "\n",
    "\n",
    "def binary_acc(y_true, y_pred):\n",
    "    pred = tf.dtypes.cast(tf.math.greater(y_pred, 0.5), tf.float64)\n",
    "    flag = tf.dtypes.cast(tf.math.equal(y_true, pred), tf.float64)\n",
    "    return tf.reduce_mean(flag, axis=-1)\n",
    "\n",
    "def log_loss_metrics(y_true, y_pred):\n",
    "    # target を output の型にキャスト\n",
    "    y_true = tf.dtypes.cast(y_true, y_pred.dtype)\n",
    "    epsilon_ = constant_op.constant(0.00001, y_pred.dtype)\n",
    "    loss_weight = tf.constant(np.array([1, 2, 4] * 25), y_pred.dtype)\n",
    "\n",
    "    # nan を防ぐためにクリップ\n",
    "    y_pred = clip_ops.clip_by_value(y_pred, epsilon_, 0.99999)\n",
    "\n",
    "    # output を (None, 25, 3) に変換\n",
    "    y_pred = tf.reshape(y_pred, (-1, 25, 3))\n",
    "    \n",
    "    # 各行ごとにsoftmaxを適用\n",
    "    y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "\n",
    "    # (75, ) に変換\n",
    "    y_pred = tf.reshape(y_pred, (-1, 75))\n",
    "\n",
    "    # 交差エントロピーの計算\n",
    "    bce = y_true * math_ops.log(y_pred + epsilon_) * loss_weight\n",
    "    bce += (1.0 - y_true) * math_ops.log(1.0 - y_pred + epsilon_)\n",
    "    \n",
    "    # 交差エントロピーの正規化\n",
    "    bce_avg = K.mean(-bce, axis=-1)\n",
    "    return bce_avg\n",
    "\n",
    "def binary_crossentropy_balance(target, output):\n",
    "    # target を output の型にキャスト\n",
    "    target = tf.dtypes.cast(target, output.dtype)\n",
    "    epsilon_ = constant_op.constant(0.00001, output.dtype)\n",
    "    loss_weight = tf.constant(np.array([1, 2, 4] * 25), output.dtype)\n",
    "\n",
    "    # nan を防ぐためにクリップ\n",
    "    output = clip_ops.clip_by_value(output, epsilon_, 0.99999)\n",
    "\n",
    "    # output を (None, 25, 3) に変換\n",
    "    output = tf.reshape(output, (-1, 25, 3))\n",
    "    \n",
    "    # 各行ごとにsoftmaxを適用\n",
    "    output = tf.nn.softmax(output, axis=-1)\n",
    "\n",
    "    # (75, ) に変換\n",
    "    output = tf.reshape(output, (-1, 75))\n",
    "\n",
    "\n",
    "    # 交差エントロピーの計算\n",
    "    bce = target * math_ops.log(output + epsilon_) * loss_weight\n",
    "    bce += (1.0 - target) * math_ops.log(1.0 - output + epsilon_)\n",
    "\n",
    "    bce_sum = -K.sum(bce, axis=-1)\n",
    "    return bce_sum\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    # カスタム入力層\n",
    "    input_shape = (INPUT_WIDTH, INPUT_HEIGHT, IN_CHANS)\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # EfficientNetB7ベースモデル（トップ層なし）\n",
    "    base_model = tf.keras.applications.EfficientNetV2M(\n",
    "        include_top=False,\n",
    "        weights=None,\n",
    "        input_tensor=inputs,\n",
    "        input_shape=(INPUT_WIDTH, INPUT_HEIGHT, IN_CHANS),\n",
    "        pooling='avg'  # グローバルプーリングを使用して特徴を抽出\n",
    "    )\n",
    "\n",
    "    # ベースモデルの出力\n",
    "    x = base_model.output\n",
    "\n",
    "    # 出力層（クラス数に応じて調整）\n",
    "    outputs = Dense(N_CLASSES, activation='sigmoid')(x)\n",
    "\n",
    "    # モデルの作成\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # optimizer : Adam\n",
    "    opt = tf.keras.optimizers.AdamW(learning_rate=Lr)\n",
    "\n",
    "    # モデルのコンパイル\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=binary_crossentropy_balance,\n",
    "                  metrics=[binary_acc, total_acc, log_loss_metrics])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "class CustomCheckpoint(Callback):\n",
    "    def __init__(self, filepath):\n",
    "        super(CustomCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.best_val_accuracy = 1.0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_log_loss_metrics = logs.get('val_log_loss_metrics')\n",
    "\n",
    "        if val_log_loss_metrics is not None and val_log_loss_metrics < self.best_val_accuracy and val_log_loss_metrics < 0.8:\n",
    "            self.best_val_accuracy = val_log_loss_metrics\n",
    "            filepath = f'{self.filepath}_epoch{epoch + 1}_val_logloss_{val_log_loss_metrics:.4f}.keras'\n",
    "            self.model.save(filepath, overwrite=True)\n",
    "            print(f'Saved model to {filepath} with validation accuracy: {val_log_loss_metrics:.4f}')\n",
    "        else:\n",
    "            # 10 epoch ごとにモデルを保存\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                filepath = f'{self.filepath}_epoch{epoch + 1}_val_logloss_{val_log_loss_metrics:.4f}.keras'\n",
    "                self.model.save(filepath, overwrite=True)\n",
    "                print(f'Saved model to {filepath} with validation accuracy: {val_log_loss_metrics:.4f}')\n",
    "\n",
    "\n",
    "# カスタムチェックポイントのコールバックを作成\n",
    "custom_checkpoint = CustomCheckpoint(filepath='model_effi')\n",
    "Ecall = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの学習\n",
    "history = model.fit(\n",
    "    generator_with_load_data(BATCH_SIZE, train_study_ids, is_augmentation=True, is_shuffle=True,\n",
    "                             file_name='train_data'),\n",
    "                             steps_per_epoch=len(train_study_ids) // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=generator_with_load_data(BATCH_SIZE, valid_study_ids, is_augmentation=False, is_shuffle=False,\n",
    "                                             file_name='valid_data'),\n",
    "    validation_steps=len(valid_study_ids) // BATCH_SIZE,\n",
    "    callbacks=[custom_checkpoint, Ecall]\n",
    ")\n",
    "\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossとaccuracyのグラフを描画\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.title('loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['binary_acc'], label='binary_acc')\n",
    "plt.plot(history.history['val_binary_acc'], label='val_binary_acc')\n",
    "plt.legend()\n",
    "plt.title('binary_acc')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['total_acc'], label='total_acc')\n",
    "plt.plot(history.history['val_total_acc'], label='val_total_acc')\n",
    "plt.legend()\n",
    "plt.title('total_acc')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "loaded_model = tf.keras.models.load_model('model_effi_epoch3_val_logloss_0.6579.keras', custom_objects={\n",
    "    'binary_crossentropy_balance': binary_crossentropy_balance,\n",
    "    'binary_acc': binary_acc,\n",
    "    'total_acc': total_acc,\n",
    "    'log_loss_metrics': log_loss_metrics\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle でのスコアを計算 (logloss)\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for x_batch, y_batch in generator_with_load_data(BATCH_SIZE, test_study_ids, 'test_data', is_augmentation=False,\n",
    "                                                 is_shuffle=False, is_only_one_epoch=True):\n",
    "    y_true.extend(y_batch)\n",
    "\n",
    "    y_pred.extend(loaded_model.predict(x_batch, verbose=2))\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# weight を y_trueと同じ形状に変換\n",
    "weights = np.array([1, 2, 4] * 25)\n",
    "weights = np.tile(weights, (y_true.shape[0], 1))\n",
    "# y_pred を (None, 25, 3) に変換\n",
    "y_pred = np.reshape(y_pred, (-1, 25, 3))\n",
    "\n",
    "# 各行ごとにsoftmaxを適用\n",
    "y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "\n",
    "# (75, ) に変換\n",
    "y_pred = np.reshape(y_pred, (-1, 75))\n",
    "\n",
    "# logloss を計算\n",
    "logloss = log_loss(y_true.flatten(), y_pred.flatten(), sample_weight=weights.flatten(), normalize=True)\n",
    "print(f'logloss : {logloss}')\n",
    "\n",
    "# このモデルのスコアをファイル名にして保存\n",
    "model_name = f'model_{logloss:.4f}.keras'\n",
    "loaded_model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'y_pred : {y_pred[0]}')\n",
    "print(f'y_true : {y_true[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正答率を保存する\n",
    "correct_rate = {}\n",
    "for condition in CONDITIONS:\n",
    "    for level in LEVELS:\n",
    "        correct_rate[f'{condition}_{level}'] = 0\n",
    "\n",
    "for i, (x_batch, y_batch) in enumerate(\n",
    "        generator_with_load_data(\n",
    "            BATCH_SIZE,\n",
    "            test_study_ids,\n",
    "            'test_data',\n",
    "            is_augmentation=False,\n",
    "            is_shuffle=False,\n",
    "            is_only_one_epoch=True\n",
    "        )):\n",
    "    y_pred = loaded_model.predict(x_batch)\n",
    "    print(f'batch {i + 1} : {y_pred.shape}')\n",
    "\n",
    "    # 1バッチ分のデータを一つずつ取り出す\n",
    "    for j in range(min(BATCH_SIZE, len(y_pred))):\n",
    "\n",
    "        # (25, 3) に変換\n",
    "        y_pred_reshaped = y_pred[j].reshape((N_LABELS, 3))\n",
    "        y_true_reshaped = y_batch[j].reshape((N_LABELS, 3))\n",
    "\n",
    "        # 0.5 以上の値を 1 に変換\n",
    "        y_pred_reshaped = np.where(y_pred_reshaped >= 0.5, 1, 0)\n",
    "\n",
    "        # 最も大きい値を取得\n",
    "        y_pred_argmax = np.argmax(y_pred_reshaped, axis=1)\n",
    "        y_true_argmax = np.argmax(y_true_reshaped, axis=1)\n",
    "\n",
    "        # print(f'y_pred_argmax : {y_pred_argmax}')\n",
    "        # print(f'y_true_argmax : {y_true_argmax}')\n",
    "        # \n",
    "        # print(f'y_true_argmax : {y_true_argmax.shape}')\n",
    "\n",
    "        # 書くレベルと条件に対して正解しているかどうかを確認\n",
    "        for i, cond in enumerate(CONDITIONS):\n",
    "            for j, level in enumerate(LEVELS):\n",
    "                index = i * len(LEVELS) + j\n",
    "                if y_pred_argmax[index] == y_true_argmax[index]:\n",
    "                    correct_rate[f'{cond}_{level}'] += 1\n",
    "\n",
    "# 正答率を計算, condition ごとに計算, levelはまとめる\n",
    "condition_rate_list = []\n",
    "for condition in CONDITIONS:\n",
    "    rate = 0\n",
    "    for level in LEVELS:\n",
    "        rate += correct_rate[f'{condition}_{level}']\n",
    "    condition_rate_list.append(rate / (len(test_study_ids) * len(LEVELS)))\n",
    "\n",
    "# levelごとの正答率を計算\n",
    "level_rate_list = []\n",
    "for level in LEVELS:\n",
    "    rate = 0\n",
    "    for condition in CONDITIONS:\n",
    "        rate += correct_rate[f'{condition}_{level}']\n",
    "    level_rate_list.append(rate / (len(test_study_ids) * len(CONDITIONS)))\n",
    "\n",
    "# 棒グラフで表示\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(CONDITIONS, condition_rate_list)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.bar(LEVELS, level_rate_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
