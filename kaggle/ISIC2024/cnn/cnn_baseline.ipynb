{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04c122e2ff6ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import h5py\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "from keras import Model, Input\n",
    "from keras.src.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.src.callbacks import Callback\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.ops import clip_ops, math_ops\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.src import ops\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef729c7eeb009f",
   "metadata": {},
   "source": [
    "### ファイル名の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092a38f3d89cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_hdf5 = \"train-image.hdf5\"\n",
    "train_metadata_csv = \"train-metadata.csv\"\n",
    "test_image_hdf5 = \"test-image.hdf5\"\n",
    "test_metadata_csv = \"test-metadata.csv\"\n",
    "sample_submission_csv = \"sample_submission.csv\"\n",
    "\n",
    "base_path = \"/home/toru/PycharmProjects/isic-2024-challenge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e983201959b7cc",
   "metadata": {},
   "source": [
    "### パラメータの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c627a073a2de07c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"SEED\": 42,\n",
    "    \"N_FOLDS\": 5,\n",
    "    \"N_EPOCHS\": 100,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"VAL_BATCH_SIZE\": 50,\n",
    "    \"LR\": 0.001,\n",
    "    \"IMAGE_HEIGHT\": 224,\n",
    "    \"IMAGE_WIDTH\": 224,\n",
    "    \"IMAGE_CHANNEL\": 3,\n",
    "    \"N_CLASSES\": 1,\n",
    "    \"PATIENCE\": 2,\n",
    "    \"TRAIN_RATIO\": 0.8,\n",
    "    \"VAL_RATIO\": 0.1,\n",
    "    \"TEST_RATIO\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1dcef53bc67ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f\"{base_path}/{train_metadata_csv}\")\n",
    "df_test = pd.read_csv(f\"{base_path}/{test_metadata_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dda5c1f0f7b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient_id が unique になるようにデータを抽出\n",
    "df_positive = df_train[df_train[\"target\"] == 1].reset_index(drop=True)\n",
    "df_negative = df_train[df_train[\"target\"] == 0].reset_index(drop=True)\n",
    "\n",
    "df_train = pd.concat([df_positive, df_negative.iloc[:df_positive.shape[0]*20, :]])  # positive:negative = 1:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8d2eac3ae74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7fa748fdb0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_isic_ids = df_train[\"isic_id\"].values\n",
    "\n",
    "train_isic_ids_positive = df_train[df_train[\"target\"] == 1][\"isic_id\"].values\n",
    "train_isic_ids_negative = df_train[df_train[\"target\"] == 0][\"isic_id\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129daf4c358a1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ラベルが1のデータ数\n",
    "sum_of_positive = len(df_train[df_train[\"target\"] == 1])\n",
    "\n",
    "# ラベルが0のデータ数\n",
    "sum_of_negative = len(df_train[df_train[\"target\"] == 0])\n",
    "\n",
    "# ラベルが1のデータ数 / ラベルが0のデータ数\n",
    "positive_negative_ratio = sum_of_positive / sum_of_negative\n",
    "\n",
    "print(f\"positive: {sum_of_positive}\")\n",
    "print(f\"negative: {sum_of_negative}\")\n",
    "print(f\"positive/negative: {positive_negative_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b3badf132e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_and_label(hdf, isic_id):\n",
    "    # 画像を取得\n",
    "    image_data = hdf[isic_id][()]\n",
    "\n",
    "    # Convert the binary data to a numpy array\n",
    "    image_data = np.frombuffer(image_data, np.uint8)\n",
    "\n",
    "    # Decode the image from the numpy array\n",
    "    image_data = cv2.cvtColor(cv2.imdecode(image_data, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image_data = cv2.resize(image_data, (CONFIG[\"IMAGE_HEIGHT\"], CONFIG[\"IMAGE_WIDTH\"]))\n",
    "\n",
    "    # 画像の正規化 (0 ~ 1) に変換\n",
    "    image_data = (image_data - image_data.min()) / (image_data.max() - image_data.min())\n",
    "    \n",
    "    # クラスを取得する. これが学習時のラベルになる\n",
    "    label = df_train[df_train[\"isic_id\"] == isic_id][\"target\"].values[0]\n",
    "    return image_data, label\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    # 画像の正規化 (0 ~ 1) に変換\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    return image\n",
    "\n",
    "def augmentation(image):\n",
    "    \n",
    "    # 確率で画像をズーム\n",
    "    if np.random.rand() > 0.5:\n",
    "        scale = np.random.uniform(0.8, 1.2)\n",
    "        image = cv2.resize(image, (0, 0), fx=scale, fy=scale)\n",
    "        image = cv2.resize(image, (CONFIG[\"IMAGE_HEIGHT\"], CONFIG[\"IMAGE_WIDTH\"]))\n",
    "    \n",
    "    # 明るさを変える\n",
    "    alpha = 1.0 + np.random.uniform(-0.01, 0.01)\n",
    "    beta = np.random.uniform(-0.01, 0.01)\n",
    "    image = image * alpha + beta\n",
    "    \n",
    "    # 画像をぼかす\n",
    "    k_size = np.random.randint(1, 10) * 2 + 1\n",
    "    image = cv2.GaussianBlur(image, (k_size, k_size), 0)\n",
    "\n",
    "    # 確率で付与するノイズを変える\n",
    "    if np.random.rand() > 0.5:\n",
    "        # ガウシアンノイズ\n",
    "        image = image + np.random.normal(0, 0.1, image.shape)\n",
    "        image = np.clip(image, 0, 1)\n",
    "        \n",
    "    # 画像を確率で反転\n",
    "    if np.random.rand() > 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "    # 特定の範囲を切り抜く\n",
    "    n_cut = np.random.randint(1, 5)\n",
    "    for _ in range(n_cut):\n",
    "        WINDOW_SIZE = np.random.randint(10, 50)\n",
    "        x_min = np.random.randint(0, image.shape[0] - WINDOW_SIZE)\n",
    "        x_max = x_min + WINDOW_SIZE\n",
    "        y_min = np.random.randint(0, image.shape[1] - WINDOW_SIZE)\n",
    "        y_max = y_min + WINDOW_SIZE\n",
    "        image[x_min:x_max, y_min:y_max] = 0\n",
    "\n",
    "    # 画像を回転\n",
    "    angle = np.random.randint(0, 360)\n",
    "    image = cv2.warpAffine(image, cv2.getRotationMatrix2D((image.shape[1] / 2, image.shape[0] / 2), angle, 1.0),\n",
    "                           (image.shape[1], image.shape[0]))\n",
    "\n",
    "    return image\n",
    "\n",
    "def generator(isic_ids,\n",
    "              batch_size=CONFIG[\"BATCH_SIZE\"],\n",
    "              mode=\"train\",\n",
    "              is_augmentation=False,\n",
    "              is_shuffle=False,\n",
    "              is_one_epoch=False,\n",
    "              is_multi_threading=False\n",
    "              ):\n",
    "    hdf_path = None\n",
    "    if mode == \"train\":\n",
    "        hdf_path = f\"{base_path}/{train_image_hdf5}\"\n",
    "    elif mode == \"test\":\n",
    "        hdf_path = f\"{base_path}/{test_image_hdf5}\"\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'train' or 'test'\")\n",
    "    \n",
    "    def process_image_label(isic_id):\n",
    "        with h5py.File(hdf_path, \"r\") as hdf:\n",
    "            image, label = get_image_and_label(hdf, isic_id)\n",
    "            if is_augmentation:\n",
    "                image = augmentation(image)\n",
    "                \n",
    "            # 画像を正規化\n",
    "            image = normalize_image(image)\n",
    "        return image, label\n",
    "\n",
    "    while True:\n",
    "        if is_shuffle:\n",
    "            random.shuffle(isic_ids)\n",
    "\n",
    "        for i in range(0, len(isic_ids), batch_size):\n",
    "            end = min(i + batch_size, len(isic_ids))\n",
    "            batch_isic_ids = isic_ids[i:end]\n",
    "            if is_multi_threading:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    results = list(executor.map(process_image_label, batch_isic_ids))\n",
    "            else:\n",
    "                results = [process_image_label(isic_id) for isic_id in batch_isic_ids]\n",
    "\n",
    "            images, labels = zip(*results)\n",
    "            images = np.array(images)\n",
    "            labels = np.array(labels)\n",
    "\n",
    "            yield images, labels\n",
    "\n",
    "        if is_one_epoch:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ad42cb9dc9ce3",
   "metadata": {},
   "source": [
    "### データセットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6263f45ceacaf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T09:51:55.898691Z",
     "start_time": "2024-07-21T09:51:55.884788Z"
    }
   },
   "outputs": [],
   "source": [
    "# id をシャッフル\n",
    "random.shuffle(train_isic_ids)\n",
    "\n",
    "# データセットを分割 (positive)\n",
    "train_datasets_pos = train_isic_ids_positive[:int(len(train_isic_ids) * CONFIG[\"TRAIN_RATIO\"])]\n",
    "valid_datasets_pos = train_isic_ids_positive[int(len(train_isic_ids) * CONFIG[\"TRAIN_RATIO\"]):]\n",
    "test_datasets_pos = train_isic_ids_positive[int(len(train_isic_ids) * CONFIG[\"TRAIN_RATIO\"]):]\n",
    "\n",
    "# データセットを分割 (negative)\n",
    "train_datasets_neg = train_isic_ids_negative[:int(len(train_isic_ids) * CONFIG[\"TRAIN_RATIO\"])]\n",
    "valid_datasets_neg = train_isic_ids_negative[int(len(train_isic_ids) * CONFIG[\"TRAIN_RATIO\"]):]\n",
    "test_datasets_neg = train_isic_ids_negative[int(len(train_isic_ids) * CONFIG[\"TRAIN_RATIO\"]):]\n",
    "\n",
    "# データセットを結合\n",
    "train_datasets = np.concatenate([train_datasets_pos, train_datasets_neg])\n",
    "valid_datasets = np.concatenate([valid_datasets_pos, valid_datasets_neg])\n",
    "test_datasets = np.concatenate([test_datasets_pos, test_datasets_neg])\n",
    "\n",
    "# 陽性と陰性のデータ数を表示\n",
    "print(f\"train positive: {len(train_datasets_pos)}, negative: {len(train_datasets_neg)}\")\n",
    "\n",
    "# データセットの数をそれぞれ表示\n",
    "print(f\"train: {len(train_datasets)}, valid: {len(valid_datasets)}, test: {len(test_datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754673634c8bcc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像を1枚だけ表示\n",
    "isic_id = \"ISIC_0015670\"\n",
    "image, label = get_image_and_label(h5py.File(f\"{base_path}/{train_image_hdf5}\", \"r\"), isic_id)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f\"label: {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a0d752df48670",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img_size = 4\n",
    "# 画像を取得して表示\n",
    "plt.figure(figsize=(20, 5))\n",
    "for images, labels in generator(train_isic_ids, batch_size=show_img_size, mode=\"train\", is_shuffle=False,\n",
    "                                is_one_epoch=True, is_augmentation=True):\n",
    "    # 画像の情報を表示\n",
    "    print(f\"image shape: {images.shape}\")\n",
    "    print(f'image type: {images.dtype}')\n",
    "    print(f\"image max: {images.max()}, min: {images.min()}\")\n",
    "    print(f\"label shape: {labels.shape}\")\n",
    "\n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        # 最小値を0, 最大値を1にする\n",
    "        \n",
    "        plot_size = min(show_img_size, len(images))\n",
    "        plt.subplot(1, plot_size, i + 1)\n",
    "        plt.imshow(image)\n",
    "        if label == 1:\n",
    "            plt.title(f\"positive\")\n",
    "        else:\n",
    "            plt.title(\"negative\")\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017c72f2d6d8ca2",
   "metadata": {},
   "source": [
    "### モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fbc14cb6d1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float = 0.80):\n",
    "    v_gt = abs(np.asarray(solution.values) - 1)\n",
    "    v_pred = np.array([1.0 - x for x in submission.values])\n",
    "    max_fpr = abs(1 - min_tpr)\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "    partial_auc = 0.5 * max_fpr ** 2 + (max_fpr - 0.5 * max_fpr ** 2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return partial_auc\n",
    "\n",
    "\n",
    "def auroc(y_true, y_pred, min_fpr=0.8):\n",
    "    v_gt = tf.abs(y_true - 1)\n",
    "    v_pr = tf.abs(y_pred - 1)\n",
    "    partial_auc_scaled = tf.py_function(comp_score, [v_gt, v_pr], tf.float64)\n",
    "    partial_auc = 0.5 * min_fpr ** 2 + (min_fpr - 0.5 * min_fpr ** 2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return partial_auc\n",
    "\n",
    "\n",
    "class AUCROCMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='aucroc', **kwargs):\n",
    "        super(AUCROCMetric, self).__init__(name=name, **kwargs)\n",
    "        self.auc_metric = tf.keras.metrics.AUC()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.auc_metric.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        return self.auc_metric.result()\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.auc_metric.reset_states()\n",
    "\n",
    "def binary_crossentropy_balance(target, output):\n",
    "    # target を output の型にキャスト\n",
    "    target = tf.dtypes.cast(target, output.dtype)\n",
    "    epsilon_ = constant_op.constant(0.00001, output.dtype)\n",
    "\n",
    "    # nan を防ぐためにクリップ\n",
    "    output = clip_ops.clip_by_value(output, epsilon_, 0.99999)\n",
    "\n",
    "    # 交差エントロピーの計算\n",
    "    bce = target * math_ops.log(output + epsilon_) * 3.0\n",
    "    bce += (1.0 - target) * math_ops.log(1.0 - output + epsilon_)\n",
    "\n",
    "    bce_sum = -K.sum(bce, axis=-1)\n",
    "    return bce_sum\n",
    "\n",
    "def create_model(model_name='DenseNet201'):\n",
    "    # カスタム入力層\n",
    "    input_shape = (CONFIG[\"IMAGE_HEIGHT\"], CONFIG[\"IMAGE_WIDTH\"], CONFIG[\"IMAGE_CHANNEL\"])\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    # DenseNet201\n",
    "    base_model = tf.keras.applications.DenseNet201(\n",
    "        include_top=True,\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=x,\n",
    "        input_shape=input_shape,\n",
    "        pooling=\"avg\",\n",
    "        classes=1000,\n",
    "        classifier_activation='softmax',\n",
    "    )\n",
    "\n",
    "    # ベースモデルの出力\n",
    "    x = base_model.output\n",
    "    x = Dense(1000, activation='sigmoid')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # モデルの作成\n",
    "    model = Model(inputs=inputs, outputs=x, name=model_name)\n",
    "\n",
    "    # optimizer : Adam\n",
    "    opt = tf.keras.optimizers.AdamW(learning_rate=CONFIG[\"LR\"])\n",
    "\n",
    "    # モデルのコンパイル\n",
    "    model.compile(optimizer=opt, loss=binary_crossentropy_balance, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5d4ecd7d75eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "# モデルの概要を表示\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde11667ef130582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "class CustomCheckpoint(Callback):\n",
    "    def __init__(self, filepath):\n",
    "        super(CustomCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.best_val_accuracy = 1.0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_log_loss_metrics = logs.get('val_loss')\n",
    "\n",
    "        if val_log_loss_metrics is not None and val_log_loss_metrics < self.best_val_accuracy and val_log_loss_metrics < 0.8:\n",
    "            self.best_val_accuracy = val_log_loss_metrics\n",
    "            filepath = f'{self.filepath}.keras'\n",
    "            self.model.save(filepath, overwrite=True)\n",
    "            print(f'Saved model to {filepath} with validation accuracy: {val_log_loss_metrics:.4f}')\n",
    "\n",
    "\n",
    "# カスタムチェックポイントのコールバックを作成\n",
    "custom_checkpoint = CustomCheckpoint(filepath='model_dense')\n",
    "Ecall = EarlyStopping(monitor='val_loss', patience=CONFIG[\"PATIENCE\"], restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19509ed6f080c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    generator(train_datasets, is_augmentation=True, is_shuffle=True),\n",
    "    steps_per_epoch=len(train_datasets) // CONFIG[\"BATCH_SIZE\"],\n",
    "    epochs=CONFIG[\"N_EPOCHS\"],\n",
    "    validation_data=generator(valid_datasets, is_augmentation=False, is_shuffle=False),\n",
    "    validation_steps=len(valid_datasets) // CONFIG[\"VAL_BATCH_SIZE\"],\n",
    "    callbacks=[custom_checkpoint, Ecall]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを読み込み\n",
    "model = tf.keras.models.load_model('model_dense.keras', custom_objects={'binary_crossentropy_balance': binary_crossentropy_balance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a280bbb6ccf84693",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T09:53:15.224755Z",
     "start_time": "2024-07-21T09:52:54.578533Z"
    }
   },
   "outputs": [],
   "source": [
    "# テストデータの予測\n",
    "y_pred = model.predict(\n",
    "    generator(test_datasets, \n",
    "              is_augmentation=False,\n",
    "              is_shuffle=False, \n",
    "              is_one_epoch=True,\n",
    "              batch_size=1,\n",
    "              ),\n",
    "    steps=len(test_datasets)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3314d36e39e9c13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T09:53:15.230145Z",
     "start_time": "2024-07-21T09:53:15.226357Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'y_pred shape: {y_pred.shape}')\n",
    "print(f'test_datasets shape: {test_datasets.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bff075315dd3a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T09:53:20.012923Z",
     "start_time": "2024-07-21T09:53:20.003278Z"
    }
   },
   "outputs": [],
   "source": [
    "# 提出用のデータフレームを作成\n",
    "submission = pd.DataFrame()\n",
    "submission[\"isic_id\"] = test_datasets.reshape(-1)\n",
    "submission[\"target\"] = y_pred\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "# 正解用のデータフレームを作成\n",
    "solution = pd.DataFrame()\n",
    "solution[\"isic_id\"] = test_datasets\n",
    "# float に変換して保存\n",
    "solution[\"target\"] = df_train[df_train[\"isic_id\"].isin(test_datasets)][\"target\"].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb608537d9d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-21T09:53:21.179021Z",
     "start_time": "2024-07-21T09:53:21.171182Z"
    }
   },
   "outputs": [],
   "source": [
    "# それぞれの列の型を調べる\n",
    "print(submission.dtypes)\n",
    "print(solution.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fceb32bb905a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80):\n",
    "    v_gt = abs(np.asarray(solution.values)-1)\n",
    "    v_pred = np.array([1.0 - x for x in submission.values])\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return partial_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a29cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# スコアを計算\n",
    "score = comp_score(solution[\"target\"], submission[\"target\"], \"\")\n",
    "print(f\"Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy を計算 (0.5 以上の場合は 1, それ以外は 0)\n",
    "submission_binary = submission.copy()\n",
    "submission_binary[\"target\"] = (submission[\"target\"] > 0.5).astype(int)\n",
    "solution[\"target\"] = solution[\"target\"].astype(int)\n",
    "accuracy = (submission[\"target\"].round() == solution[\"target\"]).mean()\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec78708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果をいくつか比較\n",
    "print(submission.head())\n",
    "print(solution.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be45007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
