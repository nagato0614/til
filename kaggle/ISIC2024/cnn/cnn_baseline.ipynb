{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04c122e2ff6ef1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T13:13:14.486496Z",
     "start_time": "2024-08-05T13:13:07.890352Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import h5py\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "from keras import Model, Input\n",
    "from keras.src.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.src.callbacks import Callback\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.ops import clip_ops, math_ops\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.src import ops\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e052a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef729c7eeb009f",
   "metadata": {},
   "source": [
    "### ファイル名の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092a38f3d89cf5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T13:13:37.086633Z",
     "start_time": "2024-08-05T13:13:37.083673Z"
    }
   },
   "outputs": [],
   "source": [
    "train_image_hdf5 = \"train-image.hdf5\"\n",
    "train_metadata_csv = \"train-metadata.csv\"\n",
    "test_image_hdf5 = \"test-image.hdf5\"\n",
    "test_metadata_csv = \"test-metadata.csv\"\n",
    "sample_submission_csv = \"sample_submission.csv\"\n",
    "\n",
    "base_path = \"/Users/toru/PycharmProjects/isic-2024-challenge\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e983201959b7cc",
   "metadata": {},
   "source": [
    "### パラメータの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c627a073a2de07c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T13:13:37.911897Z",
     "start_time": "2024-08-05T13:13:37.909187Z"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"SEED\": 42,\n",
    "    \"N_FOLDS\": 5,\n",
    "    \"N_EPOCHS\": 100,\n",
    "    \"BATCH_SIZE\": 32,\n",
    "    \"VAL_BATCH_SIZE\": 32,\n",
    "    \"LR\": 0.001,\n",
    "    \"IMAGE_HEIGHT\": 224,\n",
    "    \"IMAGE_WIDTH\": 224,\n",
    "    \"IMAGE_CHANNEL\": 3,\n",
    "    \"N_CLASSES\": 1,\n",
    "    \"PATIENCE\": 10,\n",
    "    \"TRAIN_RATIO\": 0.9,\n",
    "    \"VAL_RATIO\": 0.05,\n",
    "    \"TEST_RATIO\": 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1dcef53bc67ab9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T13:13:41.017474Z",
     "start_time": "2024-08-05T13:13:38.294739Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(f\"{base_path}/{train_metadata_csv}\")\n",
    "df_test = pd.read_csv(f\"{base_path}/{test_metadata_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8d2eac3ae74dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T13:13:43.088743Z",
     "start_time": "2024-08-05T13:13:43.066005Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b3badf132e245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T13:13:45.083040Z",
     "start_time": "2024-08-05T13:13:45.071897Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_image_and_label(hdf, isic_id):\n",
    "    # 画像を取得\n",
    "    image_data = hdf[isic_id][()]\n",
    "\n",
    "    # Convert the binary data to a numpy array\n",
    "    image_data = np.frombuffer(image_data, np.uint8)\n",
    "\n",
    "    # Decode the image from the numpy array\n",
    "    image_data = cv2.cvtColor(cv2.imdecode(image_data, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image_data = cv2.resize(image_data, (CONFIG[\"IMAGE_HEIGHT\"], CONFIG[\"IMAGE_WIDTH\"]))\n",
    "\n",
    "    # 画像の正規化 (0 ~ 1) に変換\n",
    "    image_data = (image_data - image_data.min()) / (image_data.max() - image_data.min())\n",
    "    \n",
    "    # クラスを取得する. これが学習時のラベルになる\n",
    "    label = df_train[df_train[\"isic_id\"] == isic_id][\"target\"].values[0]\n",
    "    return image_data, label\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    # 画像の正規化 (0 ~ 1) に変換\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    return image\n",
    "\n",
    "def augmentation(image):\n",
    "    \n",
    "    # 確率で画像をズーム\n",
    "    if np.random.rand() > 0.5:\n",
    "        scale = np.random.uniform(0.8, 1.2)\n",
    "        image = cv2.resize(image, (0, 0), fx=scale, fy=scale)\n",
    "        image = cv2.resize(image, (CONFIG[\"IMAGE_HEIGHT\"], CONFIG[\"IMAGE_WIDTH\"]))\n",
    "    \n",
    "    # 明るさを変える\n",
    "    alpha = 1.0 + np.random.uniform(-0.01, 0.01)\n",
    "    beta = np.random.uniform(-0.01, 0.01)\n",
    "    image = image * alpha + beta\n",
    "    \n",
    "    # 画像をぼかす\n",
    "    k_size = np.random.randint(1, 10) * 2 + 1\n",
    "    image = cv2.GaussianBlur(image, (k_size, k_size), 0)\n",
    "\n",
    "    # 確率で付与するノイズを変える\n",
    "    if np.random.rand() > 0.5:\n",
    "        # ガウシアンノイズ\n",
    "        image = image + np.random.normal(0, 0.1, image.shape)\n",
    "        image = np.clip(image, 0, 1)\n",
    "        \n",
    "    # 画像を確率で反転\n",
    "    if np.random.rand() > 0.5:\n",
    "        image = cv2.flip(image, 1)\n",
    "\n",
    "    # 特定の範囲を切り抜く\n",
    "    n_cut = np.random.randint(1, 5)\n",
    "    for _ in range(n_cut):\n",
    "        WINDOW_SIZE = np.random.randint(10, 50)\n",
    "        x_min = np.random.randint(0, image.shape[0] - WINDOW_SIZE)\n",
    "        x_max = x_min + WINDOW_SIZE\n",
    "        y_min = np.random.randint(0, image.shape[1] - WINDOW_SIZE)\n",
    "        y_max = y_min + WINDOW_SIZE\n",
    "        image[x_min:x_max, y_min:y_max] = 0\n",
    "\n",
    "    # 画像を回転\n",
    "    angle = np.random.randint(0, 360)\n",
    "    image = cv2.warpAffine(image, cv2.getRotationMatrix2D((image.shape[1] / 2, image.shape[0] / 2), angle, 1.0),\n",
    "                           (image.shape[1], image.shape[0]))\n",
    "\n",
    "    return image\n",
    "\n",
    "def generator(isic_ids_df: pd.DataFrame,\n",
    "              batch_size=CONFIG[\"BATCH_SIZE\"],\n",
    "              mode=\"train\", # kaggle での実行時に関係する. testの場合はテスト用のデータセットを読み込める\n",
    "              is_augmentation=False,\n",
    "              is_shuffle=False,\n",
    "              is_one_epoch=False,\n",
    "              is_multi_threading=False,\n",
    "              is_train=True, # 学習中の場合は, 50% の確率で陽性のデータを使用する\n",
    "              ):\n",
    "    hdf_path = None\n",
    "    if mode == \"train\":\n",
    "        hdf_path = f\"{base_path}/{train_image_hdf5}\"\n",
    "    elif mode == \"test\":\n",
    "        hdf_path = f\"{base_path}/{test_image_hdf5}\"\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'train' or 'test'\")\n",
    "    \n",
    "    def process_image_label(isic_id):\n",
    "        with h5py.File(hdf_path, \"r\") as hdf:\n",
    "            image, label = get_image_and_label(hdf, isic_id)\n",
    "            if is_augmentation:\n",
    "                image = augmentation(image)\n",
    "                \n",
    "            # 画像を正規化\n",
    "            image = normalize_image(image)\n",
    "        return image, label\n",
    "\n",
    "    while True:\n",
    "        if is_shuffle:\n",
    "            isic_ids_df = isic_ids_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        size_of_df = len(isic_ids_df)\n",
    "        for i in range(0, size_of_df, batch_size):\n",
    "            end = min(i + batch_size, size_of_df)\n",
    "            \n",
    "            batch_isic_ids = []\n",
    "            \n",
    "            # バッチ数分のデータを取得\n",
    "            if is_train:\n",
    "                df_positive = isic_ids_df[isic_ids_df[\"target\"] == 1]\n",
    "                df_negative = isic_ids_df[isic_ids_df[\"target\"] == 0]\n",
    "                for j in range(batch_size):\n",
    "                    # 50% の確率で陽性のデータを取得\n",
    "                    if np.random.rand() > 0.5:\n",
    "                        batch_isic_ids.append(df_positive.sample()[\"isic_id\"].values[0])\n",
    "                    else:\n",
    "                        batch_isic_ids.append(df_negative.sample()[\"isic_id\"].values[0])\n",
    "            else:\n",
    "                # 学習中でない場合は, すべてのデータを取得\n",
    "                for j in range(i, end):\n",
    "                    batch_isic_ids.append(isic_ids_df[\"isic_id\"].iloc[j])\n",
    "            \n",
    "            if is_multi_threading:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    results = list(executor.map(process_image_label, batch_isic_ids))\n",
    "            else:\n",
    "                results = [process_image_label(isic_id) for isic_id in batch_isic_ids]\n",
    "\n",
    "            images, labels = zip(*results)\n",
    "            images = np.array(images)\n",
    "            labels = np.array(labels)\n",
    "\n",
    "            yield images, labels\n",
    "\n",
    "        if is_one_epoch:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ad42cb9dc9ce3",
   "metadata": {},
   "source": [
    "### データセットの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6263f45ceacaf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T13:13:45.922104Z",
     "start_time": "2024-08-05T13:13:45.914539Z"
    }
   },
   "outputs": [],
   "source": [
    "df_positive_data = df_train[df_train[\"target\"] == 1]\n",
    "df_negative_data = df_train[df_train[\"target\"] == 0]\n",
    "\n",
    "# pos : neg = 1 : 20 になるようにデータをサンプリング\n",
    "df_negative_data = df_negative_data.sample(len(df_positive_data) * 20)\n",
    "df_concat = pd.concat([df_positive_data, df_negative_data])\n",
    "\n",
    "# データをシャッフル\n",
    "df_concat = df_concat.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 学習データと検証データ, テストデータに分割\n",
    "train_size = int(len(df_concat) * CONFIG[\"TRAIN_RATIO\"])\n",
    "val_size = int(len(df_concat) * CONFIG[\"VAL_RATIO\"])\n",
    "test_size = int(len(df_concat) * CONFIG[\"TEST_RATIO\"])\n",
    "\n",
    "df_train_dataset = df_concat[:train_size]\n",
    "df_val_dataset = df_concat[train_size:train_size + val_size]\n",
    "df_test_dataset = df_concat[train_size + val_size:]\n",
    "\n",
    "# データセットのサイズ\n",
    "print(f\"train_size: {len(df_train_dataset)}\")\n",
    "print(f\"val_size: {len(df_val_dataset)}\")\n",
    "print(f\"test_size: {len(df_test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154eccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754673634c8bcc95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T13:13:46.705772Z",
     "start_time": "2024-08-05T13:13:46.482850Z"
    }
   },
   "outputs": [],
   "source": [
    "# 画像を1枚だけ表示\n",
    "isic_id = \"ISIC_0015670\"\n",
    "image, label = get_image_and_label(h5py.File(f\"{base_path}/{train_image_hdf5}\", \"r\"), isic_id)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(f\"label: {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a0d752df48670",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T13:13:48.101026Z",
     "start_time": "2024-08-05T13:13:47.516736Z"
    }
   },
   "outputs": [],
   "source": [
    "show_img_size = 4\n",
    "# 画像を取得して表示\n",
    "plt.figure(figsize=(20, 5))\n",
    "for images, labels in generator(df_train_dataset, batch_size=show_img_size, mode=\"train\", is_shuffle=False,\n",
    "                                is_one_epoch=True, is_augmentation=True, is_train=False):\n",
    "    # 画像の情報を表示\n",
    "    print(f\"image shape: {images.shape}\")\n",
    "    print(f'image type: {images.dtype}')\n",
    "    print(f\"image max: {images.max()}, min: {images.min()}\")\n",
    "    print(f\"label shape: {labels.shape}\")\n",
    "\n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        # 最小値を0, 最大値を1にする\n",
    "        \n",
    "        plot_size = min(show_img_size, len(images))\n",
    "        plt.subplot(1, plot_size, i + 1)\n",
    "        plt.imshow(image)\n",
    "        if label == 1:\n",
    "            plt.title(f\"positive\")\n",
    "        else:\n",
    "            plt.title(\"negative\")\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017c72f2d6d8ca2",
   "metadata": {},
   "source": [
    "### モデルの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fbc14cb6d1087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T13:13:50.942974Z",
     "start_time": "2024-08-05T13:13:50.931843Z"
    }
   },
   "outputs": [],
   "source": [
    "def comp_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float = 0.80):\n",
    "    v_gt = abs(np.asarray(solution.values) - 1)\n",
    "    v_pred = np.array([1.0 - x for x in submission.values])\n",
    "    max_fpr = abs(1 - min_tpr)\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "    partial_auc = 0.5 * max_fpr ** 2 + (max_fpr - 0.5 * max_fpr ** 2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return partial_auc\n",
    "\n",
    "\n",
    "def auroc(y_true, y_pred, min_fpr=0.8):\n",
    "    v_gt = tf.abs(y_true - 1)\n",
    "    v_pr = tf.abs(y_pred - 1)\n",
    "    partial_auc_scaled = tf.py_function(comp_score, [v_gt, v_pr], tf.float64)\n",
    "    partial_auc = 0.5 * min_fpr ** 2 + (min_fpr - 0.5 * min_fpr ** 2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return partial_auc\n",
    "\n",
    "\n",
    "class AUCROCMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='aucroc', **kwargs):\n",
    "        super(AUCROCMetric, self).__init__(name=name, **kwargs)\n",
    "        self.auc_metric = tf.keras.metrics.AUC()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.auc_metric.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        return self.auc_metric.result()\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.auc_metric.reset_states()\n",
    "\n",
    "def binary_crossentropy_balance(target, output):\n",
    "    # target を output の型にキャスト\n",
    "    target = tf.dtypes.cast(target, output.dtype)\n",
    "    epsilon_ = constant_op.constant(0.00001, output.dtype)\n",
    "\n",
    "    # nan を防ぐためにクリップ\n",
    "    output = clip_ops.clip_by_value(output, epsilon_, 0.99999)\n",
    "\n",
    "    # 交差エントロピーの計算\n",
    "    bce = target * math_ops.log(output + epsilon_) * 1.0\n",
    "    bce += (1.0 - target) * math_ops.log(1.0 - output + epsilon_)\n",
    "\n",
    "    bce_sum = -K.sum(bce, axis=-1)\n",
    "    return bce_sum\n",
    "\n",
    "class GeM(tf.keras.layers.Layer):\n",
    "    def __init__(self, p=3.0, eps=1e-6, **kwargs):\n",
    "        super(GeM, self).__init__(**kwargs)\n",
    "        self.p = tf.Variable(initial_value=p, trainable=True, dtype=tf.float32)\n",
    "        self.eps = eps\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.reduce_mean(tf.pow(tf.clip_by_value(inputs, self.eps, tf.reduce_max(inputs)), self.p), axis=[1, 2])\n",
    "\n",
    "def create_model(model_name='DenseNet201'):\n",
    "    # カスタム入力層\n",
    "    input_shape = (CONFIG[\"IMAGE_HEIGHT\"], CONFIG[\"IMAGE_WIDTH\"], CONFIG[\"IMAGE_CHANNEL\"])\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    # DenseNet201\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_tensor=x,\n",
    "        input_shape=input_shape,\n",
    "        pooling=None,\n",
    "    )\n",
    "\n",
    "    # ベースモデルの出力\n",
    "    x = base_model.output\n",
    "    x = GeM()(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # モデルの作成\n",
    "    model = Model(inputs=inputs, outputs=x, name=model_name)\n",
    "\n",
    "    # optimizer : Adam\n",
    "    opt = tf.keras.optimizers.AdamW(learning_rate=CONFIG[\"LR\"])\n",
    "\n",
    "    # モデルのコンパイル\n",
    "    model.compile(optimizer=opt, loss=binary_crossentropy_balance, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c5d4ecd7d75eae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T13:14:02.388645Z",
     "start_time": "2024-08-05T13:13:59.411165Z"
    }
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "# モデルの概要を表示\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde11667ef130582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "class CustomCheckpoint(Callback):\n",
    "    def __init__(self, filepath):\n",
    "        super(CustomCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.best_val_accuracy = 1.0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        val_log_loss_metrics = logs.get('val_loss')\n",
    "\n",
    "        if val_log_loss_metrics is not None and val_log_loss_metrics < self.best_val_accuracy and val_log_loss_metrics < 0.8:\n",
    "            self.best_val_accuracy = val_log_loss_metrics\n",
    "            filepath = f'{self.filepath}.keras'\n",
    "            self.model.save(filepath, overwrite=True)\n",
    "            print(f'Saved model to {filepath} with validation accuracy: {val_log_loss_metrics:.4f}')\n",
    "\n",
    "\n",
    "# カスタムチェックポイントのコールバックを作成\n",
    "custom_checkpoint = CustomCheckpoint(filepath='model_dense')\n",
    "Ecall = EarlyStopping(monitor='val_loss', patience=CONFIG[\"PATIENCE\"], restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19509ed6f080c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = int(np.ceil(len(df_train_dataset) / CONFIG[\"BATCH_SIZE\"]))\n",
    "val_steps = int(np.ceil(len(df_val_dataset) / CONFIG[\"VAL_BATCH_SIZE\"]))\n",
    "\n",
    "history = model.fit(\n",
    "    generator(df_train_dataset, is_augmentation=True, is_shuffle=True, is_train=True, is_multi_threading=True),\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=CONFIG[\"N_EPOCHS\"],\n",
    "    validation_data=generator(df_val_dataset, is_augmentation=False, is_shuffle=False, is_train=False, is_multi_threading=True),\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=[custom_checkpoint, Ecall]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを読み込み\n",
    "model = tf.keras.models.load_model('model_dense.keras', custom_objects={'binary_crossentropy_balance': binary_crossentropy_balance})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a280bbb6ccf84693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの予測\n",
    "y_pred = model.predict(\n",
    "    generator(test_datasets, \n",
    "              is_augmentation=False,\n",
    "              is_shuffle=False, \n",
    "              is_one_epoch=True,\n",
    "              batch_size=1,\n",
    "              ),\n",
    "    steps=len(test_datasets)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3314d36e39e9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'y_pred shape: {y_pred.shape}')\n",
    "print(f'test_datasets shape: {test_datasets.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bff075315dd3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出用のデータフレームを作成\n",
    "submission = pd.DataFrame()\n",
    "submission[\"isic_id\"] = test_datasets.reshape(-1)\n",
    "submission[\"target\"] = y_pred\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "# 正解用のデータフレームを作成\n",
    "solution = pd.DataFrame()\n",
    "solution[\"isic_id\"] = test_datasets\n",
    "# float に変換して保存\n",
    "solution[\"target\"] = df_train[df_train[\"isic_id\"].isin(test_datasets)][\"target\"].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb608537d9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれの列の型を調べる\n",
    "print(submission.dtypes)\n",
    "print(solution.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60fceb32bb905a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80):\n",
    "    v_gt = abs(np.asarray(solution.values)-1)\n",
    "    v_pred = np.array([1.0 - x for x in submission.values])\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return partial_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a29cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# スコアを計算\n",
    "score = comp_score(solution[\"target\"], submission[\"target\"], \"\")\n",
    "print(f\"Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy を計算 (0.5 以上の場合は 1, それ以外は 0)\n",
    "submission_binary = submission.copy()\n",
    "submission_binary[\"target\"] = (submission[\"target\"] > 0.5).astype(int)\n",
    "solution[\"target\"] = solution[\"target\"].astype(int)\n",
    "accuracy = (submission[\"target\"].round() == solution[\"target\"]).mean()\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec78708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果をいくつか比較\n",
    "print(submission.head())\n",
    "print(solution.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
